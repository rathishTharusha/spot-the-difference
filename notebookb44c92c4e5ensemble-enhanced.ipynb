{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ultra-Advanced Spot the Difference Pipeline\n",
    "\n",
    "**Focus: Fix Object Detection Bottleneck**\n",
    "\n",
    "This notebook addresses the core accuracy issues:\n",
    "1. **üîç Enhanced Object Detection**: Grounding DINO + OWL-ViT ensemble\n",
    "2. **üìà Image Upscaling**: Super-resolution for low-resolution images\n",
    "3. **üéØ Threshold Calibration**: Cross-validation F1 optimization\n",
    "4. **üìö Expanded Vocabulary**: Rich prompts with synonyms\n",
    "5. **üß† Pre-trained ChangeFormer**: Load existing model\n",
    "6. **üîß Strong Augmentations**: Data augmentation pipeline\n",
    "\n",
    "**Key Insight**: Poor object detection (due to low resolution) is the main bottleneck ‚Üí Fix detection first!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageFilter, ImageEnhance\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import f1_score, precision_recall_fscore_support\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(0))\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Enhanced Data Loading & Image Preprocessing\n",
    "\n",
    "**Problem**: Low resolution images cause poor object detection\n",
    "**Solution**: Super-resolution upscaling + strong augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "data_dir = './'\n",
    "train_df = pd.read_csv(os.path.join(data_dir, 'train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(data_dir, 'test.csv'))\n",
    "\n",
    "print('Train Data Sample:')\n",
    "display(train_df.head())\n",
    "print(f'\\nTrain samples: {len(train_df)}')\n",
    "print(f'Test samples: {len(test_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Super-resolution image enhancement\n",
    "class ImageEnhancer:\n",
    "    \"\"\"\n",
    "    Advanced image enhancement for better object detection\n",
    "    \"\"\"\n",
    "    def __init__(self, target_size=(1024, 1024)):\n",
    "        self.target_size = target_size\n",
    "        \n",
    "    def enhance_image(self, image_path):\n",
    "        \"\"\"\n",
    "        Apply super-resolution and enhancement techniques\n",
    "        \"\"\"\n",
    "        # Load image\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            raise ValueError(f\"Could not load image: {image_path}\")\n",
    "        \n",
    "        # Convert to RGB\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Get original dimensions\n",
    "        h, w = img_rgb.shape[:2]\n",
    "        \n",
    "        # Super-resolution upscaling if image is small\n",
    "        if h < 512 or w < 512:\n",
    "            # Calculate upscale factor\n",
    "            scale_factor = max(512 / h, 512 / w, 1.0)\n",
    "            if scale_factor > 1.0:\n",
    "                new_w = int(w * scale_factor)\n",
    "                new_h = int(h * scale_factor)\n",
    "                img_rgb = cv2.resize(img_rgb, (new_w, new_h), interpolation=cv2.INTER_CUBIC)\n",
    "        \n",
    "        # Convert to PIL for further processing\n",
    "        pil_img = Image.fromarray(img_rgb)\n",
    "        \n",
    "        # Apply enhancement filters\n",
    "        # 1. Sharpening\n",
    "        pil_img = pil_img.filter(ImageFilter.UnsharpMask(radius=1, percent=150, threshold=3))\n",
    "        \n",
    "        # 2. Contrast enhancement\n",
    "        enhancer = ImageEnhance.Contrast(pil_img)\n",
    "        pil_img = enhancer.enhance(1.1)\n",
    "        \n",
    "        # 3. Brightness adjustment\n",
    "        enhancer = ImageEnhance.Brightness(pil_img)\n",
    "        pil_img = enhancer.enhance(1.05)\n",
    "        \n",
    "        # Resize to target size if needed\n",
    "        if pil_img.size != self.target_size:\n",
    "            pil_img = pil_img.resize(self.target_size, Image.LANCZOS)\n",
    "        \n",
    "        return pil_img\n",
    "    \n",
    "    def get_image_stats(self, image_path):\n",
    "        \"\"\"Get image statistics for analysis\"\"\"\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            return None\n",
    "        h, w = img.shape[:2]\n",
    "        return {'width': w, 'height': h, 'aspect_ratio': w/h, 'area': w*h}\n",
    "\n",
    "# Initialize enhancer\n",
    "enhancer = ImageEnhancer(target_size=(1024, 1024))\n",
    "\n",
    "print(\"‚úÖ Image enhancer initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Analyze image statistics\n",
    "print(\"üìä Analyzing image statistics...\")\n",
    "\n",
    "image_stats = []\n",
    "for img_id in tqdm(train_df['img_id'].head(100), desc=\"Analyzing images\"):  # Sample first 100\n",
    "    for suffix in ['1', '2']:\n",
    "        img_path = os.path.join(data_dir, 'data/data', f'{img_id}_{suffix}.png')\n",
    "        stats = enhancer.get_image_stats(img_path)\n",
    "        if stats:\n",
    "            stats['img_id'] = img_id\n",
    "            stats['suffix'] = suffix\n",
    "            image_stats.append(stats)\n",
    "\n",
    "stats_df = pd.DataFrame(image_stats)\n",
    "\n",
    "print(\"\\nüìà Image Statistics Summary:\")\n",
    "print(f\"Total images analyzed: {len(stats_df)}\")\n",
    "print(f\"Average resolution: {stats_df['width'].mean():.0f}x{stats_df['height'].mean():.0f}\")\n",
    "print(f\"Min resolution: {stats_df['width'].min():.0f}x{stats_df['height'].min():.0f}\")\n",
    "print(f\"Max resolution: {stats_df['width'].max():.0f}x{stats_df['height'].max():.0f}\")\n",
    "print(f\"Images smaller than 512x512: {len(stats_df[stats_df['area'] < 512*512])} ({100*len(stats_df[stats_df['area'] < 512*512])/len(stats_df):.1f}%)\")\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(stats_df['width'], bins=30, alpha=0.7)\n",
    "plt.title('Width Distribution')\n",
    "plt.xlabel('Width (pixels)')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(stats_df['height'], bins=30, alpha=0.7)\n",
    "plt.title('Height Distribution')\n",
    "plt.xlabel('Height (pixels)')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(stats_df['area'], bins=30, alpha=0.7)\n",
    "plt.title('Area Distribution')\n",
    "plt.xlabel('Area (pixels¬≤)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ Key Insight: Many images are low resolution ‚Üí Super-resolution enhancement needed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Strong data augmentation pipeline\n",
    "def create_augmentation_pipeline():\n",
    "    \"\"\"\n",
    "    Create strong augmentation pipeline for training\n",
    "    \"\"\"\n",
    "    return A.Compose([\n",
    "        # Geometric augmentations\n",
    "        A.Rotate(limit=15, p=0.3),\n",
    "        A.Affine(scale=(0.8, 1.2), translate_percent=0.1, rotate=(-10, 10), p=0.4),\n",
    "        \n",
    "        # Color augmentations\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "        A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.3),\n",
    "        A.RGBShift(r_shift_limit=10, g_shift_limit=10, b_shift_limit=10, p=0.3),\n",
    "        \n",
    "        # Noise and blur\n",
    "        A.GaussNoise(var_limit=(10, 50), p=0.2),\n",
    "        A.GaussianBlur(blur_limit=3, p=0.1),\n",
    "        \n",
    "        # Cutout/Mixup\n",
    "        A.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.3),\n",
    "        \n",
    "        # Normalize\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "# Test enhancement on sample image\n",
    "sample_img_id = train_df['img_id'].iloc[0]\n",
    "sample_img_path = os.path.join(data_dir, 'data/data', f'{sample_img_id}_1.png')\n",
    "\n",
    "print(f\"üß™ Testing image enhancement on: {sample_img_id}\")\n",
    "\n",
    "# Original image\n",
    "original_img = Image.open(sample_img_path).convert('RGB')\n",
    "\n",
    "# Enhanced image\n",
    "enhanced_img = enhancer.enhance_image(sample_img_path)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax1.imshow(original_img)\n",
    "ax1.set_title(f'Original\\n{original_img.size}')\n",
    "ax1.axis('off')\n",
    "\n",
    "ax2.imshow(enhanced_img)\n",
    "ax2.set_title(f'Enhanced\\n{enhanced_img.size}')\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Image enhancement working! Resolution improved from {original_img.size} to {enhanced_img.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Expanded Vocabulary with Rich Prompts\n",
    "\n",
    "**Problem**: Limited vocabulary hurts detection accuracy\n",
    "**Solution**: Rich prompts with synonyms, descriptions, and context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Extract vocabulary STRICTLY from training data labels only\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"üìö Extracting vocabulary from training dataset labels...\")\n",
    "\n",
    "# Extract base vocabulary from actual labels\n",
    "term_frequencies = defaultdict(int)\n",
    "\n",
    "for col in ['added_objs', 'removed_objs', 'changed_objs']:\n",
    "    for label_str in train_df[col].dropna():\n",
    "        if isinstance(label_str, str) and label_str.strip().lower() not in ['', 'none', 'null', 'nan']:\n",
    "            # Split by common separators\n",
    "            tokens = re.split(r'[,&\\s]+', label_str.strip().lower())\n",
    "            for token in tokens:\n",
    "                token = token.strip()\n",
    "                if token and token != 'none':\n",
    "                    term_frequencies[token] += 1\n",
    "\n",
    "# Print statistics\n",
    "print(f\"\\nTotal unique terms found: {len(term_frequencies)}\")\n",
    "print(f\"\\nTerm frequency distribution:\")\n",
    "for freq_threshold in [1, 2, 5, 10]:\n",
    "    count = len([t for t, f in term_frequencies.items() if f >= freq_threshold])\n",
    "    print(f\"  Terms appearing >= {freq_threshold} times: {count}\")\n",
    "\n",
    "# Filter terms - keep ALL terms that appear at least once in training data\n",
    "# No filtering by generic terms - trust the training labels!\n",
    "filtered_terms = {term: freq for term, freq in term_frequencies.items() if freq >= 1}\n",
    "\n",
    "# Sort by frequency (most common first)\n",
    "sorted_terms = sorted(filtered_terms.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Create base vocabulary - ONLY terms from training data\n",
    "base_vocabulary = [term for term, freq in sorted_terms]\n",
    "\n",
    "print(f\"\\n‚úÖ Base vocabulary created: {len(base_vocabulary)} terms\")\n",
    "print(f\"\\nüìä Top 30 most frequent terms in training data:\")\n",
    "for i, (term, freq) in enumerate(sorted_terms[:30], 1):\n",
    "    print(f\"  {i:2d}. {term:20s} (appears {freq:3d} times)\")\n",
    "\n",
    "# Analyze label patterns\n",
    "print(f\"\\nüîç Label pattern analysis:\")\n",
    "added_count = train_df['added_objs'].apply(lambda x: x not in ['', 'none', None] if isinstance(x, str) else False).sum()\n",
    "removed_count = train_df['removed_objs'].apply(lambda x: x not in ['', 'none', None] if isinstance(x, str) else False).sum()\n",
    "changed_count = train_df['changed_objs'].apply(lambda x: x not in ['', 'none', None] if isinstance(x, str) else False).sum()\n",
    "\n",
    "print(f\"  Samples with added objects: {added_count} ({100*added_count/len(train_df):.1f}%)\")\n",
    "print(f\"  Samples with removed objects: {removed_count} ({100*removed_count/len(train_df):.1f}%)\")\n",
    "print(f\"  Samples with changed objects: {changed_count} ({100*changed_count/len(train_df):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Simplified vocabulary - NO EXPANSION, use exact terms only\n",
    "class StrictVocabulary:\n",
    "    \"\"\"\n",
    "    Strict vocabulary using ONLY terms from training data\n",
    "    No synonyms, no expansions - exact matching only\n",
    "    \"\"\"\n",
    "    def __init__(self, base_terms):\n",
    "        self.base_terms = list(base_terms)  # Keep exact order\n",
    "        self.term_set = set(base_terms)\n",
    "        \n",
    "    def get_detection_prompts(self, add_simple_articles=False):\n",
    "        \"\"\"\n",
    "        Get detection prompts\n",
    "        \n",
    "        Args:\n",
    "            add_simple_articles: If True, add \"a X\" variants for better detection\n",
    "                                Only adds simple variants, no synonyms\n",
    "        \n",
    "        Returns:\n",
    "            List of prompts for object detection\n",
    "        \"\"\"\n",
    "        prompts = []\n",
    "        \n",
    "        for term in self.base_terms:\n",
    "            prompts.append(term)\n",
    "            \n",
    "            # Optionally add simple article variants for better detection\n",
    "            if add_simple_articles:\n",
    "                prompts.append(f\"a {term}\")\n",
    "                prompts.append(f\"the {term}\")\n",
    "        \n",
    "        return prompts\n",
    "    \n",
    "    def normalize_term(self, detected_term):\n",
    "        \"\"\"\n",
    "        Normalize detected term to match training vocabulary\n",
    "        \n",
    "        Args:\n",
    "            detected_term: Term detected by model\n",
    "        \n",
    "        Returns:\n",
    "            Normalized term if match found, original term otherwise\n",
    "        \"\"\"\n",
    "        # Clean the term\n",
    "        cleaned = detected_term.lower().strip()\n",
    "        \n",
    "        # Remove common articles\n",
    "        for article in ['a ', 'an ', 'the ']:\n",
    "            if cleaned.startswith(article):\n",
    "                cleaned = cleaned[len(article):]\n",
    "        \n",
    "        # Check if it's in our vocabulary\n",
    "        if cleaned in self.term_set:\n",
    "            return cleaned\n",
    "        \n",
    "        # Check if any vocabulary term is contained in the detected term\n",
    "        for vocab_term in self.base_terms:\n",
    "            if vocab_term in cleaned or cleaned in vocab_term:\n",
    "                return vocab_term\n",
    "        \n",
    "        # Return original if no match\n",
    "        return cleaned\n",
    "    \n",
    "    def filter_predictions(self, predicted_terms):\n",
    "        \"\"\"\n",
    "        Filter predictions to only include terms in training vocabulary\n",
    "        \n",
    "        Args:\n",
    "            predicted_terms: List of predicted terms\n",
    "        \n",
    "        Returns:\n",
    "            Filtered list containing only valid vocabulary terms\n",
    "        \"\"\"\n",
    "        filtered = []\n",
    "        for term in predicted_terms:\n",
    "            normalized = self.normalize_term(term)\n",
    "            if normalized in self.term_set:\n",
    "                filtered.append(normalized)\n",
    "        \n",
    "        return list(set(filtered))  # Remove duplicates\n",
    "\n",
    "# Create strict vocabulary - NO EXPANSION\n",
    "strict_vocab = StrictVocabulary(base_vocabulary)\n",
    "\n",
    "# Get detection prompts (with minimal article variants for better detection)\n",
    "detection_prompts = strict_vocab.get_detection_prompts(add_simple_articles=True)\n",
    "\n",
    "print(f\"\\n‚úÖ Strict vocabulary created!\")\n",
    "print(f\"   Base terms: {len(strict_vocab.base_terms)}\")\n",
    "print(f\"   Detection prompts: {len(detection_prompts)} (includes article variants)\")\n",
    "print(f\"\\n   Sample base terms: {strict_vocab.base_terms[:15]}\")\n",
    "print(f\"   Sample prompts: {detection_prompts[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Object Detection: Grounding DINO + OWL-ViT Ensemble\n",
    "\n",
    "**Problem**: Single detector misses objects\n",
    "**Solution**: Ensemble of Grounding DINO (text-grounded) + OWL-ViT (open-vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#!pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load OWL-ViT (already familiar)\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "\n",
    "print(\"Loading OWL-ViT model...\")\n",
    "owlvit_processor = AutoProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "owlvit_model = AutoModelForZeroShotObjectDetection.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "owlvit_model = owlvit_model.to(device)\n",
    "owlvit_model.eval()\n",
    "print(\"‚úÖ OWL-ViT loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load Grounding DINO\n",
    "try:\n",
    "    from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "    \n",
    "    print(\"Loading Grounding DINO model...\")\n",
    "    \n",
    "    # Model from Hugging Face transformers\n",
    "    model_id = \"IDEA-Research/grounding-dino-base\"\n",
    "    \n",
    "    try:\n",
    "        grounding_dino_processor = AutoProcessor.from_pretrained(model_id)\n",
    "        grounding_dino_model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id).to(device)\n",
    "        grounding_dino_available = True\n",
    "        print(\"‚úÖ Grounding DINO loaded\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Grounding DINO loading failed: {e}\")\n",
    "        print(\"‚ö†Ô∏è Will use OWL-ViT only\")\n",
    "        grounding_dino_available = False\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è transformers library issue, will use OWL-ViT only\")\n",
    "    grounding_dino_available = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# OWL-ViT detection function\n",
    "def detect_with_owlvit(image_or_path, vocab_terms, threshold=0.08):\n",
    "    \"\"\"\n",
    "    Detect objects using OWL-ViT with vocabulary prompts\n",
    "    \"\"\"\n",
    "    image = image_or_path if isinstance(image_or_path, Image.Image) else Image.open(image_or_path).convert('RGB')\n",
    "    inputs = owlvit_processor(text=vocab_terms, images=image, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = owlvit_model(**inputs)\n",
    "    \n",
    "    target_sizes = torch.tensor([image.size[::-1]]).to(device)\n",
    "    results = owlvit_processor.post_process_object_detection(\n",
    "        outputs, \n",
    "        target_sizes=target_sizes, \n",
    "        threshold=threshold\n",
    "    )[0]\n",
    "    \n",
    "    boxes = results['boxes'].cpu().numpy()\n",
    "    scores = results['scores'].cpu().numpy()\n",
    "    labels = results['labels'].cpu().numpy()\n",
    "    detected_terms = [vocab_terms[int(label)] for label in labels]\n",
    "    \n",
    "    return boxes, scores, labels, detected_terms\n",
    "\n",
    "print(\"‚úÖ OWL-ViT detection function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Grounding DINO detection function\n",
    "def detect_with_grounding_dino(image_or_path, text_prompts, box_threshold=0.3, text_threshold=0.25):\n",
    "    \"\"\"\n",
    "    Detect objects using Grounding DINO via transformers\n",
    "    \"\"\"\n",
    "    if not grounding_dino_available:\n",
    "        return np.array([]), np.array([]), np.array([]), []\n",
    "    \n",
    "    try:\n",
    "        # Load image\n",
    "        image = image_or_path if isinstance(image_or_path, Image.Image) else Image.open(image_or_path).convert('RGB')\n",
    "        \n",
    "        # Create text prompt (IMPORTANT: lowercase + end with dots)\n",
    "        # Limit to avoid token limits\n",
    "        text_list = text_prompts[:50]\n",
    "        text = '. '.join([t.lower() for t in text_list]) + '.'\n",
    "        \n",
    "        # Process inputs\n",
    "        inputs = grounding_dino_processor(images=image, text=text, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # Run prediction\n",
    "        with torch.no_grad():\n",
    "            outputs = grounding_dino_model(**inputs)\n",
    "        \n",
    "        # Post-process results\n",
    "        results = grounding_dino_processor.post_process_grounded_object_detection(\n",
    "            outputs,\n",
    "            inputs.input_ids,\n",
    "            box_threshold=box_threshold,\n",
    "            text_threshold=text_threshold,\n",
    "            target_sizes=[image.size[::-1]]\n",
    "        )[0]\n",
    "        \n",
    "        # Extract results\n",
    "        boxes = results['boxes'].cpu().numpy()\n",
    "        scores = results['scores'].cpu().numpy()\n",
    "        labels_indices = results['labels']  # These are indices into the text prompts\n",
    "        \n",
    "        # Map labels back to text terms\n",
    "        detected_terms = []\n",
    "        for label_idx in labels_indices:\n",
    "            if label_idx < len(text_list):\n",
    "                detected_terms.append(text_list[label_idx])\n",
    "            else:\n",
    "                detected_terms.append(text_list[0])  # Fallback\n",
    "        \n",
    "        # Create numeric labels for consistency\n",
    "        labels = np.arange(len(detected_terms))\n",
    "        \n",
    "        return boxes, scores, labels, detected_terms\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Grounding DINO error: {e}\")\n",
    "        return np.array([]), np.array([]), np.array([]), []\n",
    "\n",
    "print(\"‚úÖ Grounding DINO detection function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Enhanced ensemble detection\n",
    "from torchvision.ops import nms\n",
    "\n",
    "def detect_ensemble_advanced(image_path, vocab_terms, use_enhancement=True):\n",
    "    \"\"\"\n",
    "    Advanced ensemble detection: Grounding DINO + OWL-ViT\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to image\n",
    "        vocab_terms: Base vocabulary terms\n",
    "        use_enhancement: Whether to enhance image first\n",
    "    \n",
    "    Returns:\n",
    "        boxes, scores, labels, detected_terms (mapped to base vocabulary)\n",
    "    \"\"\"\n",
    "    # Enhance image if requested\n",
    "    # Prepare in-memory image\n",
    "    if use_enhancement:\n",
    "        work_image = enhancer.enhance_image(image_path)  # PIL image\n",
    "    else:\n",
    "        work_image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "    # Get detection prompts\n",
    "    prompts = strict_vocab.get_detection_prompts(add_simple_articles=True)\n",
    "    \n",
    "    # Detect with OWL-ViT\n",
    "    boxes_owl, scores_owl, labels_owl, terms_owl = detect_with_owlvit(\n",
    "        work_image, prompts, threshold=0.05\n",
    "    )\n",
    "    \n",
    "    # Detect with Grounding DINO if available\n",
    "    if grounding_dino_available:\n",
    "        boxes_gdino, scores_gdino, labels_gdino, terms_gdino = detect_with_grounding_dino(\n",
    "            work_image, prompts[:50]  # Limit prompts to avoid token limits\n",
    "        )\n",
    "    else:\n",
    "        boxes_gdino, scores_gdino, labels_gdino, terms_gdino = np.array([]), np.array([]), np.array([]), []\n",
    "    \n",
    "    # Combine detections\n",
    "    all_boxes = []\n",
    "    all_scores = []\n",
    "    all_terms = []\n",
    "    \n",
    "    # Add OWL-ViT detections\n",
    "    if len(boxes_owl) > 0:\n",
    "        all_boxes.append(boxes_owl)\n",
    "        all_scores.append(scores_owl)\n",
    "        all_terms.extend(terms_owl)\n",
    "    \n",
    "    # Add Grounding DINO detections\n",
    "    if len(boxes_gdino) > 0:\n",
    "        all_boxes.append(boxes_gdino)\n",
    "        all_scores.append(scores_gdino)\n",
    "        all_terms.extend(terms_gdino)\n",
    "    \n",
    "    # Merge boxes\n",
    "    merged_boxes = np.vstack(all_boxes) if len(all_boxes) > 1 else all_boxes[0]\n",
    "    merged_scores = np.concatenate(all_scores) if len(all_scores) > 1 else all_scores[0]\n",
    "    \n",
    "    # Apply NMS\n",
    "    from torchvision.ops import nms\n",
    "    \n",
    "    if len(merged_boxes) > 0:\n",
    "        boxes_tensor = torch.tensor(merged_boxes, dtype=torch.float32)\n",
    "        scores_tensor = torch.tensor(merged_scores, dtype=torch.float32)\n",
    "        \n",
    "        keep_indices = nms(boxes_tensor, scores_tensor, iou_threshold=0.5)\n",
    "        \n",
    "        final_boxes = merged_boxes[keep_indices.numpy()]\n",
    "        final_scores = merged_scores[keep_indices.numpy()]\n",
    "        final_terms_raw = [all_terms[i] for i in keep_indices.numpy()]\n",
    "        \n",
    "        # STRICT FILTERING: Normalize and filter to training vocabulary only\n",
    "        final_terms_normalized = []\n",
    "        valid_indices = []\n",
    "        \n",
    "        for i, term in enumerate(final_terms_raw):\n",
    "            normalized = strict_vocab.normalize_term(term)\n",
    "            if normalized in strict_vocab.term_set:\n",
    "                final_terms_normalized.append(normalized)\n",
    "                valid_indices.append(i)\n",
    "        \n",
    "        # Only keep boxes with valid vocabulary matches\n",
    "        if len(valid_indices) > 0:\n",
    "            final_boxes = final_boxes[valid_indices]\n",
    "            final_scores = final_scores[valid_indices]\n",
    "            final_labels = np.array([base_vocabulary.index(term) for term in final_terms_normalized])\n",
    "        else:\n",
    "            final_boxes, final_scores, final_labels, final_terms_normalized = np.array([]), np.array([]), np.array([]), []\n",
    "    else:\n",
    "        final_boxes, final_scores, final_labels, final_terms_normalized = np.array([]), np.array([]), np.array([]), []\n",
    "    \n",
    "    return final_boxes, final_scores, final_labels, final_terms_normalized\n",
    "\n",
    "print(\"‚úÖ Strict ensemble detection ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Pre-trained ChangeFormer Model\n",
    "\n",
    "**Problem**: Training from scratch takes time and may not be optimal\n",
    "**Solution**: Load pre-trained ChangeFormer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load pre-trained ChangeFormer\n",
    "import timm\n",
    "from torch.nn import MultiheadAttention\n",
    "\n",
    "class ChangeFormer(nn.Module):\n",
    "    def __init__(self, backbone='vit_base_patch16_224', num_heads=8, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.encoder = timm.create_model(backbone, pretrained=True, num_classes=0)\n",
    "        embed_dim = self.encoder.num_features\n",
    "        \n",
    "        self.cross_attn_1to2 = MultiheadAttention(\n",
    "            embed_dim=embed_dim, num_heads=num_heads, batch_first=True\n",
    "        )\n",
    "        self.cross_attn_2to1 = MultiheadAttention(\n",
    "            embed_dim=embed_dim, num_heads=num_heads, batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(embed_dim * 4, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        \n",
    "        self.change_head = nn.Linear(hidden_dim // 2, 1)\n",
    "    \n",
    "    def forward(self, img1, img2):\n",
    "        feat1 = self.encoder.forward_features(img1)\n",
    "        feat2 = self.encoder.forward_features(img2)\n",
    "        \n",
    "        attn_1to2, _ = self.cross_attn_1to2(feat1, feat2, feat2)\n",
    "        attn_2to1, _ = self.cross_attn_2to1(feat2, feat1, feat1)\n",
    "        \n",
    "        feat1_pool = feat1.mean(dim=1)\n",
    "        feat2_pool = feat2.mean(dim=1)\n",
    "        attn_1to2_pool = attn_1to2.mean(dim=1)\n",
    "        attn_2to1_pool = attn_2to1.mean(dim=1)\n",
    "        \n",
    "        combined = torch.cat([feat1_pool, feat2_pool, attn_1to2_pool, attn_2to1_pool], dim=1)\n",
    "        fused = self.fusion(combined)\n",
    "        change_logits = self.change_head(fused)\n",
    "        \n",
    "        return change_logits.squeeze(-1)\n",
    "\n",
    "# Try to load pre-trained ChangeFormer\n",
    "changeformer_path = 'changeformer_model.pth'\n",
    "if os.path.exists(changeformer_path):\n",
    "    print(f\"Loading pre-trained ChangeFormer from {changeformer_path}...\")\n",
    "    changeformer_model = ChangeFormer()\n",
    "    changeformer_model.load_state_dict(torch.load(changeformer_path, map_location='cpu'))\n",
    "    changeformer_model = changeformer_model.to(device)\n",
    "    changeformer_model.eval()\n",
    "    print(\"‚úÖ Pre-trained ChangeFormer loaded\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Pre-trained ChangeFormer not found at {changeformer_path}\")\n",
    "    print(\"Creating new ChangeFormer model...\")\n",
    "    changeformer_model = ChangeFormer().to(device)\n",
    "    print(\"‚úÖ New ChangeFormer created (will need training)\")\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in changeformer_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Threshold Calibration with Cross-Validation\n",
    "\n",
    "**Problem**: Fixed thresholds don't work well across different scenarios\n",
    "**Solution**: Cross-validation to find optimal thresholds for F1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Threshold calibration with cross-validation\n",
    "class ThresholdCalibrator:\n",
    "    \"\"\"\n",
    "    Calibrate thresholds using cross-validation for optimal F1 scores\n",
    "    \"\"\"\n",
    "    def __init__(self, n_splits=5):\n",
    "        self.n_splits = n_splits\n",
    "        self.best_thresholds = {\n",
    "            'iou_match': 0.5,\n",
    "            'change_score': 0.3,\n",
    "            'detection_conf': 0.1\n",
    "        }\n",
    "    \n",
    "    def calibrate_on_validation_set(self, val_df, vocab_terms, max_samples=50):\n",
    "        \"\"\"\n",
    "        Calibrate thresholds using validation data\n",
    "        \"\"\"\n",
    "        print(f\"üéØ Calibrating thresholds using {min(len(val_df), max_samples)} validation samples...\")\n",
    "        \n",
    "        # Use subset for speed\n",
    "        val_subset = val_df.sample(min(len(val_df), max_samples), random_state=42)\n",
    "        \n",
    "        # Threshold ranges to test\n",
    "        iou_thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "        change_thresholds = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "        conf_thresholds = [0.05, 0.1, 0.15, 0.2, 0.25]\n",
    "        \n",
    "        best_f1 = 0\n",
    "        best_thresholds = self.best_thresholds.copy()\n",
    "        \n",
    "        # Grid search\n",
    "        for iou_thresh in iou_thresholds:\n",
    "            for change_thresh in change_thresholds:\n",
    "                for conf_thresh in conf_thresholds:\n",
    "                    f1_scores = []\n",
    "                    \n",
    "                    for idx, row in val_subset.iterrows():\n",
    "                        img_id = row['img_id']\n",
    "                        \n",
    "                        # Get predictions with current thresholds\n",
    "                        pred_result = self._predict_with_thresholds(\n",
    "                            img_id, vocab_terms, iou_thresh, change_thresh, conf_thresh\n",
    "                        )\n",
    "                        \n",
    "                        # Calculate F1 for this sample\n",
    "                        true_added = set(self._normalize_labels(row['added_objs']))\n",
    "                        true_removed = set(self._normalize_labels(row['removed_objs']))\n",
    "                        true_changed = set(self._normalize_labels(row['changed_objs']))\n",
    "                        \n",
    "                        pred_added = set(pred_result['added'])\n",
    "                        pred_removed = set(pred_result['removed'])\n",
    "                        pred_changed = set(pred_result['changed'])\n",
    "                        \n",
    "                        # Calculate F1 for each category\n",
    "                        f1_added = self._calculate_f1(true_added, pred_added)\n",
    "                        f1_removed = self._calculate_f1(true_removed, pred_removed)\n",
    "                        f1_changed = self._calculate_f1(true_changed, pred_changed)\n",
    "                        \n",
    "                        # Average F1\n",
    "                        avg_f1 = (f1_added + f1_removed + f1_changed) / 3\n",
    "                        f1_scores.append(avg_f1)\n",
    "                    \n",
    "                    # Average F1 across samples\n",
    "                    mean_f1 = np.mean(f1_scores)\n",
    "                    \n",
    "                    if mean_f1 > best_f1:\n",
    "                        best_f1 = mean_f1\n",
    "                        best_thresholds = {\n",
    "                            'iou_match': iou_thresh,\n",
    "                            'change_score': change_thresh,\n",
    "                            'detection_conf': conf_thresh\n",
    "                        }\n",
    "                        print(f\"üÜï New best F1: {best_f1:.4f} with thresholds: {best_thresholds}\")\n",
    "        \n",
    "        self.best_thresholds = best_thresholds\n",
    "        print(f\"\\n‚úÖ Calibration complete! Best F1: {best_f1:.4f}\")\n",
    "        print(f\"Optimal thresholds: {self.best_thresholds}\")\n",
    "        \n",
    "        return self.best_thresholds\n",
    "    \n",
    "    def _predict_with_thresholds(self, img_id, vocab_terms, iou_thresh, change_thresh, conf_thresh):\n",
    "        \"\"\"Make prediction with specific thresholds\"\"\"\n",
    "        img1_path = os.path.join(data_dir, 'data/data', f'{img_id}_1.png')\n",
    "        img2_path = os.path.join(data_dir, 'data/data', f'{img_id}_2.png')\n",
    "        \n",
    "        # Detect objects\n",
    "        boxes1, scores1, labels1, terms1 = detect_ensemble_advanced(img1_path, vocab_terms)\n",
    "        boxes2, scores2, labels2, terms2 = detect_ensemble_advanced(img2_path, vocab_terms)\n",
    "        \n",
    "        # Filter by confidence\n",
    "        if len(scores1) > 0:\n",
    "            keep1 = scores1 >= conf_thresh\n",
    "            boxes1, scores1, labels1, terms1 = boxes1[keep1], scores1[keep1], labels1[keep1], [terms1[i] for i in range(len(terms1)) if keep1[i]]\n",
    "        \n",
    "        if len(scores2) > 0:\n",
    "            keep2 = scores2 >= conf_thresh\n",
    "            boxes2, scores2, labels2, terms2 = boxes2[keep2], scores2[keep2], labels2[keep2], [terms2[i] for i in range(len(terms2)) if keep2[i]]\n",
    "        \n",
    "        # Change detection\n",
    "        transform = T.Compose([\n",
    "            T.Resize((224, 224)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        img1_tensor = transform(Image.open(img1_path).convert('RGB')).unsqueeze(0).to(device)\n",
    "        img2_tensor = transform(Image.open(img2_path).convert('RGB')).unsqueeze(0).to(device)\n",
    "        \n",
    "        changeformer_model.eval()\n",
    "        with torch.no_grad():\n",
    "            change_score = torch.sigmoid(changeformer_model(img1_tensor, img2_tensor)).item()\n",
    "        \n",
    "        # If no significant change, return empty\n",
    "        if change_score < change_thresh:\n",
    "            return {'added': [], 'removed': [], 'changed': [], 'change_score': change_score}\n",
    "        \n",
    "        # Object matching with calibrated IoU threshold\n",
    "        matched_pairs = self._match_objects(boxes1, labels1, boxes2, labels2, iou_thresh)\n",
    "        matched_set = set(matched_pairs)\n",
    "        \n",
    "        added = [vocab_terms[int(labels2[j])] for j in range(len(labels2)) \n",
    "                if all((i, j) not in matched_set for i in range(len(labels1)))]\n",
    "        \n",
    "        removed = [vocab_terms[int(labels1[i])] for i in range(len(labels1)) \n",
    "                  if all((i, j) not in matched_set for j in range(len(labels2)))]\n",
    "        \n",
    "        changed = [vocab_terms[int(labels1[i])] for i, j in matched_pairs \n",
    "                  if self._compute_iou(boxes1[i], boxes2[j]) < iou_thresh]\n",
    "        \n",
    "        return {\n",
    "            'added': list(set(added)), \n",
    "            'removed': list(set(removed)), \n",
    "            'changed': list(set(changed)), \n",
    "            'change_score': change_score\n",
    "        }\n",
    "    \n",
    "    def _match_objects(self, boxes1, labels1, boxes2, labels2, iou_thresh):\n",
    "        \"\"\"Match objects using Hungarian algorithm\"\"\"\n",
    "        if len(boxes1) == 0 or len(boxes2) == 0:\n",
    "            return []\n",
    "        \n",
    "        from scipy.optimize import linear_sum_assignment\n",
    "        cost_matrix = np.ones((len(boxes1), len(boxes2)))\n",
    "        \n",
    "        for i in range(len(boxes1)):\n",
    "            for j in range(len(boxes2)):\n",
    "                if labels1[i] == labels2[j]:\n",
    "                    cost_matrix[i, j] = 1 - self._compute_iou(boxes1[i], boxes2[j])\n",
    "        \n",
    "        row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "        matched_pairs = [(i, j) for i, j in zip(row_ind, col_ind) if cost_matrix[i, j] < (1 - iou_thresh)]\n",
    "        \n",
    "        return matched_pairs\n",
    "    \n",
    "    def _compute_iou(self, boxA, boxB):\n",
    "        \"\"\"Compute IoU\"\"\"\n",
    "        xA = max(boxA[0], boxB[0])\n",
    "        yA = max(boxA[1], boxB[1])\n",
    "        xB = min(boxA[2], boxB[2])\n",
    "        yB = min(boxA[3], boxB[3])\n",
    "        interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "        boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "        boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "        if boxAArea + boxBArea - interArea == 0:\n",
    "            return 0\n",
    "        return interArea / float(boxAArea + boxBArea - interArea)\n",
    "    \n",
    "    def _normalize_labels(self, label_str):\n",
    "        \"\"\"Normalize labels for comparison\"\"\"\n",
    "        if pd.isna(label_str) or label_str.strip() == '' or label_str.strip() == 'none':\n",
    "            return []\n",
    "        tokens = re.split(r'[,&\\s]+', label_str.strip().lower())\n",
    "        return [token.strip() for token in tokens if token.strip() and token != 'none']\n",
    "    \n",
    "    def _calculate_f1(self, true_set, pred_set):\n",
    "        \"\"\"Calculate F1 score for a set comparison\"\"\"\n",
    "        if len(true_set) == 0 and len(pred_set) == 0:\n",
    "            return 1.0\n",
    "        if len(true_set) == 0 or len(pred_set) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        true_pos = len(true_set & pred_set)\n",
    "        false_pos = len(pred_set - true_set)\n",
    "        false_neg = len(true_set - pred_set)\n",
    "        \n",
    "        if true_pos + false_pos == 0:\n",
    "            precision = 0\n",
    "        else:\n",
    "            precision = true_pos / (true_pos + false_pos)\n",
    "        \n",
    "        if true_pos + false_neg == 0:\n",
    "            recall = 0\n",
    "        else:\n",
    "            recall = true_pos / (true_pos + false_neg)\n",
    "        \n",
    "        if precision + recall == 0:\n",
    "            return 0\n",
    "        return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "# Initialize calibrator\n",
    "calibrator = ThresholdCalibrator(n_splits=5)\n",
    "\n",
    "print(\"‚úÖ Threshold calibrator ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Complete Ultra-Advanced Pipeline\n",
    "\n",
    "**Integration**: Enhanced detection + calibrated thresholds + ChangeFormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Complete ultra-advanced pipeline\n",
    "def ultra_advanced_pipeline(img_id, vocab_terms):\n",
    "    \"\"\"\n",
    "    Complete ultra-advanced pipeline with all improvements:\n",
    "    1. Enhanced image preprocessing\n",
    "    2. Ensemble detection (Grounding DINO + OWL-ViT)\n",
    "    3. Rich vocabulary with synonyms\n",
    "    4. Calibrated thresholds\n",
    "    5. ChangeFormer for change localization\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    img1_path = os.path.join(data_dir, 'data/data', f'{img_id}_1.png')\n",
    "    img2_path = os.path.join(data_dir, 'data/data', f'{img_id}_2.png')\n",
    "    \n",
    "    # Step 1: Enhanced object detection\n",
    "    print(f\"  1Ô∏è‚É£ Enhanced ensemble detection...\")\n",
    "    boxes1, scores1, labels1, terms1 = detect_ensemble_advanced(img1_path, vocab_terms, use_enhancement=True)\n",
    "    boxes2, scores2, labels2, terms2 = detect_ensemble_advanced(img2_path, vocab_terms, use_enhancement=True)\n",
    "\n",
    "    print(f\"     Image 1: {len(terms1)} objects detected\")\n",
    "    print(f\"     Image 2: {len(terms2)} objects detected\")\n",
    "    \n",
    "    # Step 2: Filter by calibrated confidence threshold\n",
    "    conf_thresh = calibrator.best_thresholds['detection_conf']\n",
    "    \n",
    "    # Ensure all outputs are arrays (handle single detection case)\n",
    "    boxes1 = np.atleast_2d(boxes1)\n",
    "    scores1 = np.atleast_1d(scores1)\n",
    "    labels1 = np.atleast_1d(labels1)\n",
    "    \n",
    "    boxes2 = np.atleast_2d(boxes2)\n",
    "    scores2 = np.atleast_1d(scores2)\n",
    "    labels2 = np.atleast_1d(labels2)\n",
    "    \n",
    "    # For Image 1:\n",
    "    if len(scores1) > 0:\n",
    "        keep1 = scores1 >= conf_thresh\n",
    "        \n",
    "        # Filter numpy arrays\n",
    "        boxes1 = boxes1[keep1]\n",
    "        scores1 = scores1[keep1]\n",
    "        labels1 = labels1[keep1]\n",
    "        \n",
    "        # Filter Python list\n",
    "        keep1_indices = np.where(keep1)[0]\n",
    "        terms1 = [terms1[i] for i in keep1_indices]\n",
    "\n",
    "    # For Image 2:\n",
    "    if len(scores2) > 0:\n",
    "        keep2 = scores2 >= conf_thresh\n",
    "        \n",
    "        # Filter numpy arrays\n",
    "        boxes2 = boxes2[keep2]\n",
    "        scores2 = scores2[keep2]\n",
    "        labels2 = labels2[keep2]\n",
    "        \n",
    "        # Filter Python list\n",
    "        keep2_indices = np.where(keep2)[0]\n",
    "        terms2 = [terms2[i] for i in keep2_indices]\n",
    "\n",
    "    print(f\"     After confidence filtering: {len(terms1)} / {len(terms2)} objects\")\n",
    "    \n",
    "    # Step 3: ChangeFormer change detection\n",
    "    print(f\"  2Ô∏è‚É£ ChangeFormer analysis...\")\n",
    "    transform = T.Compose([\n",
    "        T.Resize((224, 224)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    img1_tensor = transform(Image.open(img1_path).convert('RGB')).unsqueeze(0).to(device)\n",
    "    img2_tensor = transform(Image.open(img2_path).convert('RGB')).unsqueeze(0).to(device)\n",
    "    \n",
    "    changeformer_model.eval()\n",
    "    with torch.no_grad():\n",
    "        change_score = torch.sigmoid(changeformer_model(img1_tensor, img2_tensor)).item()\n",
    "    \n",
    "    print(f\"     Change score: {change_score:.4f}\")\n",
    "    \n",
    "    # Step 4: Check if significant change detected\n",
    "    change_thresh = calibrator.best_thresholds['change_score']\n",
    "    if change_score < change_thresh:\n",
    "        print(f\"     No significant change detected (threshold: {change_thresh:.2f})\")\n",
    "        return {\n",
    "            'added': [], 'removed': [], 'changed': [], \n",
    "            'change_score': change_score,\n",
    "            'objects_img1': terms1, 'objects_img2': terms2\n",
    "        }\n",
    "    \n",
    "    # Step 5: Object matching with calibrated IoU threshold\n",
    "    print(f\"  3Ô∏è‚É£ Object matching...\")\n",
    "    iou_thresh = calibrator.best_thresholds['iou_match']\n",
    "    \n",
    "    matched_pairs = calibrator._match_objects(boxes1, labels1, boxes2, labels2, iou_thresh)\n",
    "    matched_set = set(matched_pairs)\n",
    "    \n",
    "    print(f\"     Matched pairs: {len(matched_pairs)}\")\n",
    "    \n",
    "    # Step 6: Classify changes\n",
    "    added = [vocab_terms[int(labels2[j])] for j in range(len(labels2)) \n",
    "            if all((i, j) not in matched_set for i in range(len(labels1)))]\n",
    "    \n",
    "    removed = [vocab_terms[int(labels1[i])] for i in range(len(labels1)) \n",
    "              if all((i, j) not in matched_set for j in range(len(labels2)))]\n",
    "    \n",
    "    changed = [vocab_terms[int(labels1[i])] for i, j in matched_pairs \n",
    "              if calibrator._compute_iou(boxes1[i], boxes2[j]) < iou_thresh]\n",
    "    \n",
    "    result = {\n",
    "        'added': list(set(added)),\n",
    "        'removed': list(set(removed)),\n",
    "        'changed': list(set(changed)),\n",
    "        'change_score': change_score,\n",
    "        'objects_img1': terms1,\n",
    "        'objects_img2': terms2,\n",
    "        'matched_pairs': len(matched_pairs)\n",
    "    }\n",
    "    \n",
    "    print(f\"     Added: {result['added']}\")\n",
    "    print(f\"     Removed: {result['removed']}\")\n",
    "    print(f\"     Changed: {result['changed']}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"‚úÖ Ultra-advanced pipeline ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Calibration & Testing\n",
    "\n",
    "**First**: Calibrate thresholds using validation data\n",
    "**Then**: Test on validation samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Calibrate thresholds using validation data\n",
    "print(\"üéØ Starting threshold calibration...\")\n",
    "print(\"This may take several minutes...\")\n",
    "\n",
    "# Use a subset of training data as validation for calibration\n",
    "val_df = train_df.sample(30, random_state=42)  # Small subset for speed\n",
    "\n",
    "optimal_thresholds = calibrator.calibrate_on_validation_set(\n",
    "    val_df, base_vocabulary, max_samples=20\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Calibration complete!\")\n",
    "print(f\"Optimal thresholds: {optimal_thresholds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Test ultra-advanced pipeline on validation samples\n",
    "print(\"\\nüß™ Testing ultra-advanced pipeline on validation samples...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_samples = train_df.sample(5, random_state=123)\n",
    "\n",
    "print(f\"\\nüöÄ Processing image {img_id} with ultra-advanced pipeline...\")\n",
    "for idx, row in test_samples.iterrows():\n",
    "    img_id = row['img_id']\n",
    "    \n",
    "    print(f\"\\nüì∑ Image: {img_id}\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    # Ground truth\n",
    "    print(\"Ground Truth:\")\n",
    "    print(f\"  Added: {row['added_objs']}\")\n",
    "    print(f\"  Removed: {row['removed_objs']}\")\n",
    "    print(f\"  Changed: {row['changed_objs']}\")\n",
    "    \n",
    "    # Predictions\n",
    "    result = ultra_advanced_pipeline(img_id, base_vocabulary)\n",
    "    print(f\"\\nPredictions (Ultra-Advanced Pipeline):\")\n",
    "    print(f\"  Added: {result['added']}\")\n",
    "    print(f\"  Removed: {result['removed']}\")\n",
    "    print(f\"  Changed: {result['changed']}\")\n",
    "    print(f\"  Change score: {result['change_score']:.4f}\")\n",
    "    print(f\"  Objects detected: {len(result['objects_img1'])} / {len(result['objects_img2'])}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generate Final Submission\n",
    "\n",
    "**Using calibrated thresholds and ultra-advanced pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Generate predictions for test set\n",
    "print(\"üöÄ Generating final predictions for test set...\")\n",
    "print(\"Using ultra-advanced pipeline with calibrated thresholds\")\n",
    "\n",
    "submission = []\n",
    "for img_id in tqdm(test_df['img_id'], desc='Processing test images'):\n",
    "    result = ultra_advanced_pipeline(img_id, base_vocabulary)\n",
    "    \n",
    "    added = 'none' if not result['added'] else ' '.join(result['added'])\n",
    "    removed = 'none' if not result['removed'] else ' '.join(result['removed'])\n",
    "    changed = 'none' if not result['changed'] else ' '.join(result['changed'])\n",
    "    \n",
    "    submission.append({\n",
    "        'img_id': img_id,\n",
    "        'added_objs': added,\n",
    "        'removed_objs': removed,\n",
    "        'changed_objs': changed\n",
    "    })\n",
    "\n",
    "submission_df = pd.DataFrame(submission)\n",
    "submission_path = 'submission_ultra_advanced.csv'\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Final submission saved to {submission_path}\")\n",
    "print(f\"Total predictions: {len(submission_df)}\")\n",
    "display(submission_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Comprehensive Error Analysis\n",
    "\n",
    "**Goal**: Understand pipeline performance, identify failure modes, and find improvement opportunities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Error Analysis on Validation Set\n",
    "print(\"üîç COMPREHENSIVE ERROR ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Select a larger validation set for thorough analysis\n",
    "error_analysis_df = train_df.sample(min(50, len(train_df)), random_state=42)\n",
    "\n",
    "# Storage for analysis\n",
    "error_results = []\n",
    "detection_stats = {\n",
    "    'total_samples': 0,\n",
    "    'perfect_matches': 0,\n",
    "    'partial_matches': 0,\n",
    "    'complete_misses': 0,\n",
    "    'false_positives': 0,\n",
    "    'false_negatives': 0,\n",
    "    'category_errors': {'added': [], 'removed': [], 'changed': []}\n",
    "}\n",
    "\n",
    "print(f\"\\nüìä Analyzing {len(error_analysis_df)} validation samples...\")\n",
    "print(\"This may take several minutes...\\n\")\n",
    "\n",
    "for idx, row in tqdm(error_analysis_df.iterrows(), total=len(error_analysis_df), desc=\"Error analysis\"):\n",
    "    img_id = row['img_id']\n",
    "    \n",
    "    try:\n",
    "        # Get predictions\n",
    "        result = ultra_advanced_pipeline(img_id, base_vocabulary)\n",
    "        \n",
    "        # Ground truth (normalized)\n",
    "        true_added = set(calibrator._normalize_labels(row['added_objs']))\n",
    "        true_removed = set(calibrator._normalize_labels(row['removed_objs']))\n",
    "        true_changed = set(calibrator._normalize_labels(row['changed_objs']))\n",
    "        \n",
    "        # Predictions\n",
    "        pred_added = set(result['added'])\n",
    "        pred_removed = set(result['removed'])\n",
    "        pred_changed = set(result['changed'])\n",
    "        \n",
    "        # Calculate metrics per category\n",
    "        def calculate_metrics(true_set, pred_set):\n",
    "            tp = len(true_set & pred_set)\n",
    "            fp = len(pred_set - true_set)\n",
    "            fn = len(true_set - pred_set)\n",
    "            \n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            \n",
    "            return {\n",
    "                'tp': tp, 'fp': fp, 'fn': fn,\n",
    "                'precision': precision, 'recall': recall, 'f1': f1\n",
    "            }\n",
    "        \n",
    "        metrics_added = calculate_metrics(true_added, pred_added)\n",
    "        metrics_removed = calculate_metrics(true_removed, pred_removed)\n",
    "        metrics_changed = calculate_metrics(true_changed, pred_changed)\n",
    "        \n",
    "        # Overall F1\n",
    "        avg_f1 = (metrics_added['f1'] + metrics_removed['f1'] + metrics_changed['f1']) / 3\n",
    "        \n",
    "        # Classify result\n",
    "        if avg_f1 == 1.0:\n",
    "            result_type = 'perfect'\n",
    "            detection_stats['perfect_matches'] += 1\n",
    "        elif avg_f1 >= 0.5:\n",
    "            result_type = 'partial'\n",
    "            detection_stats['partial_matches'] += 1\n",
    "        else:\n",
    "            result_type = 'miss'\n",
    "            detection_stats['complete_misses'] += 1\n",
    "        \n",
    "        # Track errors by category\n",
    "        detection_stats['category_errors']['added'].append(metrics_added['f1'])\n",
    "        detection_stats['category_errors']['removed'].append(metrics_removed['f1'])\n",
    "        detection_stats['category_errors']['changed'].append(metrics_changed['f1'])\n",
    "        \n",
    "        # Store detailed error info\n",
    "        error_results.append({\n",
    "            'img_id': img_id,\n",
    "            'result_type': result_type,\n",
    "            'avg_f1': avg_f1,\n",
    "            'change_score': result['change_score'],\n",
    "            # Ground truth\n",
    "            'true_added': true_added,\n",
    "            'true_removed': true_removed,\n",
    "            'true_changed': true_changed,\n",
    "            # Predictions\n",
    "            'pred_added': pred_added,\n",
    "            'pred_removed': pred_removed,\n",
    "            'pred_changed': pred_changed,\n",
    "            # Metrics\n",
    "            'f1_added': metrics_added['f1'],\n",
    "            'f1_removed': metrics_removed['f1'],\n",
    "            'f1_changed': metrics_changed['f1'],\n",
    "            'tp_added': metrics_added['tp'],\n",
    "            'fp_added': metrics_added['fp'],\n",
    "            'fn_added': metrics_added['fn'],\n",
    "            'tp_removed': metrics_removed['tp'],\n",
    "            'fp_removed': metrics_removed['fp'],\n",
    "            'fn_removed': metrics_removed['fn'],\n",
    "            'tp_changed': metrics_changed['tp'],\n",
    "            'fp_changed': metrics_changed['fp'],\n",
    "            'fn_changed': metrics_changed['fn'],\n",
    "            # Detection info\n",
    "            'num_objects_img1': len(result['objects_img1']),\n",
    "            'num_objects_img2': len(result['objects_img2']),\n",
    "            'matched_pairs': result['matched_pairs']\n",
    "        })\n",
    "        \n",
    "        detection_stats['total_samples'] += 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {img_id}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "error_df = pd.DataFrame(error_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìà ERROR ANALYSIS RESULTS\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 1. Overall Performance Summary\n",
    "print(\"\\n1Ô∏è‚É£ OVERALL PERFORMANCE SUMMARY\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "total = detection_stats['total_samples']\n",
    "if total > 0:\n",
    "    print(f\"Total samples analyzed: {total}\")\n",
    "    print(f\"\\nPerformance breakdown:\")\n",
    "    print(f\"  ‚úÖ Perfect matches (F1=1.0): {detection_stats['perfect_matches']} ({100*detection_stats['perfect_matches']/total:.1f}%)\")\n",
    "    print(f\"  ‚ö†Ô∏è Partial matches (F1‚â•0.5): {detection_stats['partial_matches']} ({100*detection_stats['partial_matches']/total:.1f}%)\")\n",
    "    print(f\"  ‚ùå Complete misses (F1<0.5): {detection_stats['complete_misses']} ({100*detection_stats['complete_misses']/total:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nAverage F1 scores by category:\")\n",
    "    for category in ['added', 'removed', 'changed']:\n",
    "        scores = detection_stats['category_errors'][category]\n",
    "        if scores:\n",
    "            avg_f1 = np.mean(scores)\n",
    "            print(f\"  {category.capitalize()}: {avg_f1:.3f}\")\n",
    "    \n",
    "    print(f\"\\nOverall average F1: {error_df['avg_f1'].mean():.3f}\")\n",
    "    print(f\"Median F1: {error_df['avg_f1'].median():.3f}\")\n",
    "    print(f\"Std deviation: {error_df['avg_f1'].std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 2. Category-wise Error Analysis\n",
    "print(\"\\n2Ô∏è‚É£ CATEGORY-WISE ERROR BREAKDOWN\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "categories = ['added', 'removed', 'changed']\n",
    "for cat in categories:\n",
    "    print(f\"\\n{cat.upper()} Objects:\")\n",
    "    tp = error_df[f'tp_{cat}'].sum()\n",
    "    fp = error_df[f'fp_{cat}'].sum()\n",
    "    fn = error_df[f'fn_{cat}'].sum()\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    print(f\"  True Positives: {tp}\")\n",
    "    print(f\"  False Positives: {fp}\")\n",
    "    print(f\"  False Negatives: {fn}\")\n",
    "    print(f\"  Precision: {precision:.3f}\")\n",
    "    print(f\"  Recall: {recall:.3f}\")\n",
    "    print(f\"  F1 Score: {f1:.3f}\")\n",
    "    \n",
    "    if fp > 0:\n",
    "        print(f\"  ‚ö†Ô∏è False positive rate: {fp/(tp+fp):.1%}\")\n",
    "    if fn > 0:\n",
    "        print(f\"  ‚ö†Ô∏è False negative rate: {fn/(tp+fn):.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 3. Detection Quality Analysis\n",
    "print(\"\\n3Ô∏è‚É£ OBJECT DETECTION QUALITY\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "print(f\"Average objects detected per image:\")\n",
    "print(f\"  Image 1: {error_df['num_objects_img1'].mean():.1f} objects\")\n",
    "print(f\"  Image 2: {error_df['num_objects_img2'].mean():.1f} objects\")\n",
    "print(f\"  Average matched pairs: {error_df['matched_pairs'].mean():.1f}\")\n",
    "\n",
    "print(f\"\\nDetection distribution:\")\n",
    "print(f\"  Images with 0 objects: {len(error_df[(error_df['num_objects_img1']==0) | (error_df['num_objects_img2']==0)])}\")\n",
    "print(f\"  Images with 1-5 objects: {len(error_df[((error_df['num_objects_img1']>=1) & (error_df['num_objects_img1']<=5)) | ((error_df['num_objects_img2']>=1) & (error_df['num_objects_img2']<=5))])}\")\n",
    "print(f\"  Images with 6+ objects: {len(error_df[(error_df['num_objects_img1']>5) | (error_df['num_objects_img2']>5)])}\")\n",
    "\n",
    "# Change score analysis\n",
    "print(f\"\\nChange score statistics:\")\n",
    "print(f\"  Mean: {error_df['change_score'].mean():.3f}\")\n",
    "print(f\"  Median: {error_df['change_score'].median():.3f}\")\n",
    "print(f\"  Min: {error_df['change_score'].min():.3f}\")\n",
    "print(f\"  Max: {error_df['change_score'].max():.3f}\")\n",
    "\n",
    "# Correlation between change score and F1\n",
    "correlation = error_df[['change_score', 'avg_f1']].corr().iloc[0, 1]\n",
    "print(f\"  Correlation with F1 score: {correlation:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 4. Failure Mode Analysis\n",
    "print(\"\\n4Ô∏è‚É£ FAILURE MODE ANALYSIS\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Identify worst performing samples\n",
    "worst_samples = error_df.nsmallest(10, 'avg_f1')\n",
    "print(f\"\\nTop 10 worst performing samples:\")\n",
    "print(f\"{'Img ID':<12} {'F1':<8} {'Change':<8} {'Added':<8} {'Removed':<8} {'Changed':<8}\")\n",
    "print(\"-\"*60)\n",
    "for _, row in worst_samples.iterrows():\n",
    "    print(f\"{row['img_id']:<12} {row['avg_f1']:<8.3f} {row['change_score']:<8.3f} \"\n",
    "          f\"{row['f1_added']:<8.3f} {row['f1_removed']:<8.3f} {row['f1_changed']:<8.3f}\")\n",
    "\n",
    "# Identify common failure patterns\n",
    "print(\"\\n‚ö†Ô∏è Common failure patterns:\")\n",
    "\n",
    "# Pattern 1: High false positives\n",
    "high_fp_added = error_df[error_df['fp_added'] >= 2]\n",
    "high_fp_removed = error_df[error_df['fp_removed'] >= 2]\n",
    "high_fp_changed = error_df[error_df['fp_changed'] >= 2]\n",
    "print(f\"  ‚Ä¢ High false positives in 'added': {len(high_fp_added)} samples\")\n",
    "print(f\"  ‚Ä¢ High false positives in 'removed': {len(high_fp_removed)} samples\")\n",
    "print(f\"  ‚Ä¢ High false positives in 'changed': {len(high_fp_changed)} samples\")\n",
    "\n",
    "# Pattern 2: High false negatives\n",
    "high_fn_added = error_df[error_df['fn_added'] >= 2]\n",
    "high_fn_removed = error_df[error_df['fn_removed'] >= 2]\n",
    "high_fn_changed = error_df[error_df['fn_changed'] >= 2]\n",
    "print(f\"  ‚Ä¢ High false negatives in 'added': {len(high_fn_added)} samples\")\n",
    "print(f\"  ‚Ä¢ High false negatives in 'removed': {len(high_fn_removed)} samples\")\n",
    "print(f\"  ‚Ä¢ High false negatives in 'changed': {len(high_fn_changed)} samples\")\n",
    "\n",
    "# Pattern 3: Low change score but has changes\n",
    "low_change_but_has_changes = error_df[\n",
    "    (error_df['change_score'] < calibrator.best_thresholds['change_score']) &\n",
    "    ((error_df['true_added'].apply(len) > 0) | \n",
    "     (error_df['true_removed'].apply(len) > 0) | \n",
    "     (error_df['true_changed'].apply(len) > 0))\n",
    "]\n",
    "print(f\"  ‚Ä¢ Low change score but has actual changes: {len(low_change_but_has_changes)} samples\")\n",
    "\n",
    "# Pattern 4: Few objects detected\n",
    "few_objects = error_df[(error_df['num_objects_img1'] <= 2) & (error_df['num_objects_img2'] <= 2)]\n",
    "print(f\"  ‚Ä¢ Very few objects detected (‚â§2): {len(few_objects)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 5. Detailed Case Studies\n",
    "print(\"\\n5Ô∏è‚É£ DETAILED CASE STUDIES\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Case Study 1: Perfect Match\n",
    "perfect = error_df[error_df['avg_f1'] == 1.0].head(1)\n",
    "if len(perfect) > 0:\n",
    "    row = perfect.iloc[0]\n",
    "    print(f\"\\n‚úÖ PERFECT MATCH EXAMPLE: {row['img_id']}\")\n",
    "    print(f\"   F1 Score: {row['avg_f1']:.3f}\")\n",
    "    print(f\"   Change score: {row['change_score']:.3f}\")\n",
    "    print(f\"   True added: {row['true_added']}\")\n",
    "    print(f\"   Pred added: {row['pred_added']}\")\n",
    "    print(f\"   True removed: {row['true_removed']}\")\n",
    "    print(f\"   Pred removed: {row['pred_removed']}\")\n",
    "    print(f\"   True changed: {row['true_changed']}\")\n",
    "    print(f\"   Pred changed: {row['pred_changed']}\")\n",
    "\n",
    "# Case Study 2: Complete Miss\n",
    "miss = error_df[error_df['result_type'] == 'miss'].head(1)\n",
    "if len(miss) > 0:\n",
    "    row = miss.iloc[0]\n",
    "    print(f\"\\n‚ùå COMPLETE MISS EXAMPLE: {row['img_id']}\")\n",
    "    print(f\"   F1 Score: {row['avg_f1']:.3f}\")\n",
    "    print(f\"   Change score: {row['change_score']:.3f}\")\n",
    "    print(f\"   True added: {row['true_added']}\")\n",
    "    print(f\"   Pred added: {row['pred_added']}\")\n",
    "    print(f\"   True removed: {row['true_removed']}\")\n",
    "    print(f\"   Pred removed: {row['pred_removed']}\")\n",
    "    print(f\"   True changed: {row['true_changed']}\")\n",
    "    print(f\"   Pred changed: {row['pred_changed']}\")\n",
    "    print(f\"   Objects detected: img1={row['num_objects_img1']}, img2={row['num_objects_img2']}\")\n",
    "    print(f\"   Matched pairs: {row['matched_pairs']}\")\n",
    "\n",
    "# Case Study 3: High False Positives\n",
    "high_fp = error_df[error_df['fp_added'] + error_df['fp_removed'] + error_df['fp_changed'] >= 3].head(1)\n",
    "if len(high_fp) > 0:\n",
    "    row = high_fp.iloc[0]\n",
    "    print(f\"\\n‚ö†Ô∏è HIGH FALSE POSITIVES EXAMPLE: {row['img_id']}\")\n",
    "    print(f\"   F1 Score: {row['avg_f1']:.3f}\")\n",
    "    print(f\"   Total FP: {row['fp_added'] + row['fp_removed'] + row['fp_changed']}\")\n",
    "    print(f\"   FP in added: {row['fp_added']} - Predicted: {row['pred_added'] - row['true_added']}\")\n",
    "    print(f\"   FP in removed: {row['fp_removed']} - Predicted: {row['pred_removed'] - row['true_removed']}\")\n",
    "    print(f\"   FP in changed: {row['fp_changed']} - Predicted: {row['pred_changed'] - row['true_changed']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 6. Visualization of Error Distribution\n",
    "print(\"\\n6Ô∏è‚É£ ERROR DISTRIBUTION VISUALIZATIONS\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Plot 1: F1 Score Distribution\n",
    "axes[0, 0].hist(error_df['avg_f1'], bins=20, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_title('F1 Score Distribution')\n",
    "axes[0, 0].set_xlabel('F1 Score')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].axvline(error_df['avg_f1'].mean(), color='red', linestyle='--', label=f'Mean: {error_df[\"avg_f1\"].mean():.3f}')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Plot 2: Category F1 Comparison\n",
    "category_f1s = [error_df['f1_added'].mean(), error_df['f1_removed'].mean(), error_df['f1_changed'].mean()]\n",
    "axes[0, 1].bar(['Added', 'Removed', 'Changed'], category_f1s, alpha=0.7, color=['green', 'red', 'blue'])\n",
    "axes[0, 1].set_title('Average F1 by Category')\n",
    "axes[0, 1].set_ylabel('F1 Score')\n",
    "axes[0, 1].set_ylim(0, 1)\n",
    "for i, v in enumerate(category_f1s):\n",
    "    axes[0, 1].text(i, v + 0.02, f'{v:.3f}', ha='center')\n",
    "\n",
    "# Plot 3: Change Score vs F1\n",
    "axes[0, 2].scatter(error_df['change_score'], error_df['avg_f1'], alpha=0.5)\n",
    "axes[0, 2].set_title('Change Score vs F1')\n",
    "axes[0, 2].set_xlabel('Change Score')\n",
    "axes[0, 2].set_ylabel('F1 Score')\n",
    "axes[0, 2].axhline(y=0.5, color='r', linestyle='--', alpha=0.3, label='F1=0.5')\n",
    "axes[0, 2].axvline(x=calibrator.best_thresholds['change_score'], color='g', linestyle='--', alpha=0.3, label='Threshold')\n",
    "axes[0, 2].legend()\n",
    "\n",
    "# Plot 4: Objects Detected Distribution\n",
    "axes[1, 0].hist(error_df['num_objects_img1'], bins=15, alpha=0.5, label='Image 1', edgecolor='black')\n",
    "axes[1, 0].hist(error_df['num_objects_img2'], bins=15, alpha=0.5, label='Image 2', edgecolor='black')\n",
    "axes[1, 0].set_title('Objects Detected Distribution')\n",
    "axes[1, 0].set_xlabel('Number of Objects')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Plot 5: Error Type Breakdown\n",
    "error_types = ['TP', 'FP', 'FN']\n",
    "for cat_idx, cat in enumerate(['added', 'removed', 'changed']):\n",
    "    tp = error_df[f'tp_{cat}'].sum()\n",
    "    fp = error_df[f'fp_{cat}'].sum()\n",
    "    fn = error_df[f'fn_{cat}'].sum()\n",
    "    \n",
    "    axes[1, 1].bar([f'{cat}\\n{et}' for et in error_types], [tp, fp, fn], alpha=0.7)\n",
    "\n",
    "axes[1, 1].set_title('Error Type Breakdown by Category')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 6: Precision-Recall by Category\n",
    "precisions = []\n",
    "recalls = []\n",
    "for cat in ['added', 'removed', 'changed']:\n",
    "    tp = error_df[f'tp_{cat}'].sum()\n",
    "    fp = error_df[f'fp_{cat}'].sum()\n",
    "    fn = error_df[f'fn_{cat}'].sum()\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "\n",
    "x = np.arange(len(['Added', 'Removed', 'Changed']))\n",
    "width = 0.35\n",
    "axes[1, 2].bar(x - width/2, precisions, width, label='Precision', alpha=0.7)\n",
    "axes[1, 2].bar(x + width/2, recalls, width, label='Recall', alpha=0.7)\n",
    "axes[1, 2].set_title('Precision vs Recall by Category')\n",
    "axes[1, 2].set_ylabel('Score')\n",
    "axes[1, 2].set_xticks(x)\n",
    "axes[1, 2].set_xticklabels(['Added', 'Removed', 'Changed'])\n",
    "axes[1, 2].set_ylim(0, 1)\n",
    "axes[1, 2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualizations complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 7. Key Insights and Recommendations\n",
    "print(\"\\n7Ô∏è‚É£ KEY INSIGHTS & RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate insights\n",
    "avg_f1 = error_df['avg_f1'].mean()\n",
    "best_category = max(['added', 'removed', 'changed'], \n",
    "                   key=lambda x: error_df[f'f1_{x}'].mean())\n",
    "worst_category = min(['added', 'removed', 'changed'], \n",
    "                    key=lambda x: error_df[f'f1_{x}'].mean())\n",
    "\n",
    "total_fp = error_df['fp_added'].sum() + error_df['fp_removed'].sum() + error_df['fp_changed'].sum()\n",
    "total_fn = error_df['fn_added'].sum() + error_df['fn_removed'].sum() + error_df['fn_changed'].sum()\n",
    "\n",
    "print(\"\\nüìä PERFORMANCE SUMMARY:\")\n",
    "print(f\"  ‚Ä¢ Overall F1 Score: {avg_f1:.3f}\")\n",
    "print(f\"  ‚Ä¢ Best performing category: {best_category} (F1={error_df[f'f1_{best_category}'].mean():.3f})\")\n",
    "print(f\"  ‚Ä¢ Worst performing category: {worst_category} (F1={error_df[f'f1_{worst_category}'].mean():.3f})\")\n",
    "print(f\"  ‚Ä¢ Total False Positives: {total_fp}\")\n",
    "print(f\"  ‚Ä¢ Total False Negatives: {total_fn}\")\n",
    "\n",
    "if total_fp > total_fn:\n",
    "    print(f\"  ‚ö†Ô∏è Model is over-predicting (more FP than FN)\")\n",
    "elif total_fn > total_fp:\n",
    "    print(f\"  ‚ö†Ô∏è Model is under-predicting (more FN than FP)\")\n",
    "else:\n",
    "    print(f\"  ‚úÖ Model is well-balanced (FP ‚âà FN)\")\n",
    "\n",
    "print(\"\\nüîç KEY INSIGHTS:\")\n",
    "\n",
    "# Insight 1: Detection quality\n",
    "avg_objects = (error_df['num_objects_img1'].mean() + error_df['num_objects_img2'].mean()) / 2\n",
    "if avg_objects < 3:\n",
    "    print(f\"  1. Low object detection rate (avg {avg_objects:.1f} objects/image)\")\n",
    "    print(f\"     ‚Üí May need to lower detection threshold or use more aggressive prompts\")\n",
    "elif avg_objects > 10:\n",
    "    print(f\"  1. High object detection rate (avg {avg_objects:.1f} objects/image)\")\n",
    "    print(f\"     ‚Üí May need to raise detection threshold to reduce false positives\")\n",
    "else:\n",
    "    print(f\"  1. Reasonable object detection rate (avg {avg_objects:.1f} objects/image)\")\n",
    "\n",
    "# Insight 2: Change detection\n",
    "if correlation < 0.3:\n",
    "    print(f\"  2. Weak correlation between change score and F1 ({correlation:.3f})\")\n",
    "    print(f\"     ‚Üí ChangeFormer may need fine-tuning or threshold adjustment\")\n",
    "elif correlation > 0.6:\n",
    "    print(f\"  2. Strong correlation between change score and F1 ({correlation:.3f})\")\n",
    "    print(f\"     ‚Üí ChangeFormer is working well\")\n",
    "else:\n",
    "    print(f\"  2. Moderate correlation between change score and F1 ({correlation:.3f})\")\n",
    "\n",
    "# Insight 3: Category imbalance\n",
    "category_std = np.std([error_df[f'f1_{cat}'].mean() for cat in ['added', 'removed', 'changed']])\n",
    "if category_std > 0.2:\n",
    "    print(f\"  3. High variance across categories (std={category_std:.3f})\")\n",
    "    print(f\"     ‚Üí Consider category-specific thresholds or separate models\")\n",
    "else:\n",
    "    print(f\"  3. Consistent performance across categories (std={category_std:.3f})\")\n",
    "\n",
    "print(\"\\nüí° RECOMMENDATIONS FOR IMPROVEMENT:\")\n",
    "\n",
    "recommendations = []\n",
    "\n",
    "# Recommendation based on FP/FN ratio\n",
    "if total_fp > total_fn * 1.5:\n",
    "    recommendations.append(\"  1. REDUCE FALSE POSITIVES:\")\n",
    "    recommendations.append(\"     ‚Ä¢ Increase detection confidence threshold\")\n",
    "    recommendations.append(\"     ‚Ä¢ Increase IoU matching threshold\")\n",
    "    recommendations.append(\"     ‚Ä¢ Use stricter NMS (lower IoU threshold)\")\n",
    "elif total_fn > total_fp * 1.5:\n",
    "    recommendations.append(\"  1. REDUCE FALSE NEGATIVES:\")\n",
    "    recommendations.append(\"     ‚Ä¢ Lower detection confidence threshold\")\n",
    "    recommendations.append(\"     ‚Ä¢ Add more vocabulary terms/synonyms\")\n",
    "    recommendations.append(\"     ‚Ä¢ Try different detection models\")\n",
    "\n",
    "# Recommendation based on change score\n",
    "if len(low_change_but_has_changes) > len(error_analysis_df) * 0.2:\n",
    "    recommendations.append(\"  2. IMPROVE CHANGE DETECTION:\")\n",
    "    recommendations.append(\"     ‚Ä¢ Lower change score threshold\")\n",
    "    recommendations.append(\"     ‚Ä¢ Fine-tune ChangeFormer on your specific data\")\n",
    "    recommendations.append(\"     ‚Ä¢ Use ensemble of change detection models\")\n",
    "\n",
    "# Recommendation based on detection quality\n",
    "if avg_objects < 3:\n",
    "    recommendations.append(\"  3. IMPROVE OBJECT DETECTION:\")\n",
    "    recommendations.append(\"     ‚Ä¢ Use larger detection models (e.g., owlvit-large)\")\n",
    "    recommendations.append(\"     ‚Ä¢ Increase image enhancement strength\")\n",
    "    recommendations.append(\"     ‚Ä¢ Add more detection models to ensemble\")\n",
    "\n",
    "# Recommendation based on worst category\n",
    "if error_df[f'f1_{worst_category}'].mean() < 0.5:\n",
    "    recommendations.append(f\"  4. FOCUS ON '{worst_category.upper()}' CATEGORY:\")\n",
    "    recommendations.append(f\"     ‚Ä¢ Category-specific threshold tuning\")\n",
    "    recommendations.append(f\"     ‚Ä¢ Augment training data for {worst_category} scenarios\")\n",
    "    recommendations.append(f\"     ‚Ä¢ Review vocabulary coverage for {worst_category}\")\n",
    "\n",
    "for rec in recommendations:\n",
    "    print(rec)\n",
    "\n",
    "print(\"\\n‚úÖ Error analysis complete!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save error analysis results\n",
    "error_analysis_path = 'error_analysis_results.csv'\n",
    "error_df.to_csv(error_analysis_path, index=False)\n",
    "print(f\"\\nüíæ Error analysis saved to: {error_analysis_path}\")\n",
    "\n",
    "# Save detailed report\n",
    "report_path = 'error_analysis_report.txt'\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "    f.write(\"ERROR ANALYSIS REPORT\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(f\"Total samples: {total}\\n\")\n",
    "    f.write(f\"Overall F1: {avg_f1:.3f}\\n\")\n",
    "    f.write(f\"Perfect matches: {detection_stats['perfect_matches']} ({100*detection_stats['perfect_matches']/total:.1f}%)\\n\")\n",
    "    f.write(f\"Partial matches: {detection_stats['partial_matches']} ({100*detection_stats['partial_matches']/total:.1f}%)\\n\")\n",
    "    f.write(f\"Complete misses: {detection_stats['complete_misses']} ({100*detection_stats['complete_misses']/total:.1f}%)\\n\\n\")\n",
    "    \n",
    "    f.write(\"Category Performance:\\n\")\n",
    "    for cat in ['added', 'removed', 'changed']:\n",
    "        f.write(f\"  {cat}: F1={error_df[f'f1_{cat}'].mean():.3f}\\n\")\n",
    "    \n",
    "    f.write(f\"\\nTotal False Positives: {total_fp}\\n\")\n",
    "    f.write(f\"Total False Negatives: {total_fn}\\n\")\n",
    "\n",
    "print(f\"üìÑ Detailed report saved to: {report_path}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "tpuV5e8",
   "dataSources": [
    {
     "databundleVersionId": 14018658,
     "sourceId": 117226,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 473420,
     "modelInstanceId": 457497,
     "sourceId": 609334,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
