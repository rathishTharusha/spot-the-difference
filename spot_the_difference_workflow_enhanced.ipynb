{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccfd6df1",
   "metadata": {},
   "source": [
    "# Spot the Difference ML Workflow — Enhanced Version\n",
    "\n",
    "This notebook implements the proposed enhancements: detector ensemble with TTA + WBF, refined change localization heatmaps, CLIP-assisted matching, threshold tuning, stronger augmentations, and improved evaluation and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cbc047c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing ensemble-boxes...\n",
      "PyTorch: 2.5.1\n",
      "CUDA available: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0. Setup: ensure dependencies and check device\n",
    "import sys, subprocess, importlib\n",
    "\n",
    "def ensure(pkg_spec, import_name=None):\n",
    "    name = import_name or pkg_spec.split('==')[0].split('[')[0].split('/')[-1]\n",
    "    try:\n",
    "        importlib.import_module(name)\n",
    "    except Exception:\n",
    "        print(f'Installing {pkg_spec}...')\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg_spec])\n",
    "\n",
    "# Core\n",
    "ensure('numpy')\n",
    "ensure('pandas')\n",
    "ensure('matplotlib')\n",
    "ensure('Pillow', 'PIL')\n",
    "ensure('scikit-learn', 'sklearn')\n",
    "ensure('opencv-python', 'cv2')\n",
    "# ML\n",
    "ensure('timm')\n",
    "ensure('transformers')\n",
    "ensure('albumentations')\n",
    "ensure('ensemble-boxes')\n",
    "ensure('open-clip-torch', 'open_clip')\n",
    "ensure('scipy')\n",
    "\n",
    "import os, json, math, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print('PyTorch:', torch.__version__)\n",
    "print('CUDA available:', torch.cuda.is_available())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8a467fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Spot the Difference ML Workflow: Step-by-Step Explanation\n",
       "\n",
       "This document explains the procedure for detecting added, removed, or changed objects between two similar images using advanced machine learning techniques. The workflow combines data preparation, change localization, object detection, and matching to robustly spot differences.\n",
       "\n",
       "---\n",
       "\n",
       "## 1. Data Preparation & Vocabulary\n",
       "- **Normalize object labels:** Clean and standardize object names in your dataset (e.g., 'man', 'guy', 'worker' → 'person').\n",
       "- **Build vocabulary:** Extract a list of unique object types to detect (e.g., 'car', 'person', 'cone').\n",
       "\n",
       "## 2. Change Localization (Where things changed)\n",
       "- **Siamese backbone:** Use a twin neural network (e.g., ViT/Swin Transformer) to process both images in parallel, extracting features.\n",
       "- **Cross-attention:** Compare features between images to focus on regions that differ.\n",
       "- **Change logit map (H):** Output a multi-scale map highlighting areas where changes likely occurred.\n",
       "\n",
       "## 3. Object Detection (What objects changed)\n",
       "- **Open-vocabulary detector:** Use a model like OWL-ViT or Grounding DINO to detect objects in both images, using your vocabulary.\n",
       "- **Bounding boxes & labels:** Get locations and types of objects present in each image.\n",
       "\n",
       "## 4. Score Fusion\n",
       "- **Combine scores:** Boost detector confidence for objects overlapping with high-change regions in the change map (H).\n",
       "- **Formula:**\n",
       "  \n",
       "  $\\text{score}' = \\text{score}_{det} \\times (1 + \\lambda \\times \\text{normalized H overlap})$\n",
       "\n",
       "## 5. Matching & Decision Rules\n",
       "- **Match objects:** Use class labels and bounding box overlap (IoU) to match objects between images.\n",
       "- **Rules:**\n",
       "  - Only in second image → \"added\"\n",
       "  - Only in first image → \"removed\"\n",
       "  - Matched but moved/appearance changed → \"changed\"\n",
       "\n",
       "## 6. Classification Heads (Optional)\n",
       "- **Global features:** Add heads to predict, for each class, whether it was added, removed, or changed.\n",
       "- **Weak supervision:** Train using category-level labels, not pixel-perfect masks.\n",
       "\n",
       "## 7. Final Output\n",
       "- **For each image pair:** Output lists of added, removed, and changed objects.\n",
       "- **Visualization:** Display results and save in the required format for submission.\n",
       "\n",
       "---\n",
       "\n",
       "## How the Techniques Work Together\n",
       "- **Siamese encoder & change map:** Tell you where to look for changes.\n",
       "- **Detector:** Tells you what objects are present.\n",
       "- **Matching & fusion:** Decide what changed and how (added, removed, changed).\n",
       "- **Weak supervision & symmetry tricks:** Enable learning even with limited labels.\n",
       "\n",
       "This pipeline allows robust difference detection between images, combining deep learning, object detection, and smart matching—even when only category-level labels are available.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Load Procedure Markdown for quick reference\n",
    "from IPython.display import Markdown, display\n",
    "with open('spot_the_difference_procedure.md', 'r', encoding='utf-8') as f:\n",
    "    display(Markdown(f.read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84638b1e",
   "metadata": {},
   "source": [
    "## Data: Load, Normalize Labels, and Build Vocabulary + Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f76700f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: 4536 Test rows: 1482\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_id</th>\n",
       "      <th>added_objs</th>\n",
       "      <th>removed_objs</th>\n",
       "      <th>changed_objs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35655</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30660</td>\n",
       "      <td>none</td>\n",
       "      <td>person vehicle</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34838</td>\n",
       "      <td>man person</td>\n",
       "      <td>car person</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34045</td>\n",
       "      <td>person</td>\n",
       "      <td>none</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30596</td>\n",
       "      <td>none</td>\n",
       "      <td>bicycle person</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   img_id  added_objs    removed_objs changed_objs\n",
       "0   35655        none            none         none\n",
       "1   30660        none  person vehicle         none\n",
       "2   34838  man person      car person         none\n",
       "3   34045      person            none          car\n",
       "4   30596        none  bicycle person         none"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: ['animal', 'baby', 'bag', 'baggage', 'bicycle', 'bicyclist', 'box', 'building', 'car', 'cart', 'case', 'chair', 'child', 'cone', 'container', 'couple', 'dog', 'dolly', 'driver', 'gate', 'girl', 'group', 'individual', 'item', 'kid', 'ladder', 'lady', 'luggage', 'motorcycle', 'object', 'person', 'personal', 'pole', 'scooter', 'shadow', 'sign', 'stroller', 'traffic', 'truck', 'umbrella', 'vehicle', 'vest']\n",
      "Total textual prompts: 252\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'data'\n",
    "train_df = pd.read_csv(os.path.join(data_dir, 'train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(data_dir, 'test.csv'))\n",
    "print('Train rows:', len(train_df), 'Test rows:', len(test_df))\n",
    "display(train_df.head())\n",
    "\n",
    "# Normalization and synonyms\n",
    "import re\n",
    "synonym_map = {\n",
    "    'man':'person','guy':'person','worker':'person','boy':'person','woman':'person','gentleman':'person','pedestrian':'person','people':'person','person':'person',\n",
    "    'auto':'car','sedan':'car','car':'car',\n",
    "    'pickup':'truck','lorry':'truck','truck':'truck','van':'van','bus':'bus',\n",
    "    'motorcycle':'motorcycle','bike':'bicycle','bicycle':'bicycle',\n",
    "    'cone':'cone','traffic cone':'cone',\n",
    "    'sign':'sign','traffic sign':'sign','road sign':'sign',\n",
    "    'pole':'pole','lamp post':'pole','lamp-post':'pole',\n",
    "    'barrier':'barrier','fence':'barrier',\n",
    "    'ladder':'ladder','gate':'gate','bag':'bag','box':'box','umbrella':'umbrella'\n",
    "}\n",
    "\n",
    "def normalize_labels(label_str):\n",
    "    if pd.isna(label_str) or not str(label_str).strip() or str(label_str).strip().lower()=='none':\n",
    "        return []\n",
    "    raw = re.split(r'[ ,]+', str(label_str).strip().lower())\n",
    "    mapped = [synonym_map.get(tok, tok) for tok in raw]\n",
    "    mapped = [tok for tok in mapped if tok and tok!='none']\n",
    "    return sorted(set(mapped))\n",
    "\n",
    "for col in ['added_objs','removed_objs','changed_objs']:\n",
    "    train_df[col+'_norm'] = train_df[col].apply(normalize_labels)\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = set()\n",
    "for col in ['added_objs_norm','removed_objs_norm','changed_objs_norm']:\n",
    "    for lst in train_df[col]:\n",
    "        vocab.update(lst)\n",
    "vocab = sorted(vocab)\n",
    "print('Vocab:', vocab)\n",
    "\n",
    "# Prompt engineering: multi-phrase prompts per label\n",
    "def prompts_for_label(lbl):\n",
    "    base = lbl.replace('_',' ')\n",
    "    return [\n",
    "        base,\n",
    "        f'a photo of a {base}',\n",
    "        f'{base} object',\n",
    "        f'small {base}',\n",
    "        f'large {base}',\n",
    "        f'{base} in the scene'\n",
    "    ]\n",
    "\n",
    "label_to_prompts = {lbl: prompts_for_label(lbl) for lbl in vocab}\n",
    "# OWL-ViT expects a list of queries; we can flatten prompts but keep an index map to labels\n",
    "flattened_prompts = []\n",
    "prompt_to_label = []\n",
    "for lbl, plist in label_to_prompts.items():\n",
    "    for p in plist:\n",
    "        flattened_prompts.append(p)\n",
    "        prompt_to_label.append(lbl)\n",
    "print('Total textual prompts:', len(flattened_prompts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba8cbe2",
   "metadata": {},
   "source": [
    "## Detector Ensemble with TTA and WBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2eeeeac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\This PC\\.conda\\envs\\octwave-spotdiff-gpu\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroundingDINO loaded\n",
      "Detections: 265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\This PC\\.conda\\envs\\octwave-spotdiff-gpu\\lib\\site-packages\\ensemble_boxes\\ensemble_boxes_wbf.py:54: UserWarning: Y1 < 0 in box. Set it to 0.\n",
      "  warnings.warn('Y1 < 0 in box. Set it to 0.')\n",
      "C:\\Users\\This PC\\.conda\\envs\\octwave-spotdiff-gpu\\lib\\site-packages\\ensemble_boxes\\ensemble_boxes_wbf.py:42: UserWarning: X1 < 0 in box. Set it to 0.\n",
      "  warnings.warn('X1 < 0 in box. Set it to 0.')\n",
      "C:\\Users\\This PC\\.conda\\envs\\octwave-spotdiff-gpu\\lib\\site-packages\\ensemble_boxes\\ensemble_boxes_wbf.py:63: UserWarning: Y2 > 1 in box. Set it to 1. Check that you normalize boxes in [0, 1] range.\n",
      "  warnings.warn('Y2 > 1 in box. Set it to 1. Check that you normalize boxes in [0, 1] range.')\n",
      "C:\\Users\\This PC\\.conda\\envs\\octwave-spotdiff-gpu\\lib\\site-packages\\ensemble_boxes\\ensemble_boxes_wbf.py:51: UserWarning: X2 > 1 in box. Set it to 1. Check that you normalize boxes in [0, 1] range.\n",
      "  warnings.warn('X2 > 1 in box. Set it to 1. Check that you normalize boxes in [0, 1] range.')\n"
     ]
    }
   ],
   "source": [
    "# Load OWL-ViT\n",
    "from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
    "owl_processor = OwlViTProcessor.from_pretrained('google/owlvit-base-patch32')\n",
    "owl_model = OwlViTForObjectDetection.from_pretrained('google/owlvit-base-patch32').to(device)\n",
    "owl_model.eval()\n",
    "\n",
    "# Try to load GroundingDINO (optional)\n",
    "has_gdino = False\n",
    "try:\n",
    "    from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "    gdino_processor = AutoProcessor.from_pretrained('IDEA-Research/grounding-dino-base')\n",
    "    gdino_model = AutoModelForZeroShotObjectDetection.from_pretrained('IDEA-Research/grounding-dino-base').to(device)\n",
    "    gdino_model.eval()\n",
    "    has_gdino = True\n",
    "    print('GroundingDINO loaded')\n",
    "except Exception as e:\n",
    "    print('GroundingDINO not available, proceeding with OWL-ViT only. Reason:', str(e))\n",
    "\n",
    "from ensemble_boxes import weighted_boxes_fusion\n",
    "import cv2\n",
    "\n",
    "def _resize_to(image: Image.Image, size=(800, 800)):\n",
    "    # optional resizing for speed/accuracy tradeoff\n",
    "    return image\n",
    "\n",
    "def _to_xyxy_norm(boxes, w, h):\n",
    "    # boxes xyxy absolute -> normalized 0..1\n",
    "    if len(boxes)==0:\n",
    "        return []\n",
    "    b = np.asarray(boxes, dtype=float)\n",
    "    b[:,0] /= w; b[:,2] /= w; b[:,1] /= h; b[:,3] /= h\n",
    "    return b.tolist()\n",
    "\n",
    "def _from_xyxy_norm(boxes, w, h):\n",
    "    if len(boxes)==0:\n",
    "        return []\n",
    "    b = np.asarray(boxes, dtype=float)\n",
    "    b[:,0] *= w; b[:,2] *= w; b[:,1] *= h; b[:,3] *= h\n",
    "    return b.tolist()\n",
    "\n",
    "def _flip_boxes_horiz_xyxy(boxes, w):\n",
    "    # flip horizontally\n",
    "    flipped = []\n",
    "    for x1,y1,x2,y2 in boxes:\n",
    "        nx1 = w - x2\n",
    "        nx2 = w - x1\n",
    "        flipped.append([nx1,y1,nx2,y2])\n",
    "    return flipped\n",
    "\n",
    "def detect_owlvit(image: Image.Image, prompts: list, score_thr=0.05):\n",
    "    w, h = image.size\n",
    "    inputs = owl_processor(text=prompts, images=image, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        out = owl_model(**inputs)\n",
    "    target_sizes = torch.tensor([[h, w]], device=device)\n",
    "    results = owl_processor.post_process_object_detection(out, target_sizes=target_sizes, threshold=score_thr)[0]\n",
    "    boxes = results['boxes'].detach().cpu().numpy().tolist()\n",
    "    scores = results['scores'].detach().cpu().numpy().tolist()\n",
    "    # labels correspond to index in prompts; map back to base label via prompt_to_label\n",
    "    labels_idx = results['labels'].detach().cpu().numpy().tolist()\n",
    "    labels = [prompt_to_label[idx] for idx in labels_idx]\n",
    "    return boxes, scores, labels\n",
    "\n",
    "def detect_gdino(image: Image.Image, labels: list, score_thr=0.05):\n",
    "    if not has_gdino:\n",
    "        return [], [], []\n",
    "    w, h = image.size\n",
    "    text = '. '.join(labels)\n",
    "    inputs = gdino_processor(images=image, text=text, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        out = gdino_model(**inputs)\n",
    "    results = gdino_processor.post_process_grounded_object_detection(out, inputs.input_ids, box_threshold=score_thr, text_threshold=0.25, target_sizes=[(h,w)])[0]\n",
    "    boxes = results['boxes'].detach().cpu().numpy().tolist()\n",
    "    scores = results['scores'].detach().cpu().numpy().tolist()\n",
    "    # labels as matched phrases from the text per prediction\n",
    "    ph = results.get('phrases', ['object']*len(boxes))\n",
    "    # map phrases to closest vocab label by simple token match\n",
    "    mapped = []\n",
    "    for p in ph:\n",
    "        p = p.lower()\n",
    "        candidates = [v for v in vocab if v in p or p in v]\n",
    "        mapped.append(candidates[0] if candidates else 'object')\n",
    "    return boxes, scores, mapped\n",
    "\n",
    "def ensemble_detect(image_path, base_score_thr=0.05, wbf_iou_thr=0.55, wbf_skip_thr=0.05):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    w, h = image.size\n",
    "    # TTA: original + hflip\n",
    "    images = [image, image.transpose(Image.FLIP_LEFT_RIGHT)]\n",
    "    # Collect boxes per TTA per detector\n",
    "    all_boxes = []\n",
    "    all_scores = []\n",
    "    all_labels = []\n",
    "    for idx, im in enumerate(images):\n",
    "        # OWL-ViT\n",
    "        b1, s1, l1 = detect_owlvit(im, flattened_prompts, score_thr=base_score_thr)\n",
    "        if idx==1: # flip back\n",
    "            b1 = _flip_boxes_horiz_xyxy(b1, w)\n",
    "        all_boxes.append(b1); all_scores.append(s1); all_labels.append(l1)\n",
    "        # GroundingDINO (optional)\n",
    "        if has_gdino:\n",
    "            b2, s2, l2 = detect_gdino(im, vocab, score_thr=base_score_thr)\n",
    "            if idx==1:\n",
    "                b2 = _flip_boxes_horiz_xyxy(b2, w)\n",
    "            all_boxes.append(b2); all_scores.append(s2); all_labels.append(l2)\n",
    "    # Prepare for WBF: need lists by TTA run; we'll merge all into a single ensemble call\n",
    "    boxes_norm = [_to_xyxy_norm(b, w, h) for b in all_boxes]\n",
    "    # Convert text labels to integer class ids based on vocab index\n",
    "    label_to_idx = {v:i for i,v in enumerate(vocab)}\n",
    "    labels_idx = [[label_to_idx.get(l, -1) for l in lab] for lab in all_labels]\n",
    "    # Filter out -1 labels\n",
    "    for i in range(len(boxes_norm)):\n",
    "        keep = [k for k,l in enumerate(labels_idx[i]) if l>=0]\n",
    "        boxes_norm[i] = [boxes_norm[i][k] for k in keep]\n",
    "        all_scores[i] = [all_scores[i][k] for k in keep]\n",
    "        labels_idx[i] = [labels_idx[i][k] for k in keep]\n",
    "    if sum(len(b) for b in boxes_norm)==0:\n",
    "        return [], [], []\n",
    "    wb, ws, wl = weighted_boxes_fusion(boxes_norm, all_scores, labels_idx, iou_thr=wbf_iou_thr, skip_box_thr=wbf_skip_thr)\n",
    "    # Back to absolute coords and labels\n",
    "    abs_boxes = _from_xyxy_norm(wb, w, h)\n",
    "    out_labels = [vocab[int(i)] for i in wl]\n",
    "    return abs_boxes, ws.tolist(), out_labels\n",
    "\n",
    "def draw_boxes(image_path, boxes, labels, scores, score_thr=0.2):\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(img)\n",
    "    ax = plt.gca()\n",
    "    import matplotlib.patches as patches\n",
    "    for (x1,y1,x2,y2), l, s in zip(boxes, labels, scores):\n",
    "        if s < score_thr: continue\n",
    "        rect = patches.Rectangle((x1,y1), x2-x1, y2-y1, linewidth=2, edgecolor='lime', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x1, max(0,y1-5), f'{l}:{s:.2f}', color='black', fontsize=9, bbox=dict(facecolor='yellow', alpha=0.6))\n",
    "    plt.axis('off'); plt.show()\n",
    "\n",
    "# Quick smoke test on one sample (non-fatal)\n",
    "try:\n",
    "    sid = train_df['img_id'].iloc[0]\n",
    "    p = os.path.join(data_dir, 'data', f'{sid}_1.png')\n",
    "    b,s,l = ensemble_detect(p, base_score_thr=0.05)\n",
    "    print('Detections:', len(b))\n",
    "except Exception as e:\n",
    "    print('Detector smoke test skipped:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cefcd8",
   "metadata": {},
   "source": [
    "## Refined Change Localization: Siamese ViT Patch-level Heatmap + Augmentations + Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df65cfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "class PairDataset(Dataset):\n",
    "    def __init__(self, df, root_dir, aug=True):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.root = root_dir\n",
    "        self.aug = aug\n",
    "        self.train_tf = A.Compose([\n",
    "            A.Resize(224,224),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            # Use Affine instead of deprecated ShiftScaleRotate\n",
    "            A.Affine(scale=(0.9, 1.1), translate_percent=(0.0, 0.02), rotate=(-10, 10), shear=None, p=0.5),\n",
    "            A.ColorJitter(p=0.5),\n",
    "            # Replace Cutout with CoarseDropout\n",
    "            A.CoarseDropout(max_holes=4, max_height=20, max_width=20, min_holes=1, min_height=10, min_width=10, fill_value=0, p=0.3),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "        self.val_tf = A.Compose([A.Resize(224,224), ToTensorV2()])\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.df.loc[idx, 'img_id']\n",
    "        p1 = os.path.join(self.root, 'data', f'{img_id}_1.png')\n",
    "        p2 = os.path.join(self.root, 'data', f'{img_id}_2.png')\n",
    "        im1 = np.array(Image.open(p1).convert('RGB'))\n",
    "        im2 = np.array(Image.open(p2).convert('RGB'))\n",
    "        tf = self.train_tf if self.aug else self.val_tf\n",
    "        t1 = tf(image=im1)['image']\n",
    "        t2 = tf(image=im2)['image']\n",
    "        # weak label: any change present?\n",
    "        has_change = (len(self.df.loc[idx, 'added_objs_norm']) + len(self.df.loc[idx, 'removed_objs_norm']) + len(self.df.loc[idx, 'changed_objs_norm'])) > 0\n",
    "        y = torch.tensor([1.0 if has_change else 0.0], dtype=torch.float32)\n",
    "        return t1, t2, y\n",
    "\n",
    "class SiameseViTChange(nn.Module):\n",
    "    def __init__(self, backbone_name='vit_base_patch16_224'):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(backbone_name, pretrained=True)\n",
    "        self.embed_dim = self.backbone.num_features\n",
    "        self.head = nn.Sequential(nn.Linear(self.embed_dim*2, 256), nn.ReLU(), nn.Linear(256, 1))\n",
    "    def forward_feats(self, x):\n",
    "        # returns token embeddings [B, N+1, C]; timm vit forward_features returns [B, tokens, C] for many models\n",
    "        f = self.backbone.forward_features(x)\n",
    "        return f\n",
    "    def forward(self, x1, x2):\n",
    "        f1 = self.forward_feats(x1)\n",
    "        f2 = self.forward_feats(x2)\n",
    "        # Global pooling on tokens\n",
    "        g1 = f1.mean(dim=1)\n",
    "        g2 = f2.mean(dim=1)\n",
    "        return self.head(torch.cat([g1,g2], dim=1))\n",
    "\n",
    "def focal_loss_with_logits(logits, targets, alpha=0.25, gamma=2.0):\n",
    "    bce = nn.functional.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n",
    "    pt = torch.exp(-bce)\n",
    "    loss = alpha * (1-pt)**gamma * bce\n",
    "    return loss.mean()\n",
    "\n",
    "def patch_change_heatmap(model, img1: Image.Image, img2: Image.Image):\n",
    "    # compute patch-wise feature L2 diff heatmap\n",
    "    t = T.Compose([T.Resize((224,224)), T.ToTensor()])\n",
    "    x1 = t(img1).unsqueeze(0).to(device)\n",
    "    x2 = t(img2).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        f1 = model.forward_feats(x1) # [1, tokens, C]\n",
    "        f2 = model.forward_feats(x2)\n",
    "    # Drop cls token if present (assume first token) to get patches\n",
    "    p1 = f1[:,1:,:]\n",
    "    p2 = f2[:,1:,:]\n",
    "    diff = (p1 - p2).pow(2).sum(dim=-1).sqrt()  # [1, N]\n",
    "    # infer grid size: 224/16=14 for vit_base_patch16_224\n",
    "    n = diff.shape[1]\n",
    "    side = int(math.sqrt(n))\n",
    "    hm = diff[0].reshape(side, side).detach().cpu().numpy()\n",
    "    hm = (hm - hm.min()) / (hm.max() - hm.min() + 1e-6)\n",
    "    # upsample to image size\n",
    "    hm_up = cv2.resize(hm, img1.size[::-1], interpolation=cv2.INTER_CUBIC)\n",
    "    # global change score as mean of top-k heat\n",
    "    k = max(1, (hm_up.size)//20)\n",
    "    score = float(np.mean(np.sort(hm_up.reshape(-1))[-k:]))\n",
    "    return hm_up, score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b922a09",
   "metadata": {},
   "source": [
    "## CLIP-assisted Matching + Score Fusion with Change Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16aac69e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4f9254927d942bea5e90ec2f825f510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "open_clip_model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\This PC\\.conda\\envs\\octwave-spotdiff-gpu\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\This PC\\.cache\\huggingface\\hub\\models--laion--CLIP-ViT-B-32-laion2B-s34B-b79K. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match smoke test skipped: name 'chg_model' is not defined\n"
     ]
    }
   ],
   "source": [
    "import open_clip\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# Load CLIP (ViT-B/32)\n",
    "clip_model, _, clip_preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\n",
    "clip_model = clip_model.to(device).eval()\n",
    "clip_tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
    "\n",
    "def crop_to_tensor(image: Image.Image, box, size=224):\n",
    "    x1,y1,x2,y2 = [int(v) for v in box]\n",
    "    x1 = max(0, x1); y1=max(0,y1); x2=min(image.width-1, x2); y2=min(image.height-1,y2)\n",
    "    crop = image.crop((x1,y1,x2,y2)).convert('RGB')\n",
    "    return clip_preprocess(crop).unsqueeze(0).to(device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def clip_image_similarity(img_path1, box1, img_path2, box2):\n",
    "    im1 = Image.open(img_path1).convert('RGB')\n",
    "    im2 = Image.open(img_path2).convert('RGB')\n",
    "    t1 = crop_to_tensor(im1, box1)\n",
    "    t2 = crop_to_tensor(im2, box2)\n",
    "    f1 = clip_model.encode_image(t1)\n",
    "    f2 = clip_model.encode_image(t2)\n",
    "    f1 = nn.functional.normalize(f1, dim=-1)\n",
    "    f2 = nn.functional.normalize(f2, dim=-1)\n",
    "    sim = (f1 @ f2.t()).item()  # cosine similarity in [-1,1]\n",
    "    return (sim+1)/2  # map to [0,1]\n",
    "\n",
    "def iou_xyxy(a,b):\n",
    "    xA = max(a[0], b[0]); yA=max(a[1],b[1]); xB=min(a[2],b[2]); yB=min(a[3],b[3])\n",
    "    inter = max(0,xB-xA)*max(0,yB-yA)\n",
    "    areaA = max(0,a[2]-a[0])*max(0,a[3]-a[1])\n",
    "    areaB = max(0,b[2]-b[0])*max(0,b[3]-b[1])\n",
    "    denom = areaA+areaB-inter + 1e-6\n",
    "    return inter/denom\n",
    "\n",
    "def fuse_scores_with_change(boxes, scores, heatmap, lambda_w=0.8, th=0.5):\n",
    "    # boost scores for boxes overlapping high-heat regions\n",
    "    H,W = heatmap.shape\n",
    "    high = (heatmap >= th).astype(np.uint8)\n",
    "    boosted = []\n",
    "    for (x1,y1,x2,y2), s in zip(boxes, scores):\n",
    "        x1c = int(max(0, min(W-1, x1))); x2c=int(max(0,min(W-1,x2)))\n",
    "        y1c = int(max(0, min(H-1, y1))); y2c=int(max(0,min(H-1,y2)))\n",
    "        if x2c<=x1c or y2c<=y1c:\n",
    "            boosted.append(s); continue\n",
    "        region = high[y1c:y2c, x1c:x2c]\n",
    "        overlap = float(region.mean()) if region.size>0 else 0.0\n",
    "        boosted.append(float(s * (1.0 + lambda_w * overlap)))\n",
    "    return boosted\n",
    "\n",
    "def match_objects(img_id, det_thr=0.25, iou_thr=0.5, alpha=0.5, heat_lambda=0.8, heat_th=0.5):\n",
    "    p1 = os.path.join(data_dir, 'data', f'{img_id}_1.png')\n",
    "    p2 = os.path.join(data_dir, 'data', f'{img_id}_2.png')\n",
    "    # detections\n",
    "    b1,s1,l1 = ensemble_detect(p1, base_score_thr=0.05)\n",
    "    b2,s2,l2 = ensemble_detect(p2, base_score_thr=0.05)\n",
    "    # change heatmap\n",
    "    im1 = Image.open(p1).convert('RGB'); im2 = Image.open(p2).convert('RGB')\n",
    "    hm, chg_score = patch_change_heatmap(chg_model, im1, im2)\n",
    "    # score fusion\n",
    "    s1b = fuse_scores_with_change(b1, s1, hm, lambda_w=heat_lambda, th=heat_th)\n",
    "    s2b = fuse_scores_with_change(b2, s2, hm, lambda_w=heat_lambda, th=heat_th)\n",
    "    # threshold filter\n",
    "    keep1 = [i for i,x in enumerate(s1b) if x>=det_thr]\n",
    "    keep2 = [i for i,x in enumerate(s2b) if x>=det_thr]\n",
    "    b1 = [b1[i] for i in keep1]; l1 = [l1[i] for i in keep1]; s1b = [s1b[i] for i in keep1]\n",
    "    b2 = [b2[i] for i in keep2]; l2 = [l2[i] for i in keep2]; s2b = [s2b[i] for i in keep2]\n",
    "    # build candidates by label\n",
    "    if len(b1)==0 and len(b2)==0:\n",
    "        return {'added':[], 'removed':[], 'changed':[], 'change_score': chg_score}\n",
    "    # Hungarian cost matrix over possible pairs with same label\n",
    "    cost = np.ones((len(b1), len(b2)), dtype=float)\n",
    "    for i in range(len(b1)):\n",
    "        for j in range(len(b2)):\n",
    "            if l1[i] != l2[j]:\n",
    "                cost[i,j] = 1.0\n",
    "                continue\n",
    "            iou = iou_xyxy(b1[i], b2[j])\n",
    "            try:\n",
    "                sim = clip_image_similarity(p1, b1[i], p2, b2[j])\n",
    "            except Exception:\n",
    "                sim = iou  # fallback\n",
    "            # combine IoU and CLIP sim (higher is better) -> cost lower is better\n",
    "            score = alpha*iou + (1-alpha)*sim\n",
    "            cost[i,j] = 1.0 - score\n",
    "    if len(b1)>0 and len(b2)>0:\n",
    "        ri, cj = linear_sum_assignment(cost)\n",
    "        matches = [(i,j) for i,j in zip(ri,cj) if (l1[i]==l2[j] and (1.0-cost[i,j])>=iou_thr)]\n",
    "    else:\n",
    "        matches = []\n",
    "    matched_i = set(i for i,_ in matches)\n",
    "    matched_j = set(j for _,j in matches)\n",
    "    added = [l2[j] for j in range(len(l2)) if j not in matched_j]\n",
    "    removed = [l1[i] for i in range(len(l1)) if i not in matched_i]\n",
    "    changed = []\n",
    "    for i,j in matches:\n",
    "        if iou_xyxy(b1[i], b2[j]) < iou_thr:\n",
    "            changed.append(l1[i])\n",
    "    # deduplicate\n",
    "    return {\n",
    "        'added': sorted(set(added)),\n",
    "        'removed': sorted(set(removed)),\n",
    "        'changed': sorted(set(changed)),\n",
    "        'change_score': chg_score\n",
    "    }\n",
    "\n",
    "# Quick run on a tiny sample\n",
    "try:\n",
    "    sid = train_df['img_id'].iloc[0]\n",
    "    res = match_objects(sid, det_thr=0.25, iou_thr=0.5, alpha=0.5, heat_lambda=0.8, heat_th=0.5)\n",
    "    print('Sample result:', res)\n",
    "except Exception as e:\n",
    "    print('Match smoke test skipped:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db1fb70",
   "metadata": {},
   "source": [
    "## Threshold Tuning on Validation Split (set-level F1 for added/removed/changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d35f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_f1(true_set, pred_set):\n",
    "    tp = len(true_set & pred_set)\n",
    "    fp = len(pred_set - true_set)\n",
    "    fn = len(true_set - pred_set)\n",
    "    precision = tp/(tp+fp+1e-9)\n",
    "    recall = tp/(tp+fn+1e-9)\n",
    "    if precision+recall==0: return 0.0\n",
    "    return 2*precision*recall/(precision+recall)\n",
    "\n",
    "def evaluate_params(det_thr, iou_thr, alpha, heat_lambda, heat_th, sample_ids):\n",
    "    f1_added=f1_removed=f1_changed=0.0; n=0\n",
    "    for img_id in sample_ids:\n",
    "        gt_a = set(train_df.loc[train_df['img_id']==img_id, 'added_objs_norm'].iloc[0])\n",
    "        gt_r = set(train_df.loc[train_df['img_id']==img_id, 'removed_objs_norm'].iloc[0])\n",
    "        gt_c = set(train_df.loc[train_df['img_id']==img_id, 'changed_objs_norm'].iloc[0])\n",
    "        try:\n",
    "            res = match_objects(img_id, det_thr, iou_thr, alpha, heat_lambda, heat_th)\n",
    "        except Exception:\n",
    "            continue\n",
    "        f1_added += set_f1(gt_a, set(res['added']))\n",
    "        f1_removed += set_f1(gt_r, set(res['removed']))\n",
    "        f1_changed += set_f1(gt_c, set(res['changed']))\n",
    "        n+=1\n",
    "    if n==0: return 0.0\n",
    "    return (f1_added+f1_removed+f1_changed)/(3*n)\n",
    "\n",
    "# small random subset for tuning\n",
    "val_sample_ids = train_df.loc[val_ids, 'img_id'].sample(min(10, len(val_ids)), random_state=123).tolist()\n",
    "grid_det = [0.15, 0.25, 0.35]\n",
    "grid_iou = [0.4, 0.5, 0.6]\n",
    "grid_alpha = [0.3, 0.5, 0.7]\n",
    "grid_lambda = [0.5, 0.8, 1.2]\n",
    "grid_hth = [0.4, 0.5, 0.6]\n",
    "best = {'score':-1}\n",
    "for dt in grid_det:\n",
    "    for it in grid_iou:\n",
    "        for a in grid_alpha:\n",
    "            for lam in grid_lambda:\n",
    "                for hth in grid_hth:\n",
    "                    score = evaluate_params(dt,it,a,lam,hth, val_sample_ids)\n",
    "                    if score > best.get('score', -1):\n",
    "                        best = {'det_thr':dt,'iou_thr':it,'alpha':a,'heat_lambda':lam,'heat_th':hth,'score':score}\n",
    "                        print('New best:', best)\n",
    "best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f143d52",
   "metadata": {},
   "source": [
    "## Evaluation on Validation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de40c295",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = best if isinstance(best, dict) and 'det_thr' in best else {\n",
    "    'det_thr':0.25,'iou_thr':0.5,'alpha':0.5,'heat_lambda':0.8,'heat_th':0.5\n",
    "}\n",
    "print('Using params:', best_params)\n",
    "\n",
    "eval_ids = train_df.loc[val_ids, 'img_id'].sample(min(5, len(val_ids)), random_state=77).tolist()\n",
    "for mid in eval_ids:\n",
    "    res = match_objects(mid, **{k:best_params[k] for k in ['det_thr','iou_thr','alpha','heat_lambda','heat_th']})\n",
    "    print(f\"Image {mid} -> added:{res['added']} removed:{res['removed']} changed:{res['changed']} score:{res['change_score']:.3f}\")\n",
    "    p1 = os.path.join(data_dir, 'data', f'{mid}_1.png')\n",
    "    b,s,l = ensemble_detect(p1, base_score_thr=0.05)\n",
    "    draw_boxes(p1, b, l, s, score_thr=best_params['det_thr'])\n",
    "    p2 = os.path.join(data_dir, 'data', f'{mid}_2.png')\n",
    "    b,s,l = ensemble_detect(p2, base_score_thr=0.05)\n",
    "    draw_boxes(p2, b, l, s, score_thr=best_params['det_thr'])\n",
    "\n",
    "# simple mean change score on validation\n",
    "val_scores = []\n",
    "for mid in val_sample_ids:\n",
    "    r = match_objects(mid, **{k:best_params[k] for k in ['det_thr','iou_thr','alpha','heat_lambda','heat_th']})\n",
    "    val_scores.append(r['change_score'])\n",
    "print('Mean validation change score:', float(np.mean(val_scores)) if val_scores else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756bc667",
   "metadata": {},
   "source": [
    "## Generate Submission and Save Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9ebca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = []\n",
    "for img_id in test_df['img_id']:\n",
    "    r = match_objects(img_id, **{k:best_params[k] for k in ['det_thr','iou_thr','alpha','heat_lambda','heat_th']})\n",
    "    def fmt(xs):\n",
    "        return 'none' if len(xs)==0 else ' '.join(sorted(set(xs)))\n",
    "    submission.append({\n",
    "        'img_id': img_id,\n",
    "        'added_objs': fmt(r['added']),\n",
    "        'removed_objs': fmt(r['removed']),\n",
    "        'changed_objs': fmt(r['changed'])\n",
    "    })\n",
    "submission_df = pd.DataFrame(submission)\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "print('Saved submission.csv with', len(submission_df), 'rows')\n",
    "\n",
    "# Save basic metrics\n",
    "mean_val_change = float(np.mean(val_scores)) if val_scores else float('nan')\n",
    "with open('eval_metrics.txt','w') as f:\n",
    "    f.write(f\"Params: {json.dumps(best_params)}\\n\")\n",
    "    f.write(f\"Mean validation change score: {mean_val_change:.5f}\\n\")\n",
    "print('Saved eval_metrics.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e7c83d",
   "metadata": {},
   "source": [
    "## Error Analysis and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d339569",
   "metadata": {},
   "outputs": [],
   "source": [
    "err = []\n",
    "chk_ids = train_df.loc[val_ids, 'img_id'].sample(min(10, len(val_ids)), random_state=99).tolist()\n",
    "for mid in chk_ids:\n",
    "    gt_a = set(train_df.loc[train_df['img_id']==mid, 'added_objs_norm'].iloc[0])\n",
    "    gt_r = set(train_df.loc[train_df['img_id']==mid, 'removed_objs_norm'].iloc[0])\n",
    "    gt_c = set(train_df.loc[train_df['img_id']==mid, 'changed_objs_norm'].iloc[0])\n",
    "    r = match_objects(mid, **{k:best_params[k] for k in ['det_thr','iou_thr','alpha','heat_lambda','heat_th']})\n",
    "    pa, pr, pc = set(r['added']), set(r['removed']), set(r['changed'])\n",
    "    err.append({\n",
    "        'img_id': mid,\n",
    "        'added_missed': list(gt_a - pa), 'added_wrong': list(pa - gt_a),\n",
    "        'removed_missed': list(gt_r - pr), 'removed_wrong': list(pr - gt_r),\n",
    "        'changed_missed': list(gt_c - pc), 'changed_wrong': list(pc - gt_c)\n",
    "    })\n",
    "for e in err:\n",
    "    print(e)\n",
    "\n",
    "print('Next steps: consider more TTA (scales), multi-phrase confidence pooling, and pseudo-label fine-tuning of change model using high-confidence predictions.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d00dc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch: add fallback version of patch_change_heatmap if not already robust\n",
    "try:\n",
    "    _ = patch_change_heatmap\n",
    "    # function exists; redefine with robust fallback\n",
    "    def patch_change_heatmap(model, img1: Image.Image, img2: Image.Image):\n",
    "        # compute patch-wise feature L2 diff heatmap; fallback to pixel-diff if tokens unavailable\n",
    "        try:\n",
    "            t = T.Compose([T.Resize((224,224)), T.ToTensor()])\n",
    "            x1 = t(img1).unsqueeze(0).to(device)\n",
    "            x2 = t(img2).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                f1 = model.forward_feats(x1) # ideally [B, tokens, C]\n",
    "                f2 = model.forward_feats(x2)\n",
    "            if f1.dim() != 3 or f2.dim() != 3 or f1.shape[1] <= 1:\n",
    "                raise RuntimeError('No token embeddings available')\n",
    "            p1 = f1[:,1:,:]\n",
    "            p2 = f2[:,1:,:]\n",
    "            diff = (p1 - p2).pow(2).sum(dim=-1).sqrt()  # [1, N]\n",
    "            n = diff.shape[1]\n",
    "            side = int(math.sqrt(n))\n",
    "            hm = diff[0].reshape(side, side).detach().cpu().numpy()\n",
    "            hm = (hm - hm.min()) / (hm.max() - hm.min() + 1e-6)\n",
    "            hm_up = cv2.resize(hm, img1.size[::-1], interpolation=cv2.INTER_CUBIC)\n",
    "        except Exception:\n",
    "            a = np.array(img1.convert('RGB'), dtype=np.float32)\n",
    "            b = np.array(img2.convert('RGB'), dtype=np.float32)\n",
    "            d = np.mean(np.abs(a - b), axis=2)\n",
    "            d = (d - d.min()) / (d.max() - d.min() + 1e-6)\n",
    "            hm_up = cv2.GaussianBlur(d, (0,0), sigmaX=3)\n",
    "        k = max(1, (hm_up.size)//20)\n",
    "        score = float(np.mean(np.sort(hm_up.reshape(-1))[-k:]))\n",
    "        return hm_up, score\n",
    "except NameError:\n",
    "    # If not defined (unexpected), define fresh\n",
    "    def patch_change_heatmap(model, img1: Image.Image, img2: Image.Image):\n",
    "        try:\n",
    "            t = T.Compose([T.Resize((224,224)), T.ToTensor()])\n",
    "            x1 = t(img1).unsqueeze(0).to(device)\n",
    "            x2 = t(img2).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                f1 = model.forward_feats(x1)\n",
    "                f2 = model.forward_feats(x2)\n",
    "            if f1.dim() != 3 or f2.dim() != 3 or f1.shape[1] <= 1:\n",
    "                raise RuntimeError('No token embeddings available')\n",
    "            p1 = f1[:,1:,:]\n",
    "            p2 = f2[:,1:,:]\n",
    "            diff = (p1 - p2).pow(2).sum(dim=-1).sqrt()\n",
    "            n = diff.shape[1]\n",
    "            side = int(math.sqrt(n))\n",
    "            hm = diff[0].reshape(side, side).detach().cpu().numpy()\n",
    "            hm = (hm - hm.min()) / (hm.max() - hm.min() + 1e-6)\n",
    "            hm_up = cv2.resize(hm, img1.size[::-1], interpolation=cv2.INTER_CUBIC)\n",
    "        except Exception:\n",
    "            a = np.array(img1.convert('RGB'), dtype=np.float32)\n",
    "            b = np.array(img2.convert('RGB'), dtype=np.float32)\n",
    "            d = np.mean(np.abs(a - b), axis=2)\n",
    "            d = (d - d.min()) / (d.max() - d.min() + 1e-6)\n",
    "            hm_up = cv2.GaussianBlur(d, (0,0), sigmaX=3)\n",
    "        k = max(1, (hm_up.size)//20)\n",
    "        score = float(np.mean(np.sort(hm_up.reshape(-1))[-k:]))\n",
    "        return hm_up, score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
