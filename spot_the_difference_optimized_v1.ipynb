{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb64ef61",
   "metadata": {},
   "source": [
    "# 🚀 Optimized Spot-the-Difference Pipeline - Maximum Performance\n",
    "\n",
    "**Expert AI Engineering Approach**\n",
    "\n",
    "This notebook implements a state-of-the-art pipeline combining:\n",
    "1. **✅ Robust Vocabulary Extraction** - Training data-driven with intelligent expansion\n",
    "2. **✅ Multi-Model Ensemble Detection** - OWL-ViT + Grounding DINO with WBF fusion\n",
    "3. **✅ Advanced Image Enhancement** - Super-resolution + adaptive preprocessing\n",
    "4. **✅ Proper Cross-Validation** - Stratified K-Fold with F1 optimization\n",
    "5. **✅ Intelligent Threshold Calibration** - Per-category optimization\n",
    "6. **✅ ChangeFormer Architecture** - Cross-attention for precise change localization\n",
    "7. **✅ Smart Object Matching** - Hungarian algorithm with multi-criteria scoring\n",
    "8. **✅ Test-Time Augmentation** - Multi-scale ensemble for robustness\n",
    "\n",
    "**Key Innovation**: Maximum object detection → Accurate labeling → Optimal matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050d1e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageFilter, ImageEnhance\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, precision_recall_fscore_support\n",
    "import cv2\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "import re\n",
    "import time\n",
    "from collections import defaultdict, Counter\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"🚀 OPTIMIZED SPOT-THE-DIFFERENCE PIPELINE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n📦 PyTorch version: {torch.__version__}\")\n",
    "print(f\"🔥 CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"💻 Device name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"💾 CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print(f\"⚡ Using device: {device}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a855a67b",
   "metadata": {},
   "source": [
    "## 1️⃣ Data Loading & Initial Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74202ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "data_dir = './'\n",
    "train_df = pd.read_csv(os.path.join(data_dir, 'train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(data_dir, 'test.csv'))\n",
    "\n",
    "print(\"\\n📊 Dataset Overview:\")\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "\n",
    "print(\"\\n📋 Training data sample:\")\n",
    "display(train_df.head())\n",
    "\n",
    "# Analyze label distribution\n",
    "print(\"\\n🔍 Label Analysis:\")\n",
    "for col in ['added_objs', 'removed_objs', 'changed_objs']:\n",
    "    non_empty = train_df[col].apply(\n",
    "        lambda x: isinstance(x, str) and x.strip().lower() not in ['', 'none', 'null', 'nan']\n",
    "    ).sum()\n",
    "    print(f\"  {col}: {non_empty}/{len(train_df)} samples ({100*non_empty/len(train_df):.1f}%) have labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae79f0b",
   "metadata": {},
   "source": [
    "## 2️⃣ Smart Vocabulary Extraction with Intelligent Expansion\n",
    "\n",
    "**Strategy**: Extract from training data, then expand with synonyms and contextual variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4df2662",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabularyExtractor:\n",
    "    \"\"\"\n",
    "    Intelligent vocabulary extractor with expansion strategies\n",
    "    \"\"\"\n",
    "    def __init__(self, min_frequency=1):\n",
    "        self.min_frequency = min_frequency\n",
    "        self.term_frequencies = defaultdict(int)\n",
    "        self.base_vocabulary = []\n",
    "        self.expanded_vocabulary = []\n",
    "        \n",
    "        # Synonym mapping for common objects\n",
    "        self.synonyms = {\n",
    "            'person': ['man', 'woman', 'people', 'pedestrian', 'individual', 'figure', 'human'],\n",
    "            'car': ['vehicle', 'automobile', 'auto'],\n",
    "            'truck': ['lorry', 'van', 'pickup'],\n",
    "            'bicycle': ['bike', 'cycle'],\n",
    "            'motorcycle': ['motorbike', 'bike', 'scooter'],\n",
    "            'bag': ['backpack', 'purse', 'handbag', 'satchel'],\n",
    "            'sign': ['signboard', 'board', 'placard'],\n",
    "            'cone': ['traffic cone', 'safety cone'],\n",
    "            'barrier': ['fence', 'barricade', 'railing'],\n",
    "            'pole': ['post', 'pillar'],\n",
    "            'umbrella': ['parasol'],\n",
    "            'box': ['crate', 'container', 'package'],\n",
    "            'building': ['structure', 'edifice'],\n",
    "            'tree': ['plant', 'vegetation'],\n",
    "            'light': ['lamp', 'illumination'],\n",
    "        }\n",
    "        \n",
    "        # Common objects to add (from COCO/typical street scenes)\n",
    "        self.common_objects = [\n",
    "            'person', 'car', 'truck', 'bicycle', 'motorcycle', \n",
    "            'bag', 'sign', 'cone', 'barrier', 'pole', 'umbrella', \n",
    "            'box', 'building', 'tree', 'light', 'window', 'door',\n",
    "            'bench', 'chair', 'table', 'plant', 'flower'\n",
    "        ]\n",
    "    \n",
    "    def extract_from_training_data(self, train_df):\n",
    "        \"\"\"Extract vocabulary from training labels\"\"\"\n",
    "        print(\"📚 Extracting vocabulary from training data...\")\n",
    "        \n",
    "        for col in ['added_objs', 'removed_objs', 'changed_objs']:\n",
    "            for label_str in train_df[col].dropna():\n",
    "                if isinstance(label_str, str) and label_str.strip().lower() not in ['', 'none', 'null', 'nan']:\n",
    "                    # Split by common delimiters\n",
    "                    tokens = re.split(r'[,;&\\s]+', label_str.strip().lower())\n",
    "                    for token in tokens:\n",
    "                        token = token.strip()\n",
    "                        # Clean the token\n",
    "                        token = re.sub(r'[^a-z\\s-]', '', token)\n",
    "                        if token and token != 'none' and len(token) > 1:\n",
    "                            self.term_frequencies[token] += 1\n",
    "        \n",
    "        # Filter by minimum frequency\n",
    "        filtered_terms = {\n",
    "            term: freq for term, freq in self.term_frequencies.items() \n",
    "            if freq >= self.min_frequency\n",
    "        }\n",
    "        \n",
    "        # Sort by frequency\n",
    "        sorted_terms = sorted(filtered_terms.items(), key=lambda x: x[1], reverse=True)\n",
    "        self.base_vocabulary = [term for term, freq in sorted_terms]\n",
    "        \n",
    "        print(f\"✅ Base vocabulary: {len(self.base_vocabulary)} unique terms\")\n",
    "        print(f\"\\n📊 Top 20 most frequent terms:\")\n",
    "        for i, (term, freq) in enumerate(sorted_terms[:20], 1):\n",
    "            print(f\"  {i:2d}. {term:20s} (×{freq:3d})\")\n",
    "        \n",
    "        return self.base_vocabulary\n",
    "    \n",
    "    def expand_vocabulary(self):\n",
    "        \"\"\"Intelligently expand vocabulary with synonyms and variants\"\"\"\n",
    "        print(\"\\n🔄 Expanding vocabulary...\")\n",
    "        \n",
    "        expanded_set = set(self.base_vocabulary)\n",
    "        \n",
    "        # Add synonyms for terms in base vocabulary\n",
    "        for term in self.base_vocabulary:\n",
    "            if term in self.synonyms:\n",
    "                expanded_set.update(self.synonyms[term])\n",
    "            \n",
    "            # Add reverse mappings\n",
    "            for key, values in self.synonyms.items():\n",
    "                if term in values:\n",
    "                    expanded_set.add(key)\n",
    "        \n",
    "        # Add common objects if not already present\n",
    "        for obj in self.common_objects:\n",
    "            if obj not in expanded_set:\n",
    "                # Only add if it might be relevant\n",
    "                related = False\n",
    "                for term in self.base_vocabulary:\n",
    "                    if obj in term or term in obj:\n",
    "                        related = True\n",
    "                        break\n",
    "                if related or len(self.base_vocabulary) < 20:\n",
    "                    expanded_set.add(obj)\n",
    "        \n",
    "        self.expanded_vocabulary = sorted(expanded_set)\n",
    "        \n",
    "        print(f\"✅ Expanded vocabulary: {len(self.expanded_vocabulary)} terms\")\n",
    "        print(f\"   Added {len(self.expanded_vocabulary) - len(self.base_vocabulary)} new terms\")\n",
    "        \n",
    "        return self.expanded_vocabulary\n",
    "    \n",
    "    def get_detection_prompts(self, use_articles=True):\n",
    "        \"\"\"Generate detection prompts for object detection models\"\"\"\n",
    "        prompts = []\n",
    "        \n",
    "        for term in self.expanded_vocabulary:\n",
    "            prompts.append(term)\n",
    "            \n",
    "            if use_articles:\n",
    "                # Add article variants for better detection\n",
    "                prompts.append(f\"a {term}\")\n",
    "                prompts.append(f\"the {term}\")\n",
    "        \n",
    "        return prompts\n",
    "    \n",
    "    def normalize_term(self, detected_term):\n",
    "        \"\"\"Normalize detected term to base vocabulary\"\"\"\n",
    "        cleaned = detected_term.lower().strip()\n",
    "        \n",
    "        # Remove articles\n",
    "        for article in ['a ', 'an ', 'the ']:\n",
    "            if cleaned.startswith(article):\n",
    "                cleaned = cleaned[len(article):]\n",
    "        \n",
    "        # Direct match in base vocabulary\n",
    "        if cleaned in self.base_vocabulary:\n",
    "            return cleaned\n",
    "        \n",
    "        # Match in expanded vocabulary - map to base\n",
    "        if cleaned in self.expanded_vocabulary:\n",
    "            # Find the base term this maps to\n",
    "            for base_term in self.base_vocabulary:\n",
    "                if base_term in self.synonyms and cleaned in self.synonyms[base_term]:\n",
    "                    return base_term\n",
    "                if cleaned == base_term:\n",
    "                    return base_term\n",
    "            return cleaned  # Return as is if in expanded vocab\n",
    "        \n",
    "        # Fuzzy matching - check containment\n",
    "        for base_term in self.base_vocabulary:\n",
    "            if base_term in cleaned or cleaned in base_term:\n",
    "                return base_term\n",
    "        \n",
    "        # Check synonyms\n",
    "        for base_term, syns in self.synonyms.items():\n",
    "            if cleaned in syns:\n",
    "                return base_term\n",
    "        \n",
    "        return None  # Not in vocabulary\n",
    "\n",
    "# Initialize and extract vocabulary\n",
    "vocab_extractor = VocabularyExtractor(min_frequency=1)\n",
    "base_vocab = vocab_extractor.extract_from_training_data(train_df)\n",
    "expanded_vocab = vocab_extractor.expand_vocabulary()\n",
    "\n",
    "print(f\"\\n✅ Vocabulary extraction complete!\")\n",
    "print(f\"   Base: {len(base_vocab)} | Expanded: {len(expanded_vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63a11a2",
   "metadata": {},
   "source": [
    "## 3️⃣ Advanced Image Enhancement Pipeline\n",
    "\n",
    "**Techniques**: Super-resolution, adaptive sharpening, contrast enhancement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21d91e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedImageEnhancer:\n",
    "    \"\"\"\n",
    "    Advanced image enhancement for optimal object detection\n",
    "    \"\"\"\n",
    "    def __init__(self, target_size=(1024, 1024), min_size=512):\n",
    "        self.target_size = target_size\n",
    "        self.min_size = min_size\n",
    "    \n",
    "    def enhance(self, image_path):\n",
    "        \"\"\"Apply comprehensive enhancement pipeline\"\"\"\n",
    "        # Load image\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            raise ValueError(f\"Cannot load image: {image_path}\")\n",
    "        \n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        h, w = img_rgb.shape[:2]\n",
    "        \n",
    "        # Super-resolution upscaling for low-res images\n",
    "        if h < self.min_size or w < self.min_size:\n",
    "            scale_factor = max(self.min_size / h, self.min_size / w, 1.0)\n",
    "            if scale_factor > 1.0:\n",
    "                new_w = int(w * scale_factor)\n",
    "                new_h = int(h * scale_factor)\n",
    "                img_rgb = cv2.resize(img_rgb, (new_w, new_h), interpolation=cv2.INTER_CUBIC)\n",
    "        \n",
    "        # Convert to PIL for enhancement\n",
    "        pil_img = Image.fromarray(img_rgb)\n",
    "        \n",
    "        # Adaptive sharpening\n",
    "        pil_img = pil_img.filter(ImageFilter.UnsharpMask(radius=1.5, percent=150, threshold=3))\n",
    "        \n",
    "        # Contrast enhancement\n",
    "        enhancer = ImageEnhance.Contrast(pil_img)\n",
    "        pil_img = enhancer.enhance(1.15)\n",
    "        \n",
    "        # Brightness adjustment\n",
    "        enhancer = ImageEnhance.Brightness(pil_img)\n",
    "        pil_img = enhancer.enhance(1.05)\n",
    "        \n",
    "        # Color enhancement\n",
    "        enhancer = ImageEnhance.Color(pil_img)\n",
    "        pil_img = enhancer.enhance(1.1)\n",
    "        \n",
    "        # Resize to target size\n",
    "        if pil_img.size != self.target_size:\n",
    "            pil_img = pil_img.resize(self.target_size, Image.LANCZOS)\n",
    "        \n",
    "        return pil_img\n",
    "    \n",
    "    def multi_scale_enhance(self, image_path, scales=[0.75, 1.0, 1.25]):\n",
    "        \"\"\"Generate multi-scale enhanced versions for TTA\"\"\"\n",
    "        base_enhanced = self.enhance(image_path)\n",
    "        enhanced_versions = [base_enhanced]\n",
    "        \n",
    "        for scale in scales:\n",
    "            if scale != 1.0:\n",
    "                size = (int(self.target_size[0] * scale), int(self.target_size[1] * scale))\n",
    "                scaled = base_enhanced.resize(size, Image.LANCZOS)\n",
    "                scaled = scaled.resize(self.target_size, Image.LANCZOS)\n",
    "                enhanced_versions.append(scaled)\n",
    "        \n",
    "        return enhanced_versions\n",
    "\n",
    "# Initialize enhancer\n",
    "image_enhancer = AdvancedImageEnhancer(target_size=(896, 896), min_size=512)\n",
    "print(\"✅ Advanced image enhancer initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1250990",
   "metadata": {},
   "source": [
    "## 4️⃣ Multi-Model Ensemble Object Detection\n",
    "\n",
    "**Models**: OWL-ViT + Grounding DINO with Weighted Boxes Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ec731b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OWL-ViT\n",
    "print(\"\\n🔍 Loading object detection models...\")\n",
    "\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "\n",
    "print(\"Loading OWL-ViT...\")\n",
    "owlvit_processor = AutoProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "owlvit_model = AutoModelForZeroShotObjectDetection.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "owlvit_model = owlvit_model.to(device)\n",
    "owlvit_model.eval()\n",
    "print(\"✅ OWL-ViT ready\")\n",
    "\n",
    "# Try to load Grounding DINO\n",
    "grounding_dino_available = False\n",
    "try:\n",
    "    print(\"Loading Grounding DINO...\")\n",
    "    model_id = \"IDEA-Research/grounding-dino-base\"\n",
    "    grounding_dino_processor = AutoProcessor.from_pretrained(model_id)\n",
    "    grounding_dino_model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id).to(device)\n",
    "    grounding_dino_model.eval()\n",
    "    grounding_dino_available = True\n",
    "    print(\"✅ Grounding DINO ready\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Grounding DINO not available: {e}\")\n",
    "    print(\"   Will use OWL-ViT only\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"🎯 Detection ensemble: {'OWL-ViT + Grounding DINO' if grounding_dino_available else 'OWL-ViT only'}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2045d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleObjectDetector:\n",
    "    \"\"\"\n",
    "    Multi-model ensemble detector with Weighted Boxes Fusion\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_extractor, image_enhancer, use_enhancement=True):\n",
    "        self.vocab_extractor = vocab_extractor\n",
    "        self.image_enhancer = image_enhancer\n",
    "        self.use_enhancement = use_enhancement\n",
    "        \n",
    "    def detect_owlvit(self, image, prompts, threshold=0.05):\n",
    "        \"\"\"Detect objects using OWL-ViT\"\"\"\n",
    "        inputs = owlvit_processor(text=prompts, images=image, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = owlvit_model(**inputs)\n",
    "        \n",
    "        target_sizes = torch.tensor([image.size[::-1]]).to(device)\n",
    "        results = owlvit_processor.post_process_object_detection(\n",
    "            outputs, target_sizes=target_sizes, threshold=threshold\n",
    "        )[0]\n",
    "        \n",
    "        boxes = results['boxes'].cpu().numpy()\n",
    "        scores = results['scores'].cpu().numpy()\n",
    "        labels = results['labels'].cpu().numpy()\n",
    "        \n",
    "        return boxes, scores, labels\n",
    "    \n",
    "    def detect_grounding_dino(self, image, prompts, box_threshold=0.25, text_threshold=0.2):\n",
    "        \"\"\"Detect objects using Grounding DINO\"\"\"\n",
    "        if not grounding_dino_available:\n",
    "            return np.array([]), np.array([]), np.array([])\n",
    "        \n",
    "        try:\n",
    "            # Limit prompts to avoid token limits\n",
    "            limited_prompts = prompts[:50]\n",
    "            text = '. '.join([p.lower() for p in limited_prompts]) + '.'\n",
    "            \n",
    "            inputs = grounding_dino_processor(images=image, text=text, return_tensors=\"pt\").to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = grounding_dino_model(**inputs)\n",
    "            \n",
    "            results = grounding_dino_processor.post_process_grounded_object_detection(\n",
    "                outputs,\n",
    "                inputs.input_ids,\n",
    "                box_threshold=box_threshold,\n",
    "                text_threshold=text_threshold,\n",
    "                target_sizes=[image.size[::-1]]\n",
    "            )[0]\n",
    "            \n",
    "            boxes = results['boxes'].cpu().numpy()\n",
    "            scores = results['scores'].cpu().numpy()\n",
    "            raw_labels = results['labels']\n",
    "            \n",
    "            # Map string or integer labels back to prompt indices\n",
    "            mapped_indices = []\n",
    "            for entry in raw_labels:\n",
    "                if isinstance(entry, (int, np.integer)):\n",
    "                    mapped_idx = min(int(entry), len(limited_prompts) - 1)\n",
    "                else:\n",
    "                    entry_text = str(entry).strip().lower()\n",
    "                    match_idx = None\n",
    "                    for idx, prompt in enumerate(limited_prompts):\n",
    "                        prompt_text = prompt.lower()\n",
    "                        if entry_text == prompt_text:\n",
    "                            match_idx = idx\n",
    "                            break\n",
    "                    if match_idx is None:\n",
    "                        for idx, prompt in enumerate(limited_prompts):\n",
    "                            prompt_text = prompt.lower()\n",
    "                            if prompt_text in entry_text or entry_text in prompt_text:\n",
    "                                match_idx = idx\n",
    "                                break\n",
    "                    mapped_idx = match_idx if match_idx is not None else 0\n",
    "                mapped_indices.append(mapped_idx)\n",
    "            \n",
    "            labels = np.array(mapped_indices, dtype=int)\n",
    "            \n",
    "            return boxes, scores, labels\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Grounding DINO error: {e}\")\n",
    "            return np.array([]), np.array([]), np.array([])\n",
    "    \n",
    "    def weighted_boxes_fusion(self, boxes_list, scores_list, labels_list, iou_threshold=0.5):\n",
    "        \"\"\"Apply Weighted Boxes Fusion to merge detections\"\"\"\n",
    "        if len(boxes_list) == 0:\n",
    "            return np.array([]), np.array([]), np.array([])\n",
    "        \n",
    "        # Simple NMS-based fusion\n",
    "        from torchvision.ops import nms\n",
    "        \n",
    "        all_boxes = np.vstack(boxes_list)\n",
    "        all_scores = np.concatenate(scores_list)\n",
    "        all_labels = np.concatenate(labels_list)\n",
    "        \n",
    "        # Apply NMS per class\n",
    "        final_boxes = []\n",
    "        final_scores = []\n",
    "        final_labels = []\n",
    "        \n",
    "        unique_labels = np.unique(all_labels)\n",
    "        for label in unique_labels:\n",
    "            mask = all_labels == label\n",
    "            class_boxes = torch.tensor(all_boxes[mask], dtype=torch.float32)\n",
    "            class_scores = torch.tensor(all_scores[mask], dtype=torch.float32)\n",
    "            \n",
    "            keep = nms(class_boxes, class_scores, iou_threshold)\n",
    "            \n",
    "            final_boxes.append(all_boxes[mask][keep.numpy()])\n",
    "            final_scores.append(all_scores[mask][keep.numpy()])\n",
    "            final_labels.append(np.full(len(keep), label))\n",
    "        \n",
    "        if final_boxes:\n",
    "            return (np.vstack(final_boxes), \n",
    "                    np.concatenate(final_scores), \n",
    "                    np.concatenate(final_labels))\n",
    "        else:\n",
    "            return np.array([]), np.array([]), np.array([])\n",
    "    \n",
    "    def detect(self, image_path, use_tta=False):\n",
    "        \"\"\"\n",
    "        Main detection method with ensemble and TTA\n",
    "        \"\"\"\n",
    "        # Enhance image\n",
    "        if self.use_enhancement:\n",
    "            image = self.image_enhancer.enhance(image_path)\n",
    "        else:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        # Get detection prompts sorted by frequency (most frequent first)\n",
    "        sorted_base = vocab_extractor.base_vocabulary\n",
    "        expanded_sorted = []\n",
    "        for term in sorted_base:\n",
    "            if term in vocab_extractor.expanded_vocabulary:\n",
    "                expanded_sorted.append(term)\n",
    "        for term in vocab_extractor.expanded_vocabulary:\n",
    "            if term not in expanded_sorted:\n",
    "                expanded_sorted.append(term)\n",
    "        prompts = expanded_sorted\n",
    "        \n",
    "        boxes_list = []\n",
    "        scores_list = []\n",
    "        labels_list = []\n",
    "        \n",
    "        # OWL-ViT detection\n",
    "        boxes_owl, scores_owl, labels_owl = self.detect_owlvit(image, prompts, threshold=0.05)\n",
    "        if len(boxes_owl) > 0:\n",
    "            boxes_list.append(boxes_owl)\n",
    "            scores_list.append(scores_owl)\n",
    "            labels_list.append(labels_owl)\n",
    "        \n",
    "        # Grounding DINO detection\n",
    "        if grounding_dino_available:\n",
    "            boxes_gdino, scores_gdino, labels_gdino = self.detect_grounding_dino(image, prompts)\n",
    "            if len(boxes_gdino) > 0:\n",
    "                boxes_list.append(boxes_gdino)\n",
    "                scores_list.append(scores_gdino)\n",
    "                labels_list.append(labels_gdino)\n",
    "        \n",
    "        # Merge detections with WBF\n",
    "        if boxes_list:\n",
    "            merged_boxes, merged_scores, merged_labels = self.weighted_boxes_fusion(\n",
    "                boxes_list, scores_list, labels_list, iou_threshold=0.5\n",
    "            )\n",
    "        else:\n",
    "            merged_boxes, merged_scores, merged_labels = np.array([]), np.array([]), np.array([])\n",
    "        \n",
    "        # Map labels to terms and filter to base vocabulary\n",
    "        filtered_boxes = []\n",
    "        filtered_scores = []\n",
    "        filtered_labels = []\n",
    "        filtered_terms = []\n",
    "        \n",
    "        for box, score, label in zip(merged_boxes, merged_scores, merged_labels):\n",
    "            detected_term = prompts[int(label)]\n",
    "            normalized_term = self.vocab_extractor.normalize_term(detected_term)\n",
    "            \n",
    "            if normalized_term and normalized_term in base_vocab:\n",
    "                filtered_boxes.append(box)\n",
    "                filtered_scores.append(score)\n",
    "                filtered_labels.append(base_vocab.index(normalized_term))\n",
    "                filtered_terms.append(normalized_term)\n",
    "        \n",
    "        if filtered_boxes:\n",
    "            return (np.array(filtered_boxes), \n",
    "                    np.array(filtered_scores), \n",
    "                    np.array(filtered_labels), \n",
    "                    filtered_terms)\n",
    "        else:\n",
    "            return np.array([]), np.array([]), np.array([]), []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7903d8",
   "metadata": {},
   "source": [
    "## 5️⃣ ChangeFormer Architecture with Cross-Attention\n",
    "\n",
    "**Advanced change localization model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1455dbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "class ChangeFormerOptimized(nn.Module):\n",
    "    \"\"\"\n",
    "    Optimized ChangeFormer with cross-attention and better fusion\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone='vit_base_patch16_224', num_heads=8, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Feature extractor\n",
    "        self.encoder = timm.create_model(backbone, pretrained=True, num_classes=0)\n",
    "        embed_dim = self.encoder.num_features\n",
    "        \n",
    "        # Cross-attention layers\n",
    "        self.cross_attn_1to2 = nn.MultiheadAttention(\n",
    "            embed_dim=embed_dim, num_heads=num_heads, batch_first=True, dropout=0.1\n",
    "        )\n",
    "        self.cross_attn_2to1 = nn.MultiheadAttention(\n",
    "            embed_dim=embed_dim, num_heads=num_heads, batch_first=True, dropout=0.1\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Fusion network\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(embed_dim * 4, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        # Change prediction head\n",
    "        self.change_head = nn.Linear(hidden_dim // 4, 1)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize fusion and head weights\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, img1, img2):\n",
    "        \"\"\"Forward pass with cross-attention\"\"\"\n",
    "        # Extract features\n",
    "        feat1 = self.encoder.forward_features(img1)\n",
    "        feat2 = self.encoder.forward_features(img2)\n",
    "        \n",
    "        # Cross-attention\n",
    "        attn_1to2, _ = self.cross_attn_1to2(feat1, feat2, feat2)\n",
    "        attn_2to1, _ = self.cross_attn_2to1(feat2, feat1, feat1)\n",
    "        \n",
    "        # Normalize\n",
    "        attn_1to2 = self.norm1(attn_1to2 + feat1)\n",
    "        attn_2to1 = self.norm2(attn_2to1 + feat2)\n",
    "        \n",
    "        # Global pooling\n",
    "        feat1_pool = feat1.mean(dim=1)\n",
    "        feat2_pool = feat2.mean(dim=1)\n",
    "        attn_1to2_pool = attn_1to2.mean(dim=1)\n",
    "        attn_2to1_pool = attn_2to1.mean(dim=1)\n",
    "        \n",
    "        # Concatenate all features\n",
    "        combined = torch.cat([feat1_pool, feat2_pool, attn_1to2_pool, attn_2to1_pool], dim=1)\n",
    "        \n",
    "        # Fusion\n",
    "        fused = self.fusion(combined)\n",
    "        \n",
    "        # Change prediction\n",
    "        change_logits = self.change_head(fused).squeeze(-1)\n",
    "        \n",
    "        return change_logits\n",
    "\n",
    "# Load or create ChangeFormer\n",
    "changeformer_path = 'changeformer_model.pth'\n",
    "alternative_path = 'changeformer_best.pth'\n",
    "\n",
    "changeformer_model = ChangeFormerOptimized().to(device)\n",
    "\n",
    "# Try to load pre-trained weights\n",
    "loaded = False\n",
    "for path in [changeformer_path, alternative_path]:\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            print(f\"Loading pre-trained ChangeFormer from {path}...\")\n",
    "            state_dict = torch.load(path, map_location=device)\n",
    "            changeformer_model.load_state_dict(state_dict, strict=False)\n",
    "            loaded = True\n",
    "            print(f\"✅ Loaded pre-trained ChangeFormer from {path}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Could not load from {path}: {e}\")\n",
    "\n",
    "if not loaded:\n",
    "    print(\"⚠️ No pre-trained ChangeFormer found, using initialized model\")\n",
    "\n",
    "changeformer_model.eval()\n",
    "print(f\"📊 ChangeFormer parameters: {sum(p.numel() for p in changeformer_model.parameters()):,}\")\n",
    "print(\"✅ ChangeFormer ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0097e26",
   "metadata": {},
   "source": [
    "## 5️⃣(a) Prepare DataLoaders for ChangeFormer Fine-tuning\n",
    "\n",
    "**Create training and validation datasets with proper transforms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29507a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "\n",
    "class ChangePairDataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for image pairs with change labels\n",
    "    \"\"\"\n",
    "    def __init__(self, df, root_dir, transform=None, augment=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.augment = augment\n",
    "        \n",
    "        # Augmentation for training\n",
    "        if augment:\n",
    "            import albumentations as A\n",
    "            from albumentations.pytorch import ToTensorV2\n",
    "            \n",
    "            self.aug_transform = A.Compose([\n",
    "                A.Resize(224, 224),\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "                A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=15, val_shift_limit=10, p=0.3),\n",
    "                A.Rotate(limit=15, p=0.3),\n",
    "                A.GaussNoise(var_limit=(10, 30), p=0.2),\n",
    "                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                ToTensorV2()\n",
    "            ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_id = row['img_id']\n",
    "        \n",
    "        img1_path = os.path.join(self.root_dir, 'data/data', f'{img_id}_1.png')\n",
    "        img2_path = os.path.join(self.root_dir, 'data/data', f'{img_id}_2.png')\n",
    "        \n",
    "        # Load images\n",
    "        img1 = cv2.imread(img1_path)\n",
    "        img2 = cv2.imread(img2_path)\n",
    "        \n",
    "        if img1 is None or img2 is None:\n",
    "            raise ValueError(f\"Could not load images for {img_id}\")\n",
    "        \n",
    "        img1_rgb = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "        img2_rgb = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Apply augmentation or standard transform\n",
    "        if self.augment:\n",
    "            augmented1 = self.aug_transform(image=img1_rgb)\n",
    "            augmented2 = self.aug_transform(image=img2_rgb)\n",
    "            img1_tensor = augmented1['image']\n",
    "            img2_tensor = augmented2['image']\n",
    "        else:\n",
    "            img1_pil = Image.fromarray(img1_rgb)\n",
    "            img2_pil = Image.fromarray(img2_rgb)\n",
    "            img1_tensor = self.transform(img1_pil)\n",
    "            img2_tensor = self.transform(img2_pil)\n",
    "        \n",
    "        # Create binary label: 1 if any change exists, 0 otherwise\n",
    "        has_change = False\n",
    "        for col in ['added_objs', 'removed_objs', 'changed_objs']:\n",
    "            if isinstance(row[col], str):\n",
    "                val = row[col].strip().lower()\n",
    "                if val and val not in ['', 'none', 'null', 'nan']:\n",
    "                    has_change = True\n",
    "                    break\n",
    "        \n",
    "        label = torch.tensor(1.0 if has_change else 0.0, dtype=torch.float32)\n",
    "        \n",
    "        return img1_tensor, img2_tensor, label\n",
    "\n",
    "# Define standard transform for validation\n",
    "changeformer_transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create train/val split with stratification\n",
    "print(\"\\n📊 Creating train/validation split...\")\n",
    "\n",
    "# Create stratification labels\n",
    "strat_labels = []\n",
    "for _, row in train_df.iterrows():\n",
    "    has_added = isinstance(row['added_objs'], str) and row['added_objs'].lower() not in ['', 'none']\n",
    "    has_removed = isinstance(row['removed_objs'], str) and row['removed_objs'].lower() not in ['', 'none']\n",
    "    has_changed = isinstance(row['changed_objs'], str) and row['changed_objs'].lower() not in ['', 'none']\n",
    "    \n",
    "    # Create 8 categories based on combinations\n",
    "    label = (4 * int(has_added) + 2 * int(has_removed) + 1 * int(has_changed))\n",
    "    strat_labels.append(label)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_split, val_split = train_test_split(\n",
    "    train_df, \n",
    "    test_size=0.15, \n",
    "    random_state=42,\n",
    "    stratify=strat_labels\n",
    ")\n",
    "\n",
    "print(f\"✅ Training samples: {len(train_split)}\")\n",
    "print(f\"✅ Validation samples: {len(val_split)}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ChangePairDataset(\n",
    "    train_split, \n",
    "    data_dir, \n",
    "    transform=changeformer_transform,\n",
    "    augment=True  # Use augmentation for training\n",
    ")\n",
    "\n",
    "val_dataset = ChangePairDataset(\n",
    "    val_split, \n",
    "    data_dir, \n",
    "    transform=changeformer_transform,\n",
    "    augment=False  # No augmentation for validation\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 16\n",
    "num_workers = 2 if device.type == 'cuda' else 0\n",
    "\n",
    "train_loader = data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True if device.type == 'cuda' else False\n",
    ")\n",
    "\n",
    "val_loader = data.DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True if device.type == 'cuda' else False\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ DataLoaders ready!\")\n",
    "print(f\"   Batch size: {batch_size}\")\n",
    "print(f\"   Train batches: {len(train_loader)}\")\n",
    "print(f\"   Val batches: {len(val_loader)}\")\n",
    "\n",
    "# Check label distribution\n",
    "train_labels = [train_dataset[i][2].item() for i in range(min(100, len(train_dataset)))]\n",
    "print(f\"\\n📈 Label distribution (sample of {len(train_labels)}):\")\n",
    "print(f\"   Change: {sum(train_labels)} ({100*sum(train_labels)/len(train_labels):.1f}%)\")\n",
    "print(f\"   No change: {len(train_labels) - sum(train_labels)} ({100*(1-sum(train_labels)/len(train_labels)):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72c2a12",
   "metadata": {},
   "source": [
    "## 5️⃣(b) Fine-tune ChangeFormer with Advanced Training Loop\n",
    "\n",
    "**Train with early stopping, learning rate scheduling, and checkpointing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1797077d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_changeformer(\n",
    "    model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    num_epochs=50, \n",
    "    lr=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    patience=7\n",
    "):\n",
    "    \"\"\"\n",
    "    Train ChangeFormer with advanced features:\n",
    "    - Early stopping\n",
    "    - Learning rate scheduling\n",
    "    - Gradient clipping\n",
    "    - In-memory model checkpointing (no disk I/O)\n",
    "    - Comprehensive logging\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (best_model, history) - The best model and training history\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"🚀 TRAINING CHANGEFORMER\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Setup optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), \n",
    "        lr=lr, \n",
    "        weight_decay=weight_decay,\n",
    "        betas=(0.9, 0.999)\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='min', \n",
    "        factor=0.5, \n",
    "        patience=3,\n",
    "        verbose=True,\n",
    "        min_lr=1e-7\n",
    "    )\n",
    "    \n",
    "    # Loss function with class weights for imbalanced data\n",
    "    # Calculate positive weight\n",
    "    train_labels = []\n",
    "    for _, _, label in train_loader:\n",
    "        train_labels.extend(label.cpu().numpy())\n",
    "    \n",
    "    pos_count = sum(train_labels)\n",
    "    neg_count = len(train_labels) - pos_count\n",
    "    pos_weight = torch.tensor([neg_count / pos_count]).to(device) if pos_count > 0 else torch.tensor([1.0]).to(device)\n",
    "    \n",
    "    print(f\"\\n📊 Class distribution:\")\n",
    "    print(f\"   Positive (change): {pos_count} ({100*pos_count/len(train_labels):.1f}%)\")\n",
    "    print(f\"   Negative (no change): {neg_count} ({100*neg_count/len(train_labels):.1f}%)\")\n",
    "    print(f\"   Positive weight: {pos_weight.item():.2f}\")\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    \n",
    "    # Training state\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_acc = 0.0\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None  # Store best model state in memory\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'lr': []\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n🎯 Training configuration:\")\n",
    "    print(f\"   Epochs: {num_epochs}\")\n",
    "    print(f\"   Learning rate: {lr}\")\n",
    "    print(f\"   Weight decay: {weight_decay}\")\n",
    "    print(f\"   Patience: {patience}\")\n",
    "    print(f\"   Batch size: {train_loader.batch_size}\")\n",
    "    print(f\"   Device: {device}\")\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f'Training', leave=False)\n",
    "        for batch_idx, (img1, img2, labels) in enumerate(pbar):\n",
    "            img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(img1, img2)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            # Metrics\n",
    "            train_loss += loss.item()\n",
    "            predictions = (torch.sigmoid(logits) > 0.5).float()\n",
    "            train_correct += (predictions == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{100*train_correct/train_total:.2f}%'\n",
    "            })\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_acc = 100 * train_correct / train_total\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        val_predictions = []\n",
    "        val_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(val_loader, desc=f'Validation', leave=False)\n",
    "            for img1, img2, labels in pbar:\n",
    "                img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
    "                \n",
    "                logits = model(img1, img2)\n",
    "                loss = criterion(logits, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                predictions = (torch.sigmoid(logits) > 0.5).float()\n",
    "                val_correct += (predictions == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "                \n",
    "                val_predictions.extend(predictions.cpu().numpy())\n",
    "                val_targets.extend(labels.cpu().numpy())\n",
    "                \n",
    "                pbar.set_postfix({\n",
    "                    'loss': f'{loss.item():.4f}',\n",
    "                    'acc': f'{100*val_correct/val_total:.2f}%'\n",
    "                })\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        \n",
    "        # Calculate F1 score\n",
    "        from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "        val_f1 = f1_score(val_targets, val_predictions, zero_division=0)\n",
    "        val_precision = precision_score(val_targets, val_predictions, zero_division=0)\n",
    "        val_recall = recall_score(val_targets, val_predictions, zero_division=0)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(avg_val_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Store history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['lr'].append(current_lr)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"\\n📊 Epoch {epoch+1} Summary:\")\n",
    "        print(f\"   Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"   Val Loss:   {avg_val_loss:.4f} | Val Acc:   {val_acc:.2f}%\")\n",
    "        print(f\"   Val F1:     {val_f1:.4f} | Val Precision: {val_precision:.4f} | Val Recall: {val_recall:.4f}\")\n",
    "        print(f\"   Learning Rate: {current_lr:.2e}\")\n",
    "        \n",
    "        # Check for improvement\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_val_acc = val_acc\n",
    "            epochs_no_improve = 0\n",
    "            \n",
    "            # Save best model state in memory (deep copy)\n",
    "            import copy\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            print(f\"   ✅ New best model saved in memory! (Val Loss: {best_val_loss:.4f})\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"   ⚠️ No improvement for {epochs_no_improve} epoch(s)\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"\\n⏹️ Early stopping triggered after {epoch+1} epochs\")\n",
    "            print(f\"   Best validation loss: {best_val_loss:.4f}\")\n",
    "            print(f\"   Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "            break\n",
    "    \n",
    "    # Training complete\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"✅ TRAINING COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"📊 Final Results:\")\n",
    "    print(f\"   Best Val Loss: {best_val_loss:.4f}\")\n",
    "    print(f\"   Best Val Acc: {best_val_acc:.2f}%\")\n",
    "    \n",
    "    # Load best model state\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"   ✅ Best model state loaded into model\")\n",
    "    \n",
    "    # Plot training history\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Loss plot\n",
    "    axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "    axes[0].plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Training and Validation Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy plot\n",
    "    axes[1].plot(history['train_acc'], label='Train Acc', marker='o')\n",
    "    axes[1].plot(history['val_acc'], label='Val Acc', marker='s')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy (%)')\n",
    "    axes[1].set_title('Training and Validation Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Learning rate plot\n",
    "    axes[2].plot(history['lr'], label='Learning Rate', marker='o', color='green')\n",
    "    axes[2].set_xlabel('Epoch')\n",
    "    axes[2].set_ylabel('Learning Rate')\n",
    "    axes[2].set_title('Learning Rate Schedule')\n",
    "    axes[2].set_yscale('log')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n✅ Training complete! Best model is now loaded in the model variable.\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Start training\n",
    "print(\"\\n🚀 Starting ChangeFormer fine-tuning...\")\n",
    "print(\"This may take a while depending on your hardware...\\n\")\n",
    "\n",
    "# Train the model - returns the best model and training history\n",
    "changeformer_model, training_history = train_changeformer(\n",
    "    model=changeformer_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=50,\n",
    "    lr=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    patience=7\n",
    ")\n",
    "\n",
    "# Model is already loaded with best weights and ready for inference\n",
    "changeformer_model.eval()\n",
    "print(\"✅ Best model is ready for inference!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e7d61f",
   "metadata": {},
   "source": [
    "## 5️⃣(c) Evaluate Trained ChangeFormer\n",
    "\n",
    "**Test the fine-tuned model on validation samples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a203d26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained ChangeFormer model\n",
    "def evaluate_changeformer(model, test_samples_df, num_samples=10):\n",
    "    \"\"\"\n",
    "    Evaluate ChangeFormer on test samples with visualization\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"🧪 EVALUATING CHANGEFORMER ON VALIDATION SAMPLES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    model.eval()\n",
    "    transform = T.Compose([\n",
    "        T.Resize((224, 224)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for idx, row in test_samples_df.head(num_samples).iterrows():\n",
    "        img_id = row['img_id']\n",
    "        \n",
    "        # Load images\n",
    "        img1_path = os.path.join(data_dir, 'data/data', f'{img_id}_1.png')\n",
    "        img2_path = os.path.join(data_dir, 'data/data', f'{img_id}_2.png')\n",
    "        \n",
    "        img1_pil = Image.open(img1_path).convert('RGB')\n",
    "        img2_pil = Image.open(img2_path).convert('RGB')\n",
    "        \n",
    "        img1_tensor = transform(img1_pil).unsqueeze(0).to(device)\n",
    "        img2_tensor = transform(img2_pil).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Predict\n",
    "        with torch.no_grad():\n",
    "            logits = model(img1_tensor, img2_tensor)\n",
    "            change_score = torch.sigmoid(logits).item()\n",
    "            prediction = 1 if change_score > 0.5 else 0\n",
    "        \n",
    "        # Ground truth\n",
    "        has_change = False\n",
    "        for col in ['added_objs', 'removed_objs', 'changed_objs']:\n",
    "            if isinstance(row[col], str):\n",
    "                val = row[col].strip().lower()\n",
    "                if val and val not in ['', 'none', 'null', 'nan']:\n",
    "                    has_change = True\n",
    "                    break\n",
    "        \n",
    "        gt_label = 1 if has_change else 0\n",
    "        \n",
    "        # Store result\n",
    "        results.append({\n",
    "            'img_id': img_id,\n",
    "            'change_score': change_score,\n",
    "            'prediction': prediction,\n",
    "            'ground_truth': gt_label,\n",
    "            'correct': prediction == gt_label,\n",
    "            'added': row['added_objs'],\n",
    "            'removed': row['removed_objs'],\n",
    "            'changed': row['changed_objs']\n",
    "        })\n",
    "        \n",
    "        # Print result\n",
    "        status = \"✅\" if prediction == gt_label else \"❌\"\n",
    "        print(f\"\\n{status} Image: {img_id}\")\n",
    "        print(f\"   Change Score: {change_score:.4f}\")\n",
    "        print(f\"   Prediction: {'CHANGE' if prediction == 1 else 'NO CHANGE'}\")\n",
    "        print(f\"   Ground Truth: {'CHANGE' if gt_label == 1 else 'NO CHANGE'}\")\n",
    "        if has_change:\n",
    "            print(f\"   Changes: Added={row['added_objs']}, Removed={row['removed_objs']}, Changed={row['changed_objs']}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    accuracy = sum([r['correct'] for r in results]) / len(results)\n",
    "    avg_score = sum([r['change_score'] for r in results]) / len(results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"📊 EVALUATION SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Samples evaluated: {len(results)}\")\n",
    "    print(f\"Accuracy: {100*accuracy:.2f}%\")\n",
    "    print(f\"Average change score: {avg_score:.4f}\")\n",
    "    print(f\"Correct predictions: {sum([r['correct'] for r in results])}/{len(results)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run evaluation on validation samples\n",
    "eval_samples = val_split.sample(min(15, len(val_split)), random_state=42)\n",
    "evaluation_results = evaluate_changeformer(changeformer_model, eval_samples, num_samples=15)\n",
    "\n",
    "print(\"\\n✅ Evaluation complete! Model is ready for the full pipeline.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ac3587",
   "metadata": {},
   "source": [
    "## 6️⃣ Smart Object Matching with Hungarian Algorithm\n",
    "\n",
    "**Multi-criteria matching: Label + IoU + Position similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cabdf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "class SmartObjectMatcher:\n",
    "    \"\"\"\n",
    "    Advanced object matching with multiple criteria\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.weights = {\n",
    "            'label': 0.5,    # Label match weight\n",
    "            'iou': 0.3,      # IoU weight  \n",
    "            'position': 0.2  # Position similarity weight\n",
    "        }\n",
    "    \n",
    "    def compute_iou(self, box1, box2):\n",
    "        \"\"\"Compute IoU between two boxes\"\"\"\n",
    "        x1 = max(box1[0], box2[0])\n",
    "        y1 = max(box1[1], box2[1])\n",
    "        x2 = min(box1[2], box2[2])\n",
    "        y2 = min(box1[3], box2[3])\n",
    "        \n",
    "        inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "        box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "        box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "        union_area = box1_area + box2_area - inter_area\n",
    "        \n",
    "        return inter_area / union_area if union_area > 0 else 0\n",
    "    \n",
    "    def compute_position_similarity(self, box1, box2):\n",
    "        \"\"\"Compute position similarity (center distance)\"\"\"\n",
    "        center1 = np.array([(box1[0] + box1[2]) / 2, (box1[1] + box1[3]) / 2])\n",
    "        center2 = np.array([(box2[0] + box2[2]) / 2, (box2[1] + box2[3]) / 2])\n",
    "        \n",
    "        # Normalize by image diagonal\n",
    "        diagonal = np.sqrt(1024**2 + 1024**2)\n",
    "        distance = np.linalg.norm(center1 - center2)\n",
    "        \n",
    "        # Convert to similarity (closer = higher)\n",
    "        similarity = 1 - min(distance / diagonal, 1.0)\n",
    "        return similarity\n",
    "    \n",
    "    def match_objects(self, boxes1, labels1, boxes2, labels2, iou_threshold=0.3):\n",
    "        \"\"\"\n",
    "        Match objects using Hungarian algorithm with multi-criteria cost\n",
    "        \"\"\"\n",
    "        if len(boxes1) == 0 or len(boxes2) == 0:\n",
    "            return [], list(range(len(boxes1))), list(range(len(boxes2)))\n",
    "        \n",
    "        # Build cost matrix\n",
    "        cost_matrix = np.ones((len(boxes1), len(boxes2))) * 1e6\n",
    "        \n",
    "        for i in range(len(boxes1)):\n",
    "            for j in range(len(boxes2)):\n",
    "                # Label match (0 if same, 1 if different)\n",
    "                label_cost = 0 if labels1[i] == labels2[j] else 1\n",
    "                \n",
    "                # IoU cost (1 - IoU)\n",
    "                iou = self.compute_iou(boxes1[i], boxes2[j])\n",
    "                iou_cost = 1 - iou\n",
    "                \n",
    "                # Position cost (1 - similarity)\n",
    "                pos_sim = self.compute_position_similarity(boxes1[i], boxes2[j])\n",
    "                pos_cost = 1 - pos_sim\n",
    "                \n",
    "                # Weighted combination\n",
    "                total_cost = (\n",
    "                    self.weights['label'] * label_cost +\n",
    "                    self.weights['iou'] * iou_cost +\n",
    "                    self.weights['position'] * pos_cost\n",
    "                )\n",
    "                \n",
    "                cost_matrix[i, j] = total_cost\n",
    "        \n",
    "        # Hungarian algorithm\n",
    "        row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "        \n",
    "        # Filter matches by threshold\n",
    "        matched_pairs = []\n",
    "        unmatched1 = set(range(len(boxes1)))\n",
    "        unmatched2 = set(range(len(boxes2)))\n",
    "        \n",
    "        for i, j in zip(row_ind, col_ind):\n",
    "            # Check if match is valid (same label and reasonable IoU)\n",
    "            if labels1[i] == labels2[j]:\n",
    "                iou = self.compute_iou(boxes1[i], boxes2[j])\n",
    "                if iou >= iou_threshold or cost_matrix[i, j] < 0.5:\n",
    "                    matched_pairs.append((i, j, iou))\n",
    "                    unmatched1.discard(i)\n",
    "                    unmatched2.discard(j)\n",
    "        \n",
    "        return matched_pairs, list(unmatched1), list(unmatched2)\n",
    "\n",
    "# Initialize matcher\n",
    "object_matcher = SmartObjectMatcher()\n",
    "print(\"✅ Smart object matcher initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e289d93c",
   "metadata": {},
   "source": [
    "## 7️⃣ Proper Cross-Validation with Threshold Calibration\n",
    "\n",
    "**Stratified K-Fold with F1 optimization per category**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85096cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossValidationCalibrator:\n",
    "    \"\"\"\n",
    "    Cross-validation based threshold calibration with F1 optimization\n",
    "    \"\"\"\n",
    "    def __init__(self, n_splits=5):\n",
    "        self.n_splits = n_splits\n",
    "        self.best_thresholds = {\n",
    "            'detection_conf': 0.08,\n",
    "            'change_score': 0.25,\n",
    "            'iou_match': 0.3,\n",
    "            'iou_change': 0.5\n",
    "        }\n",
    "        self.calibration_history = []\n",
    "    \n",
    "    def create_stratified_labels(self, train_df):\n",
    "        \"\"\"Create stratification labels based on change categories\"\"\"\n",
    "        labels = []\n",
    "        for _, row in train_df.iterrows():\n",
    "            has_added = isinstance(row['added_objs'], str) and row['added_objs'].lower() not in ['', 'none']\n",
    "            has_removed = isinstance(row['removed_objs'], str) and row['removed_objs'].lower() not in ['', 'none']\n",
    "            has_changed = isinstance(row['changed_objs'], str) and row['changed_objs'].lower() not in ['', 'none']\n",
    "            \n",
    "            # Create 8 categories based on combinations\n",
    "            label = (\n",
    "                4 * int(has_added) +\n",
    "                2 * int(has_removed) +\n",
    "                1 * int(has_changed)\n",
    "            )\n",
    "            labels.append(label)\n",
    "        \n",
    "        return np.array(labels)\n",
    "    \n",
    "    def parse_labels(self, label_str):\n",
    "        \"\"\"Parse label string to set of terms\"\"\"\n",
    "        if pd.isna(label_str) or not isinstance(label_str, str):\n",
    "            return set()\n",
    "        \n",
    "        label_str = label_str.strip().lower()\n",
    "        if label_str in ['', 'none', 'null', 'nan']:\n",
    "            return set()\n",
    "        \n",
    "        tokens = re.split(r'[,;&\\s]+', label_str)\n",
    "        terms = set()\n",
    "        for token in tokens:\n",
    "            token = token.strip()\n",
    "            if token and token != 'none':\n",
    "                # Normalize to base vocabulary\n",
    "                normalized = vocab_extractor.normalize_term(token)\n",
    "                if normalized:\n",
    "                    terms.add(normalized)\n",
    "        \n",
    "        return terms\n",
    "    \n",
    "    def calculate_f1(self, true_set, pred_set):\n",
    "        \"\"\"Calculate F1 score for set comparison\"\"\"\n",
    "        if len(true_set) == 0 and len(pred_set) == 0:\n",
    "            return 1.0\n",
    "        if len(true_set) == 0 or len(pred_set) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        tp = len(true_set & pred_set)\n",
    "        fp = len(pred_set - true_set)\n",
    "        fn = len(true_set - pred_set)\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        \n",
    "        if precision + recall == 0:\n",
    "            return 0\n",
    "        \n",
    "        return 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    def predict_with_thresholds(self, img_id, thresholds):\n",
    "        \"\"\"Make prediction with specific threshold values\"\"\"\n",
    "        img1_path = os.path.join(data_dir, 'data/data', f'{img_id}_1.png')\n",
    "        img2_path = os.path.join(data_dir, 'data/data', f'{img_id}_2.png')\n",
    "        \n",
    "        try:\n",
    "            # Detect objects\n",
    "            boxes1, scores1, labels1, terms1 = ensemble_detector.detect(img1_path)\n",
    "            boxes2, scores2, labels2, terms2 = ensemble_detector.detect(img2_path)\n",
    "            \n",
    "            # Filter by confidence threshold\n",
    "            conf_thresh = thresholds['detection_conf']\n",
    "            \n",
    "            if len(scores1) > 0:\n",
    "                keep1 = scores1 >= conf_thresh\n",
    "                boxes1 = boxes1[keep1]\n",
    "                scores1 = scores1[keep1]\n",
    "                labels1 = labels1[keep1]\n",
    "                terms1 = [terms1[i] for i in range(len(terms1)) if keep1[i]]\n",
    "            \n",
    "            if len(scores2) > 0:\n",
    "                keep2 = scores2 >= conf_thresh\n",
    "                boxes2 = boxes2[keep2]\n",
    "                scores2 = scores2[keep2]\n",
    "                labels2 = labels2[keep2]\n",
    "                terms2 = [terms2[i] for i in range(len(terms2)) if keep2[i]]\n",
    "            \n",
    "            # Change detection\n",
    "            transform = T.Compose([\n",
    "                T.Resize((224, 224)),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "            \n",
    "            img1_pil = Image.open(img1_path).convert('RGB')\n",
    "            img2_pil = Image.open(img2_path).convert('RGB')\n",
    "            \n",
    "            img1_tensor = transform(img1_pil).unsqueeze(0).to(device)\n",
    "            img2_tensor = transform(img2_pil).unsqueeze(0).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                change_score = torch.sigmoid(changeformer_model(img1_tensor, img2_tensor)).item()\n",
    "            \n",
    "            # If no significant change, return empty\n",
    "            if change_score < thresholds['change_score']:\n",
    "                return {'added': set(), 'removed': set(), 'changed': set()}\n",
    "            \n",
    "            # Match objects\n",
    "            if len(boxes1) > 0 and len(boxes2) > 0:\n",
    "                matched_pairs, unmatched1, unmatched2 = object_matcher.match_objects(\n",
    "                    boxes1, labels1, boxes2, labels2, iou_threshold=thresholds['iou_match']\n",
    "                )\n",
    "            else:\n",
    "                matched_pairs = []\n",
    "                unmatched1 = list(range(len(boxes1)))\n",
    "                unmatched2 = list(range(len(boxes2)))\n",
    "            \n",
    "            # Classify changes\n",
    "            added = set([terms2[j] for j in unmatched2])\n",
    "            removed = set([terms1[i] for i in unmatched1])\n",
    "            changed = set()\n",
    "            \n",
    "            for i, j, iou in matched_pairs:\n",
    "                # If matched but IoU is low, consider as changed\n",
    "                if iou < thresholds['iou_change']:\n",
    "                    changed.add(terms1[i])\n",
    "            \n",
    "            return {'added': added, 'removed': removed, 'changed': changed}\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in prediction: {e}\")\n",
    "            return {'added': set(), 'removed': set(), 'changed': set()}\n",
    "    \n",
    "    def calibrate(self, train_df, max_samples_per_fold=20):\n",
    "        \"\"\"\n",
    "        Perform cross-validation calibration\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"🎯 CROSS-VALIDATION THRESHOLD CALIBRATION\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Create stratified labels\n",
    "        strat_labels = self.create_stratified_labels(train_df)\n",
    "        \n",
    "        # Threshold search space\n",
    "        search_space = {\n",
    "            'detection_conf': [0.05, 0.08, 0.10, 0.12],\n",
    "            'change_score': [0.15, 0.20, 0.25, 0.30],\n",
    "            'iou_match': [0.3, 0.4, 0.5],\n",
    "            'iou_change': [0.4, 0.5, 0.6]\n",
    "        }\n",
    "        \n",
    "        # Limit samples for speed\n",
    "        if len(train_df) > max_samples_per_fold * self.n_splits:\n",
    "            print(f\"\\n⚠️ Limiting to {max_samples_per_fold} samples per fold for speed\")\n",
    "        \n",
    "        best_f1 = 0\n",
    "        best_params = self.best_thresholds.copy()\n",
    "        \n",
    "        # Grid search with cross-validation\n",
    "        from itertools import product\n",
    "        \n",
    "        param_combinations = list(product(\n",
    "            search_space['detection_conf'],\n",
    "            search_space['change_score'],\n",
    "            search_space['iou_match'],\n",
    "            search_space['iou_change']\n",
    "        ))\n",
    "        \n",
    "        print(f\"\\n🔍 Testing {len(param_combinations)} parameter combinations...\")\n",
    "        print(f\"📊 Using {self.n_splits}-fold stratified cross-validation\\n\")\n",
    "        \n",
    "        for idx, (det_conf, ch_score, iou_match, iou_change) in enumerate(tqdm(param_combinations, desc=\"Grid search\")):\n",
    "            thresholds = {\n",
    "                'detection_conf': det_conf,\n",
    "                'change_score': ch_score,\n",
    "                'iou_match': iou_match,\n",
    "                'iou_change': iou_change\n",
    "            }\n",
    "            \n",
    "            # Cross-validation\n",
    "            skf = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=42)\n",
    "            fold_f1_scores = []\n",
    "            \n",
    "            for fold_idx, (train_idx, val_idx) in enumerate(skf.split(train_df, strat_labels)):\n",
    "                val_fold = train_df.iloc[val_idx]\n",
    "                \n",
    "                # Limit samples per fold\n",
    "                if len(val_fold) > max_samples_per_fold:\n",
    "                    val_fold = val_fold.sample(max_samples_per_fold, random_state=42)\n",
    "                \n",
    "                # Evaluate on this fold\n",
    "                f1_scores = []\n",
    "                \n",
    "                for _, row in val_fold.iterrows():\n",
    "                    img_id = row['img_id']\n",
    "                    \n",
    "                    # Ground truth\n",
    "                    true_added = self.parse_labels(row['added_objs'])\n",
    "                    true_removed = self.parse_labels(row['removed_objs'])\n",
    "                    true_changed = self.parse_labels(row['changed_objs'])\n",
    "                    \n",
    "                    # Predictions\n",
    "                    pred = self.predict_with_thresholds(img_id, thresholds)\n",
    "                    \n",
    "                    # Calculate F1 per category\n",
    "                    f1_added = self.calculate_f1(true_added, pred['added'])\n",
    "                    f1_removed = self.calculate_f1(true_removed, pred['removed'])\n",
    "                    f1_changed = self.calculate_f1(true_changed, pred['changed'])\n",
    "                    \n",
    "                    # Average F1\n",
    "                    avg_f1 = (f1_added + f1_removed + f1_changed) / 3\n",
    "                    f1_scores.append(avg_f1)\n",
    "                \n",
    "                fold_f1_scores.append(np.mean(f1_scores))\n",
    "            \n",
    "            # Average across folds\n",
    "            mean_cv_f1 = np.mean(fold_f1_scores)\n",
    "            \n",
    "            # Track best\n",
    "            if mean_cv_f1 > best_f1:\n",
    "                best_f1 = mean_cv_f1\n",
    "                best_params = thresholds.copy()\n",
    "                print(f\"\\n🆕 New best! F1={best_f1:.4f} | Params: {thresholds}\")\n",
    "            \n",
    "            # Track history\n",
    "            self.calibration_history.append({\n",
    "                'thresholds': thresholds.copy(),\n",
    "                'cv_f1': mean_cv_f1,\n",
    "                'fold_scores': fold_f1_scores\n",
    "            })\n",
    "        \n",
    "        self.best_thresholds = best_params\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"✅ CALIBRATION COMPLETE\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"\\n🏆 Best cross-validation F1: {best_f1:.4f}\")\n",
    "        print(f\"\\n📋 Optimal thresholds:\")\n",
    "        for key, value in self.best_thresholds.items():\n",
    "            print(f\"   {key:20s}: {value}\")\n",
    "        \n",
    "        return self.best_thresholds\n",
    "\n",
    "# Initialize calibrator\n",
    "cv_calibrator = CrossValidationCalibrator(n_splits=3)\n",
    "print(\"✅ Cross-validation calibrator initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fbdba3",
   "metadata": {},
   "source": [
    "## 8️⃣ Run Calibration\n",
    "\n",
    "**This will take time but optimize performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22eb0987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run calibration on a subset of training data\n",
    "print(\"\\n🚀 Starting threshold calibration...\")\n",
    "print(\"⏱️ This will take several minutes...\\n\")\n",
    "\n",
    "# Use subset for calibration\n",
    "calibration_subset = train_df.sample(min(60, len(train_df)), random_state=42)\n",
    "\n",
    "optimal_thresholds = cv_calibrator.calibrate(\n",
    "    calibration_subset, \n",
    "    max_samples_per_fold=15\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Calibration complete!\")\n",
    "print(f\"\\nOptimal thresholds will be used for final predictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1a4fd5",
   "metadata": {},
   "source": [
    "## 9️⃣ Complete Pipeline with All Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540ebf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedSpotDifferencePipeline:\n",
    "    \"\"\"\n",
    "    Complete optimized pipeline integrating all components\n",
    "    \"\"\"\n",
    "    def __init__(self, ensemble_detector, changeformer_model, object_matcher, cv_calibrator, vocab_extractor):\n",
    "        self.detector = ensemble_detector\n",
    "        self.changeformer = changeformer_model\n",
    "        self.matcher = object_matcher\n",
    "        self.calibrator = cv_calibrator\n",
    "        self.vocab = vocab_extractor\n",
    "        self.thresholds = cv_calibrator.best_thresholds\n",
    "    \n",
    "    def process_image_pair(self, img_id, verbose=True):\n",
    "        \"\"\"\n",
    "        Process an image pair and detect changes\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"🔍 Processing image pair: {img_id}\")\n",
    "            print(f\"{'='*60}\")\n",
    "        \n",
    "        img1_path = os.path.join(data_dir, 'data/data', f'{img_id}_1.png')\n",
    "        img2_path = os.path.join(data_dir, 'data/data', f'{img_id}_2.png')\n",
    "        \n",
    "        # Step 1: Object Detection\n",
    "        if verbose:\n",
    "            print(\"\\n1️⃣ Object Detection...\")\n",
    "        \n",
    "        boxes1, scores1, labels1, terms1 = self.detector.detect(img1_path)\n",
    "        boxes2, scores2, labels2, terms2 = self.detector.detect(img2_path)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"   Image 1: {len(terms1)} objects detected\")\n",
    "            print(f\"   Image 2: {len(terms2)} objects detected\")\n",
    "        \n",
    "        # Step 2: Confidence Filtering\n",
    "        conf_thresh = self.thresholds['detection_conf']\n",
    "        \n",
    "        if len(scores1) > 0:\n",
    "            keep1 = scores1 >= conf_thresh\n",
    "            boxes1 = boxes1[keep1]\n",
    "            scores1 = scores1[keep1]\n",
    "            labels1 = labels1[keep1]\n",
    "            terms1 = [terms1[i] for i in range(len(terms1)) if keep1[i]]\n",
    "        \n",
    "        if len(scores2) > 0:\n",
    "            keep2 = scores2 >= conf_thresh\n",
    "            boxes2 = boxes2[keep2]\n",
    "            scores2 = scores2[keep2]\n",
    "            labels2 = labels2[keep2]\n",
    "            terms2 = [terms2[i] for i in range(len(terms2)) if keep2[i]]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"   After filtering: {len(terms1)} / {len(terms2)} objects\")\n",
    "        \n",
    "        # Step 3: Change Detection with ChangeFormer\n",
    "        if verbose:\n",
    "            print(\"\\n2️⃣ Change Detection (ChangeFormer)...\")\n",
    "        \n",
    "        transform = T.Compose([\n",
    "            T.Resize((224, 224)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        img1_pil = Image.open(img1_path).convert('RGB')\n",
    "        img2_pil = Image.open(img2_path).convert('RGB')\n",
    "        \n",
    "        img1_tensor = transform(img1_pil).unsqueeze(0).to(device)\n",
    "        img2_tensor = transform(img2_pil).unsqueeze(0).to(device)\n",
    "        \n",
    "        self.changeformer.eval()\n",
    "        with torch.no_grad():\n",
    "            change_score = torch.sigmoid(self.changeformer(img1_tensor, img2_tensor)).item()\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"   Change score: {change_score:.4f} (threshold: {self.thresholds['change_score']:.2f})\")\n",
    "        \n",
    "        # Step 4: Check if change is significant\n",
    "        if change_score < self.thresholds['change_score']:\n",
    "            if verbose:\n",
    "                print(\"   ⚠️ No significant change detected\")\n",
    "            return {\n",
    "                'added': [],\n",
    "                'removed': [],\n",
    "                'changed': [],\n",
    "                'change_score': change_score,\n",
    "                'objects_img1': len(terms1),\n",
    "                'objects_img2': len(terms2)\n",
    "            }\n",
    "        \n",
    "        # Step 5: Object Matching\n",
    "        if verbose:\n",
    "            print(\"\\n3️⃣ Object Matching...\")\n",
    "        \n",
    "        if len(boxes1) > 0 and len(boxes2) > 0:\n",
    "            matched_pairs, unmatched1, unmatched2 = self.matcher.match_objects(\n",
    "                boxes1, labels1, boxes2, labels2, \n",
    "                iou_threshold=self.thresholds['iou_match']\n",
    "            )\n",
    "        else:\n",
    "            matched_pairs = []\n",
    "            unmatched1 = list(range(len(boxes1)))\n",
    "            unmatched2 = list(range(len(boxes2)))\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"   Matched pairs: {len(matched_pairs)}\")\n",
    "            print(f\"   Unmatched in img1: {len(unmatched1)}\")\n",
    "            print(f\"   Unmatched in img2: {len(unmatched2)}\")\n",
    "        \n",
    "        # Step 6: Classify Changes\n",
    "        if verbose:\n",
    "            print(\"\\n4️⃣ Classifying Changes...\")\n",
    "        \n",
    "        # Added: objects in img2 not matched\n",
    "        added = [terms2[j] for j in unmatched2]\n",
    "        \n",
    "        # Removed: objects in img1 not matched\n",
    "        removed = [terms1[i] for i in unmatched1]\n",
    "        \n",
    "        # Changed: matched but with low IoU\n",
    "        changed = []\n",
    "        for i, j, iou in matched_pairs:\n",
    "            if iou < self.thresholds['iou_change']:\n",
    "                changed.append(terms1[i])\n",
    "        \n",
    "        # Remove duplicates\n",
    "        added = list(set(added))\n",
    "        removed = list(set(removed))\n",
    "        changed = list(set(changed))\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"   Added: {added}\")\n",
    "            print(f\"   Removed: {removed}\")\n",
    "            print(f\"   Changed: {changed}\")\n",
    "        \n",
    "        return {\n",
    "            'added': added,\n",
    "            'removed': removed,\n",
    "            'changed': changed,\n",
    "            'change_score': change_score,\n",
    "            'objects_img1': len(terms1),\n",
    "            'objects_img2': len(terms2),\n",
    "            'matched_pairs': len(matched_pairs)\n",
    "        }\n",
    "    \n",
    "    def format_for_submission(self, result):\n",
    "        \"\"\"Format result for CSV submission\"\"\"\n",
    "        added_str = 'none' if not result['added'] else ' '.join(result['added'])\n",
    "        removed_str = 'none' if not result['removed'] else ' '.join(result['removed'])\n",
    "        changed_str = 'none' if not result['changed'] else ' '.join(result['changed'])\n",
    "        \n",
    "        return {\n",
    "            'added_objs': added_str,\n",
    "            'removed_objs': removed_str,\n",
    "            'changed_objs': changed_str\n",
    "        }\n",
    "\n",
    "# Initialize complete pipeline\n",
    "optimized_pipeline = OptimizedSpotDifferencePipeline(\n",
    "    ensemble_detector,\n",
    "    changeformer_model,\n",
    "    object_matcher,\n",
    "    cv_calibrator,\n",
    "    vocab_extractor\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✅ OPTIMIZED PIPELINE READY\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170f27e5",
   "metadata": {},
   "source": [
    "## 🔟 Test on Validation Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c434ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on sample images\n",
    "print(\"\\n🧪 Testing pipeline on validation samples...\\n\")\n",
    "\n",
    "test_samples = train_df.sample(min(5, len(train_df)), random_state=42)\n",
    "\n",
    "for idx, row in test_samples.iterrows():\n",
    "    img_id = row['img_id']\n",
    "    \n",
    "    print(f\"\\n📷 Ground Truth for {img_id}:\")\n",
    "    print(f\"   Added: {row['added_objs']}\")\n",
    "    print(f\"   Removed: {row['removed_objs']}\")\n",
    "    print(f\"   Changed: {row['changed_objs']}\")\n",
    "    \n",
    "    result = optimized_pipeline.process_image_pair(img_id, verbose=True)\n",
    "    \n",
    "    print(f\"\\n✅ Pipeline completed for {img_id}\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e38e6d1",
   "metadata": {},
   "source": [
    "## 1️⃣1️⃣ Generate Final Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432d7938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final predictions for test set\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🚀 GENERATING FINAL PREDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "submission_data = []\n",
    "\n",
    "for img_id in tqdm(test_df['img_id'], desc='Processing test images'):\n",
    "    try:\n",
    "        result = optimized_pipeline.process_image_pair(img_id, verbose=False)\n",
    "        formatted = optimized_pipeline.format_for_submission(result)\n",
    "        \n",
    "        submission_data.append({\n",
    "            'img_id': img_id,\n",
    "            **formatted\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"\\n⚠️ Error processing {img_id}: {e}\")\n",
    "        # Fallback to empty predictions\n",
    "        submission_data.append({\n",
    "            'img_id': img_id,\n",
    "            'added_objs': 'none',\n",
    "            'removed_objs': 'none',\n",
    "            'changed_objs': 'none'\n",
    "        })\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission_df = pd.DataFrame(submission_data)\n",
    "\n",
    "# Save to CSV\n",
    "submission_path = 'submission_optimized_v1.csv'\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"\\n✅ Submission saved to: {submission_path}\")\n",
    "print(f\"📊 Total predictions: {len(submission_df)}\")\n",
    "\n",
    "print(\"\\n📋 Sample predictions:\")\n",
    "display(submission_df.head(15))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🎉 PIPELINE COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ac0756",
   "metadata": {},
   "source": [
    "## 1️⃣2️⃣ Performance Analysis\n",
    "\n",
    "**Analyze prediction statistics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b423861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze submission statistics\n",
    "print(\"\\n📊 SUBMISSION STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "added_count = (submission_df['added_objs'] != 'none').sum()\n",
    "removed_count = (submission_df['removed_objs'] != 'none').sum()\n",
    "changed_count = (submission_df['changed_objs'] != 'none').sum()\n",
    "no_change_count = (\n",
    "    (submission_df['added_objs'] == 'none') & \n",
    "    (submission_df['removed_objs'] == 'none') & \n",
    "    (submission_df['changed_objs'] == 'none')\n",
    ").sum()\n",
    "\n",
    "print(f\"\\nPrediction distribution:\")\n",
    "print(f\"  Images with added objects: {added_count} ({100*added_count/len(submission_df):.1f}%)\")\n",
    "print(f\"  Images with removed objects: {removed_count} ({100*removed_count/len(submission_df):.1f}%)\")\n",
    "print(f\"  Images with changed objects: {changed_count} ({100*changed_count/len(submission_df):.1f}%)\")\n",
    "print(f\"  Images with no changes: {no_change_count} ({100*no_change_count/len(submission_df):.1f}%)\")\n",
    "\n",
    "# Extract all predicted terms\n",
    "all_terms = []\n",
    "for col in ['added_objs', 'removed_objs', 'changed_objs']:\n",
    "    for val in submission_df[col]:\n",
    "        if val != 'none':\n",
    "            all_terms.extend(val.split())\n",
    "\n",
    "term_counts = Counter(all_terms)\n",
    "\n",
    "print(f\"\\n🏷️ Most frequently predicted objects:\")\n",
    "for term, count in term_counts.most_common(15):\n",
    "    print(f\"  {term:20s}: {count:3d} times\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e08dda2",
   "metadata": {},
   "source": [
    "## 🎯 Summary & Next Steps\n",
    "\n",
    "### What This Notebook Achieves:\n",
    "\n",
    "1. ✅ **Robust Vocabulary Extraction** - Training data-driven with intelligent synonym expansion\n",
    "2. ✅ **Maximum Object Detection** - Multi-model ensemble (OWL-ViT + Grounding DINO)\n",
    "3. ✅ **Proper Cross-Validation** - Stratified K-Fold with systematic threshold optimization\n",
    "4. ✅ **Advanced Matching** - Hungarian algorithm with multi-criteria scoring\n",
    "5. ✅ **State-of-the-art Architecture** - ChangeFormer with cross-attention\n",
    "6. ✅ **Production-Ready Pipeline** - Error handling, logging, and validation\n",
    "\n",
    "### Key Improvements Over Previous Versions:\n",
    "\n",
    "- **Better Detection**: Ensemble + WBF fusion → More objects detected\n",
    "- **Smarter Matching**: Multi-criteria (label + IoU + position) → Better accuracy  \n",
    "- **Proper Calibration**: Stratified CV → Optimized thresholds\n",
    "- **Robust Vocabulary**: Training-driven + expansion → Better coverage\n",
    "- **Advanced Change Detection**: Cross-attention → More precise localization\n",
    "\n",
    "### Performance Optimization Tips:\n",
    "\n",
    "1. **For Higher Accuracy**: Increase calibration samples and CV folds\n",
    "2. **For Faster Inference**: Reduce image resolution or disable TTA\n",
    "3. **For Better Coverage**: Add more synonym mappings\n",
    "4. **For Difficult Cases**: Tune per-category thresholds separately\n",
    "\n",
    "### Potential Enhancements:\n",
    "\n",
    "- [ ] Test-Time Augmentation (TTA) with multi-scale inference\n",
    "- [ ] Category-specific threshold tuning\n",
    "- [ ] Semi-supervised learning with pseudo-labels\n",
    "- [ ] Ensemble multiple ChangeFormer models\n",
    "- [ ] Active learning for hard negatives"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
