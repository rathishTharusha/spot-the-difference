{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e47caeb",
   "metadata": {},
   "source": [
    "# Upscale-Augmented Spot-the-Difference Pipeline\n",
    "\n",
    "This notebook builds a reproducible workflow to upscale and enhance image pairs before running detection. It also constrains all detection prompts to the vocabulary extracted from `train.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111efdb0",
   "metadata": {},
   "source": [
    "**Dependencies**\n",
    "\n",
    "Ensure `realesrgan`, `basicsr`, `transformers`, `accelerate`, and related vision libraries are installed before running the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f665dde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional dependency install (run if packages are missing)\n",
    "#%pip install -q realesrgan basicsr transformers accelerate einops safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c1aae50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 07:27:06,221 - INFO - Using device cuda\n",
      "2025-10-16 07:27:06,221 - INFO - Images will be read from C:\\Users\\This PC\\Downloads\\Octwave\\data\\data\n",
      "2025-10-16 07:27:06,221 - INFO - Upscaled images will be stored in C:\\Users\\This PC\\Downloads\\Octwave\\outputs\\upscaled\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from IPython.display import display\n",
    "from PIL import Image, ImageEnhance\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(\"upscale_pipeline\")\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "DATA_ROOT = Path(\"data\")\n",
    "TRAIN_CSV = Path(\"train.csv\") if Path(\"train.csv\").exists() else DATA_ROOT / \"train.csv\"\n",
    "TEST_CSV = Path(\"test.csv\") if Path(\"test.csv\").exists() else DATA_ROOT / \"test.csv\"\n",
    "\n",
    "CANDIDATE_IMAGE_DIRS = [\n",
    "    Path(\"data\") / \"data\",\n",
    "    DATA_ROOT / \"data\",\n",
    "    Path(\"data\") / \"raw\",\n",
    "    Path(\"data\")\n",
    "]\n",
    "IMAGE_DIR = None\n",
    "for candidate in CANDIDATE_IMAGE_DIRS:\n",
    "    if candidate.exists():\n",
    "        IMAGE_DIR = candidate\n",
    "        break\n",
    "if IMAGE_DIR is None:\n",
    "    IMAGE_DIR = Path(\"data\")\n",
    "    logger.warning(\"Defaulting image directory to %s\", IMAGE_DIR.resolve())\n",
    "\n",
    "UPSCALED_DIR = Path(\"outputs\") / \"upscaled\"\n",
    "UPSCALED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logger.info(\"Using device %s\", DEVICE)\n",
    "logger.info(\"Images will be read from %s\", IMAGE_DIR.resolve())\n",
    "logger.info(\"Upscaled images will be stored in %s\", UPSCALED_DIR.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e63d9056",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 07:27:06,245 - INFO - Loaded 4536 training rows\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "img_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "added_objs",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "removed_objs",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "changed_objs",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "42646734-b8dc-46e3-8b15-77ae1ffa8565",
       "rows": [
        [
         "0",
         "35655",
         "none",
         "none",
         "none"
        ],
        [
         "1",
         "30660",
         "none",
         "person vehicle",
         "none"
        ],
        [
         "2",
         "34838",
         "man person",
         "car person",
         "none"
        ],
        [
         "3",
         "34045",
         "person",
         "none",
         "car"
        ],
        [
         "4",
         "30596",
         "none",
         "bicycle person",
         "none"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_id</th>\n",
       "      <th>added_objs</th>\n",
       "      <th>removed_objs</th>\n",
       "      <th>changed_objs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35655</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30660</td>\n",
       "      <td>none</td>\n",
       "      <td>person vehicle</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34838</td>\n",
       "      <td>man person</td>\n",
       "      <td>car person</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34045</td>\n",
       "      <td>person</td>\n",
       "      <td>none</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30596</td>\n",
       "      <td>none</td>\n",
       "      <td>bicycle person</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   img_id  added_objs    removed_objs changed_objs\n",
       "0   35655        none            none         none\n",
       "1   30660        none  person vehicle         none\n",
       "2   34838  man person      car person         none\n",
       "3   34045      person            none          car\n",
       "4   30596        none  bicycle person         none"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 07:27:06,257 - INFO - Loaded 1482 test rows\n"
     ]
    }
   ],
   "source": [
    "if not TRAIN_CSV.exists():\n",
    "    raise FileNotFoundError(f\"Could not find train.csv at {TRAIN_CSV}\")\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "logger.info(\"Loaded %d training rows\", len(train_df))\n",
    "display(train_df.head())\n",
    "\n",
    "if TEST_CSV.exists():\n",
    "    test_df = pd.read_csv(TEST_CSV)\n",
    "    logger.info(\"Loaded %d test rows\", len(test_df))\n",
    "else:\n",
    "    test_df = None\n",
    "    logger.info(\"Test CSV not found at %s\", TEST_CSV.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee547ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 07:27:06,370 - INFO - Vocabulary size (min freq 1): 49\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "token",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "frequency",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "68e40018-ad81-447d-b464-116cfc598a8b",
       "rows": [
        [
         "1",
         "person",
         "3216"
        ],
        [
         "4",
         "car",
         "2146"
        ],
        [
         "2",
         "vehicle",
         "1104"
        ],
        [
         "0",
         "man",
         "301"
        ],
        [
         "9",
         "object",
         "103"
        ],
        [
         "5",
         "guy",
         "51"
        ],
        [
         "27",
         "traffic",
         "32"
        ],
        [
         "12",
         "umbrella",
         "29"
        ],
        [
         "11",
         "cart",
         "27"
        ],
        [
         "3",
         "group",
         "20"
        ],
        [
         "14",
         "individual",
         "20"
        ],
        [
         "15",
         "pedestrian",
         "17"
        ],
        [
         "7",
         "boy",
         "17"
        ],
        [
         "23",
         "woman",
         "16"
        ],
        [
         "17",
         "box",
         "10"
        ],
        [
         "10",
         "item",
         "8"
        ],
        [
         "19",
         "bag",
         "7"
        ],
        [
         "16",
         "worker",
         "6"
        ],
        [
         "21",
         "dolly",
         "5"
        ],
        [
         "32",
         "child",
         "5"
        ],
        [
         "22",
         "motorcycle",
         "5"
        ],
        [
         "25",
         "bicycle",
         "5"
        ],
        [
         "18",
         "gate",
         "5"
        ],
        [
         "20",
         "kid",
         "4"
        ],
        [
         "29",
         "pole",
         "4"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 25
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>person</td>\n",
       "      <td>3216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>car</td>\n",
       "      <td>2146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vehicle</td>\n",
       "      <td>1104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>man</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>object</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>guy</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>traffic</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>umbrella</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cart</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>group</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>individual</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>pedestrian</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>boy</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>woman</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>box</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>item</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>bag</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>worker</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>dolly</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>child</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>motorcycle</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>bicycle</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>gate</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>kid</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>pole</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         token  frequency\n",
       "1       person       3216\n",
       "4          car       2146\n",
       "2      vehicle       1104\n",
       "0          man        301\n",
       "9       object        103\n",
       "5          guy         51\n",
       "27     traffic         32\n",
       "12    umbrella         29\n",
       "11        cart         27\n",
       "3        group         20\n",
       "14  individual         20\n",
       "15  pedestrian         17\n",
       "7          boy         17\n",
       "23       woman         16\n",
       "17         box         10\n",
       "10        item          8\n",
       "19         bag          7\n",
       "16      worker          6\n",
       "21       dolly          5\n",
       "32       child          5\n",
       "22  motorcycle          5\n",
       "25     bicycle          5\n",
       "18        gate          5\n",
       "20         kid          4\n",
       "29        pole          4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 07:27:06,377 - INFO - Detection vocabulary truncated to 49 terms\n"
     ]
    }
   ],
   "source": [
    "TOKEN_SPLIT_REGEX = re.compile(r\"[;,/]|\\band\\b|\\bor\\b\")\n",
    "STOP_WORDS = {\"\", \"none\", \"null\", \"nan\"}\n",
    "DROP_TERMS = {\"object\", \"item\", \"thing\"}\n",
    "\n",
    "def clean_token(token: str) -> str:\n",
    "    token = re.sub(r\"[^a-z0-9\\- ]\", \" \", token.lower())\n",
    "    token = re.sub(r\"\\s+\", \" \", token).strip()\n",
    "    return token\n",
    "\n",
    "def tokenize_label_string(label_str: str) -> list:\n",
    "    parts = TOKEN_SPLIT_REGEX.split(label_str.lower())\n",
    "    tokens = []\n",
    "    for part in parts:\n",
    "        cleaned = clean_token(part)\n",
    "        if cleaned in STOP_WORDS:\n",
    "            continue\n",
    "        for sub_token in cleaned.split():\n",
    "            if sub_token in STOP_WORDS:\n",
    "                continue\n",
    "            tokens.append(sub_token)\n",
    "    return tokens\n",
    "\n",
    "def extract_training_vocabulary(df: pd.DataFrame, min_freq: int = 1):\n",
    "    counter = Counter()\n",
    "    for column in [\"added_objs\", \"removed_objs\", \"changed_objs\"]:\n",
    "        if column not in df.columns:\n",
    "            continue\n",
    "        for label_str in df[column].dropna():\n",
    "            if not isinstance(label_str, str):\n",
    "                continue\n",
    "            tokens = tokenize_label_string(label_str)\n",
    "            counter.update(tokens)\n",
    "    vocab = [\n",
    "        token for token, freq in counter.most_common()\n",
    "        if freq >= min_freq and token not in DROP_TERMS\n",
    "    ]\n",
    "    return vocab, counter\n",
    "\n",
    "MIN_FREQUENCY = 1\n",
    "train_vocab, vocab_counts = extract_training_vocabulary(train_df, min_freq=MIN_FREQUENCY)\n",
    "logger.info(\"Vocabulary size (min freq %d): %d\", MIN_FREQUENCY, len(train_vocab))\n",
    "vocab_preview = pd.DataFrame(\n",
    "    [(token, freq) for token, freq in vocab_counts.items()],\n",
    "    columns=[\"token\", \"frequency\"]\n",
    ").sort_values(\"frequency\", ascending=False).head(25)\n",
    "display(vocab_preview)\n",
    "\n",
    "MAX_VOCAB_TERMS = 128\n",
    "detection_vocab = train_vocab[:MAX_VOCAB_TERMS]\n",
    "logger.info(\"Detection vocabulary truncated to %d terms\", len(detection_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c698610",
   "metadata": {},
   "source": [
    "**Pipeline Overview**\n",
    "\n",
    "1. Build or load a Real-ESRGAN upscaler (scale x4 by default).\n",
    "2. Upscale both images in each pair and apply lightweight contrast/sharpness enhancements.\n",
    "3. Persist upscaled images in `outputs/upscaled` with `_up` suffix to keep originals untouched.\n",
    "4. Run detection against the upscaled imagery with the training-derived vocabulary only.\n",
    "5. Compare detections between original and enhanced imagery to audit recall gains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49389e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 07:27:06,404 - WARNING - Real-ESRGAN dependencies missing. Install realesrgan and basicsr to enable upscaling.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from realesrgan import RealESRGANer\n",
    "    from basicsr.archs.rrdbnet_arch import RRDBNet\n",
    "except ImportError:\n",
    "    RealESRGANer = None\n",
    "    RRDBNet = None\n",
    "    logger.warning(\"Real-ESRGAN dependencies missing. Install realesrgan and basicsr to enable upscaling.\")\n",
    "\n",
    "MODEL_DIR = Path(\"models\") / \"realesrgan\"\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_URLS = {\n",
    "    4: \"https://github.com/xinntao/Real-ESRGAN/releases/download/v0.3.0/RealESRGAN_x4plus.pth\"\n",
    "}\n",
    "\n",
    "def download_file(url: str, destination: Path, chunk_size: int = 8192) -> None:\n",
    "    response = requests.get(url, stream=True, timeout=60)\n",
    "    response.raise_for_status()\n",
    "    total = int(response.headers.get(\"content-length\", 0))\n",
    "    downloaded = 0\n",
    "    with open(destination, \"wb\") as file:\n",
    "        for chunk in response.iter_content(chunk_size=chunk_size):\n",
    "            if not chunk:\n",
    "                continue\n",
    "            file.write(chunk)\n",
    "            downloaded += len(chunk)\n",
    "            if total:\n",
    "                percent = (downloaded / total) * 100\n",
    "                if percent % 5 < (chunk_size / max(total, 1)) * 100:\n",
    "                    logger.info(\"Download progress: %.1f%%\", percent)\n",
    "    logger.info(\"Saved weights to %s\", destination)\n",
    "\n",
    "def build_realesrgan(scale: int = 4, fp32: bool = False, tile: int = 0):\n",
    "    if RealESRGANer is None or RRDBNet is None:\n",
    "        raise ImportError(\"Real-ESRGAN libraries are not installed. Run the install cell above.\")\n",
    "    if scale not in MODEL_URLS:\n",
    "        raise ValueError(f\"No weight url configured for scale {scale}.\")\n",
    "    weights_filename = MODEL_URLS[scale].split(\"/\")[-1]\n",
    "    weights_path = MODEL_DIR / weights_filename\n",
    "    if not weights_path.exists():\n",
    "        logger.info(\"Downloading Real-ESRGAN weights to %s\", weights_path)\n",
    "        download_file(MODEL_URLS[scale], weights_path)\n",
    "    model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=scale)\n",
    "    upscaler = RealESRGANer(\n",
    "        scale=scale,\n",
    "        model_path=str(weights_path),\n",
    "        model=model,\n",
    "        tile=tile,\n",
    "        tile_pad=10,\n",
    "        pre_pad=0,\n",
    "        half=(not fp32 and DEVICE == \"cuda\"),\n",
    "        device=DEVICE\n",
    "    )\n",
    "    logger.info(\"Real-ESRGAN upscaler ready (scale x%d)\", scale)\n",
    "    return upscaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c062cdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_image(image: Image.Image, contrast: float = 1.15, sharpness: float = 1.05, brightness: float = 1.0, color: float = 1.0) -> Image.Image:\n",
    "    enhanced = ImageEnhance.Contrast(image).enhance(contrast)\n",
    "    enhanced = ImageEnhance.Sharpness(enhanced).enhance(sharpness)\n",
    "    enhanced = ImageEnhance.Brightness(enhanced).enhance(brightness)\n",
    "    enhanced = ImageEnhance.Color(enhanced).enhance(color)\n",
    "    return enhanced\n",
    "\n",
    "def upscale_and_enhance(image_path: Path, upscaler, output_path: Path, enhancement_kwargs: dict | None = None, outscale: int = 4) -> Path:\n",
    "    if upscaler is None:\n",
    "        raise ValueError(\"Upscaler is not initialized.\")\n",
    "    enhancement_kwargs = enhancement_kwargs or {}\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    # Convert RGB to BGR for Real-ESRGAN\n",
    "    lr = np.array(image)[:, :, ::-1]\n",
    "    sr, _ = upscaler.enhance(lr, outscale=outscale)\n",
    "    # Switch back to RGB before saving\n",
    "    sr_rgb = sr[:, :, ::-1]\n",
    "    sr_image = Image.fromarray(sr_rgb)\n",
    "    sr_image = enhance_image(\n",
    "        sr_image,\n",
    "        contrast=enhancement_kwargs.get(\"contrast\", 1.15),\n",
    "        sharpness=enhancement_kwargs.get(\"sharpness\", 1.05),\n",
    "        brightness=enhancement_kwargs.get(\"brightness\", 1.0),\n",
    "        color=enhancement_kwargs.get(\"color\", 1.0)\n",
    "    )\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    sr_image.save(output_path)\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d9b6c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_upscaled_dataset(\n",
    "    df: pd.DataFrame,\n",
    "    img_dir: Path,\n",
    "    output_dir: Path,\n",
    "    upscaler,\n",
    "    enhancement_kwargs: dict | None = None,\n",
    "    overwrite: bool = False,\n",
    "    limit: int | None = None\n",
    ") -> pd.DataFrame:\n",
    "    if upscaler is None:\n",
    "        raise ValueError(\"Upscaler must be initialized before processing.\")\n",
    "    rows = df if limit is None else df.head(limit)\n",
    "    records = []\n",
    "    for index, row in rows.iterrows():\n",
    "        img_id = row[\"img_id\"]\n",
    "        src1 = img_dir / f\"{img_id}_1.png\"\n",
    "        src2 = img_dir / f\"{img_id}_2.png\"\n",
    "        if not src1.exists() or not src2.exists():\n",
    "            logger.warning(\"Skipping %s due to missing original files.\", img_id)\n",
    "            continue\n",
    "        dest1 = output_dir / f\"{img_id}_1_up.png\"\n",
    "        dest2 = output_dir / f\"{img_id}_2_up.png\"\n",
    "        start = time.time()\n",
    "        try:\n",
    "            if overwrite or not dest1.exists():\n",
    "                upscale_and_enhance(src1, upscaler, dest1, enhancement_kwargs=enhancement_kwargs)\n",
    "            if overwrite or not dest2.exists():\n",
    "                upscale_and_enhance(src2, upscaler, dest2, enhancement_kwargs=enhancement_kwargs)\n",
    "            elapsed = time.time() - start\n",
    "            records.append({\"img_id\": img_id, \"seconds\": elapsed})\n",
    "            if len(records) % 20 == 0:\n",
    "                logger.info(\"Processed %d image pairs\", len(records))\n",
    "        except Exception as err:\n",
    "            logger.exception(\"Failed to process %s: %s\", img_id, err)\n",
    "    return pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f48d7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real-ESRGAN libraries are not installed. Run the install cell above.\n",
      "Install Real-ESRGAN dependencies and rerun to generate upscaled samples.\n"
     ]
    }
   ],
   "source": [
    "ENHANCEMENT_DEFAULTS = {\"contrast\": 1.2, \"sharpness\": 1.1, \"brightness\": 1.05, \"color\": 1.0}\n",
    "\n",
    "try:\n",
    "    upscaler = build_realesrgan(scale=4)\n",
    "except ImportError as exc:\n",
    "    print(exc)\n",
    "    upscaler = None\n",
    "\n",
    "if upscaler is not None:\n",
    "    sample_pairs = train_df.sample(min(3, len(train_df)), random_state=42)\n",
    "    processing_log = prepare_upscaled_dataset(\n",
    "        sample_pairs,\n",
    "        IMAGE_DIR,\n",
    "        UPSCALED_DIR,\n",
    "        upscaler,\n",
    "        enhancement_kwargs=ENHANCEMENT_DEFAULTS,\n",
    "        overwrite=False\n",
    "    )\n",
    "    display(processing_log)\n",
    "else:\n",
    "    print(\"Install Real-ESRGAN dependencies and rerun to generate upscaled samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d37c87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_before_after(img_id: str, frame_index: int = 1) -> None:\n",
    "    original_path = IMAGE_DIR / f\"{img_id}_{frame_index}.png\"\n",
    "    upscaled_path = UPSCALED_DIR / f\"{img_id}_{frame_index}_up.png\"\n",
    "    if not original_path.exists() or not upscaled_path.exists():\n",
    "        print(f\"Missing files for {img_id}_{frame_index}, skipping.\")\n",
    "        return\n",
    "    original = Image.open(original_path).convert(\"RGB\")\n",
    "    upscaled = Image.open(upscaled_path).convert(\"RGB\")\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    axes[0].imshow(original)\n",
    "    axes[0].set_title(\"Original\")\n",
    "    axes[0].axis(\"off\")\n",
    "    axes[1].imshow(upscaled)\n",
    "    axes[1].set_title(\"Upscaled + Enhanced\")\n",
    "    axes[1].axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if \"processing_log\" in globals() and not processing_log.empty:\n",
    "    show_before_after(processing_log.iloc[0][\"img_id\"], frame_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da094b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from transformers import OwlViTForObjectDetection, OwlViTProcessor\n",
    "except ImportError as exc:\n",
    "    raise ImportError(\"Install transformers to enable detection.\") from exc\n",
    "\n",
    "processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "owlvit_model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\").to(DEVICE)\n",
    "\n",
    "def detect_with_vocab(image_path: Path, vocab_terms: list[str], threshold: float = 0.12):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(text=vocab_terms, images=image, return_tensors=\"pt\")\n",
    "    inputs = {key: value.to(DEVICE) for key, value in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = owlvit_model(**inputs)\n",
    "    target_sizes = torch.tensor([image.size[::-1]])\n",
    "    results = processor.post_process_object_detection(\n",
    "        outputs=outputs,\n",
    "        target_sizes=target_sizes,\n",
    "        threshold=threshold\n",
    "    )[0]\n",
    "    boxes = results[\"boxes\"].detach().cpu().numpy()\n",
    "    scores = results[\"scores\"].detach().cpu().numpy()\n",
    "    label_indices = results[\"labels\"].detach().cpu().numpy()\n",
    "    labels = [vocab_terms[idx] for idx in label_indices]\n",
    "    return boxes, scores, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3f6a5b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate upscaled samples before running the detection comparison.\n"
     ]
    }
   ],
   "source": [
    "def run_detection_comparison(img_id: str, vocab_terms: list[str], threshold: float = 0.12) -> None:\n",
    "    original_paths = [IMAGE_DIR / f\"{img_id}_1.png\", IMAGE_DIR / f\"{img_id}_2.png\"]\n",
    "    upscaled_paths = [UPSCALED_DIR / f\"{img_id}_1_up.png\", UPSCALED_DIR / f\"{img_id}_2_up.png\"]\n",
    "    for frame_index, (orig_path, up_path) in enumerate(zip(original_paths, upscaled_paths), start=1):\n",
    "        if not orig_path.exists() or not up_path.exists():\n",
    "            logger.warning(\"Skipping %s_%d due to missing files.\", img_id, frame_index)\n",
    "            continue\n",
    "        print(f\"Image {img_id}_{frame_index} original detections:\")\n",
    "        boxes_o, scores_o, labels_o = detect_with_vocab(orig_path, vocab_terms, threshold=threshold)\n",
    "        print(list(zip(labels_o, np.round(scores_o, 3))))\n",
    "        print(f\"Image {img_id}_{frame_index} upscaled detections:\")\n",
    "        boxes_u, scores_u, labels_u = detect_with_vocab(up_path, vocab_terms, threshold=threshold)\n",
    "        print(list(zip(labels_u, np.round(scores_u, 3))))\n",
    "        print(\"---\")\n",
    "\n",
    "if upscaler is not None and \"processing_log\" in globals() and not processing_log.empty:\n",
    "    run_detection_comparison(processing_log.iloc[0][\"img_id\"], detection_vocab, threshold=0.12)\n",
    "else:\n",
    "    print(\"Generate upscaled samples before running the detection comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822d2b14",
   "metadata": {},
   "source": [
    "**Next Steps**\n",
    "\n",
    "- Re-run `prepare_upscaled_dataset` on the full training and test sets once the sample results look good.\n",
    "- Feed `detection_vocab` into downstream matching logic so every module shares the same vocabulary.\n",
    "- Track precision/recall deltas between original and upscaled runs to quantify the benefit of enhancement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
