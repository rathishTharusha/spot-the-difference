{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3db8b53",
   "metadata": {},
   "source": [
    "# Spot the Difference ML Workflow Version 2\n",
    "\n",
    "This enhanced notebook implements an improved pipeline using **YOLOv9** for object detection and **EfficientNet-B7** as the Siamese backbone for change localization. This version provides better accuracy, faster inference, and more robust object matching compared to the original implementation.\n",
    "\n",
    "## Key Improvements:\n",
    "- **YOLOv9**: State-of-the-art object detection with better accuracy and speed\n",
    "- **EfficientNet-B7**: More efficient and accurate backbone for change detection\n",
    "- **Enhanced Data Augmentation**: Improved training robustness\n",
    "- **Advanced Matching Algorithm**: Better object correspondence between image pairs\n",
    "- **Optimized Training Pipeline**: Early stopping, learning rate scheduling, and validation monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf94a4f",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Library Imports\n",
    "Import required libraries including ultralytics for YOLOv9, timm for EfficientNet, and other necessary dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a6f410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced imports for Version 2\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import cv2\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# YOLOv9 and EfficientNet specific imports\n",
    "try:\n",
    "    from ultralytics import YOLO\n",
    "    print(\"‚úì Ultralytics YOLOv9 available\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Installing ultralytics for YOLOv9...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call(['pip', 'install', 'ultralytics'])\n",
    "    from ultralytics import YOLO\n",
    "\n",
    "import timm\n",
    "from collections import Counter, defaultdict\n",
    "import random\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "\n",
    "print(\"Environment setup complete! üöÄ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0f93f9",
   "metadata": {},
   "source": [
    "## 2. Device Configuration and Model Downloads\n",
    "Check CUDA availability, configure device settings, and download pre-trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9a9456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üîß Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úì CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"‚úì PyTorch Version: {torch.__version__}\")\n",
    "    print(f\"‚úì GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"‚úì GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    \n",
    "    # Optimize CUDA settings\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è CUDA not available, using CPU\")\n",
    "\n",
    "# Download and initialize YOLOv9 model\n",
    "print(\"\\nüì• Loading YOLOv9 model...\")\n",
    "try:\n",
    "    # Load YOLOv9 model (will download if not present)\n",
    "    yolo_model = YOLO('yolov9c.pt')  # YOLOv9c for better accuracy\n",
    "    yolo_model.to(device)\n",
    "    print(\"‚úì YOLOv9 model loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading YOLOv9: {e}\")\n",
    "\n",
    "# Check available EfficientNet models\n",
    "print(\"\\nüìã Available EfficientNet models:\")\n",
    "efficientnet_models = [model for model in timm.list_models() if 'efficientnet' in model and 'b7' in model]\n",
    "print(f\"EfficientNet-B7 variants: {efficientnet_models[:5]}\")\n",
    "\n",
    "print(\"\\nüéØ Model initialization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6c62b",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Preprocessing\n",
    "Load train and test datasets with enhanced data validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafe5c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced data loading with validation\n",
    "data_dir = 'data'\n",
    "print(f\"üìÇ Loading data from: {data_dir}\")\n",
    "\n",
    "# Load datasets with error handling\n",
    "try:\n",
    "    train_df = pd.read_csv(os.path.join(data_dir, 'train.csv'))\n",
    "    test_df = pd.read_csv(os.path.join(data_dir, 'test.csv'))\n",
    "    \n",
    "    print(f\"‚úì Train dataset: {len(train_df)} samples\")\n",
    "    print(f\"‚úì Test dataset: {len(test_df)} samples\")\n",
    "    \n",
    "    # Validate data structure\n",
    "    required_columns = ['img_id', 'added_objs', 'removed_objs', 'changed_objs']\n",
    "    missing_cols = [col for col in required_columns if col not in train_df.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"‚ö†Ô∏è Missing columns: {missing_cols}\")\n",
    "    else:\n",
    "        print(\"‚úì All required columns present\")\n",
    "        \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå Error loading data: {e}\")\n",
    "    raise\n",
    "\n",
    "# Display enhanced data summary\n",
    "print(\"\\nüìä Dataset Overview:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "print(f\"Total images: {(len(train_df) + len(test_df)) * 2}\")\n",
    "\n",
    "# Display sample data with styling\n",
    "print(\"\\nüîç Sample Training Data:\")\n",
    "display(train_df.head(3).style.set_properties(**{'background-color': '#f0f0f0'}))\n",
    "\n",
    "print(\"\\nüîç Sample Test Data:\")\n",
    "display(test_df.head(3).style.set_properties(**{'background-color': '#f0f8ff'}))\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nüîç Data Quality Check:\")\n",
    "missing_data = train_df.isnull().sum()\n",
    "print(\"Missing values per column:\")\n",
    "for col, count in missing_data.items():\n",
    "    if count > 0:\n",
    "        print(f\"  {col}: {count} ({count/len(train_df)*100:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"  ‚úì {col}: 0 missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e875670",
   "metadata": {},
   "source": [
    "## 4. Enhanced Label Normalization and Vocabulary Building\n",
    "Implement improved label normalization with extended synonym mapping for better YOLOv9 compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a975d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced label normalization optimized for YOLOv9 COCO classes\n",
    "import re\n",
    "\n",
    "# Extended synonym mapping aligned with COCO dataset classes\n",
    "enhanced_synonym_map = {\n",
    "    # People\n",
    "    'man': 'person', 'guy': 'person', 'worker': 'person', 'boy': 'person', \n",
    "    'woman': 'person', 'gentleman': 'person', 'pedestrian': 'person', 'people': 'person',\n",
    "    'individual': 'person', 'human': 'person', 'child': 'person', 'adult': 'person',\n",
    "    \n",
    "    # Vehicles\n",
    "    'auto': 'car', 'automobile': 'car', 'vehicle': 'car', 'sedan': 'car',\n",
    "    'pickup': 'truck', 'van': 'truck', 'lorry': 'truck',\n",
    "    'motorcycle': 'motorcycle', 'bike': 'bicycle', 'motorbike': 'motorcycle',\n",
    "    'cycle': 'bicycle', 'scooter': 'motorcycle',\n",
    "    \n",
    "    # Common objects (COCO classes)\n",
    "    'umbrella': 'umbrella', 'bag': 'handbag', 'purse': 'handbag', 'backpack': 'backpack',\n",
    "    'box': 'suitcase', 'case': 'suitcase', 'luggage': 'suitcase',\n",
    "    'cone': 'sports ball', 'ball': 'sports ball',\n",
    "    'sign': 'stop sign', 'signboard': 'stop sign',\n",
    "    'pole': 'stop sign', 'post': 'stop sign',\n",
    "    'ladder': 'chair', 'stool': 'chair', 'seat': 'chair',\n",
    "    'gate': 'door', 'entrance': 'door',\n",
    "    \n",
    "    # Animals\n",
    "    'dog': 'dog', 'puppy': 'dog', 'canine': 'dog',\n",
    "    'cat': 'cat', 'kitten': 'cat', 'feline': 'cat',\n",
    "    'bird': 'bird', 'pigeon': 'bird', 'dove': 'bird',\n",
    "    'horse': 'horse', 'pony': 'horse',\n",
    "    \n",
    "    # Remove generic terms\n",
    "    'object': '', 'item': '', 'thing': '', 'stuff': '',\n",
    "    'shadow': '', 'reflection': '', 'light': '',\n",
    "    'group': '', 'crowd': 'person',\n",
    "    'barrier': 'chair', 'fence': 'chair'\n",
    "}\n",
    "\n",
    "def enhanced_normalize_labels(label_str):\n",
    "    \"\"\"Enhanced label normalization with better error handling\"\"\"\n",
    "    if pd.isna(label_str) or not isinstance(label_str, str):\n",
    "        return []\n",
    "    \n",
    "    if label_str.strip().lower() in ['', 'none', 'null', 'nan']:\n",
    "        return []\n",
    "    \n",
    "    # Split by common delimiters\n",
    "    tokens = re.split(r'[,\\s]+', label_str.strip().lower())\n",
    "    \n",
    "    # Apply synonym mapping and filter\n",
    "    normalized = []\n",
    "    for token in tokens:\n",
    "        if token and token != 'none':\n",
    "            mapped = enhanced_synonym_map.get(token, token)\n",
    "            if mapped:  # Skip empty mappings\n",
    "                normalized.append(mapped)\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    seen = set()\n",
    "    result = []\n",
    "    for item in normalized:\n",
    "        if item not in seen:\n",
    "            seen.add(item)\n",
    "            result.append(item)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Apply enhanced normalization\n",
    "print(\"üîÑ Applying enhanced label normalization...\")\n",
    "for col in ['added_objs', 'removed_objs', 'changed_objs']:\n",
    "    train_df[col + '_norm'] = train_df[col].apply(enhanced_normalize_labels)\n",
    "\n",
    "# Build comprehensive vocabulary\n",
    "print(\"üìö Building enhanced vocabulary...\")\n",
    "vocab = set()\n",
    "label_counts = defaultdict(int)\n",
    "\n",
    "for col in ['added_objs_norm', 'removed_objs_norm', 'changed_objs_norm']:\n",
    "    for label_list in train_df[col]:\n",
    "        for label in label_list:\n",
    "            vocab.add(label)\n",
    "            label_counts[label] += 1\n",
    "\n",
    "vocab = sorted(vocab)\n",
    "print(f\"‚úì Enhanced vocabulary size: {len(vocab)} unique labels\")\n",
    "print(f\"‚úì Total label instances: {sum(label_counts.values())}\")\n",
    "\n",
    "# Display top labels\n",
    "print(\"\\nüè∑Ô∏è Top 15 most frequent labels:\")\n",
    "top_labels = sorted(label_counts.items(), key=lambda x: x[1], reverse=True)[:15]\n",
    "for label, count in top_labels:\n",
    "    print(f\"  {label}: {count}\")\n",
    "\n",
    "# Store vocabulary for later use\n",
    "print(f\"\\nüìã Complete vocabulary: {vocab}\")\n",
    "\n",
    "# Display normalized sample\n",
    "print(\"\\nüîç Normalization Examples:\")\n",
    "sample_rows = train_df[['img_id', 'added_objs', 'added_objs_norm', 'removed_objs', 'removed_objs_norm']].head(3)\n",
    "for _, row in sample_rows.iterrows():\n",
    "    print(f\"Image {row['img_id']}:\")\n",
    "    print(f\"  Added: '{row['added_objs']}' ‚Üí {row['added_objs_norm']}\")\n",
    "    print(f\"  Removed: '{row['removed_objs']}' ‚Üí {row['removed_objs_norm']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210d2a92",
   "metadata": {},
   "source": [
    "## 5. Enhanced Exploratory Data Analysis\n",
    "Comprehensive visualization of label distributions and image pair analysis with improved plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fa87ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced visualization with better styling\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Collect all normalized labels\n",
    "added_labels = [label for sublist in train_df['added_objs_norm'] for label in sublist]\n",
    "removed_labels = [label for sublist in train_df['removed_objs_norm'] for label in sublist]\n",
    "changed_labels = [label for sublist in train_df['changed_objs_norm'] for label in sublist]\n",
    "\n",
    "added_counts = Counter(added_labels)\n",
    "removed_counts = Counter(removed_labels)\n",
    "changed_counts = Counter(changed_labels)\n",
    "\n",
    "# Create enhanced visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 12))\n",
    "fig.suptitle('Enhanced Label Distribution Analysis (Version 2)', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Added objects\n",
    "top_added = dict(added_counts.most_common(10))\n",
    "ax1 = axes[0, 0]\n",
    "bars1 = ax1.bar(range(len(top_added)), list(top_added.values()), \n",
    "                color='lightgreen', alpha=0.8, edgecolor='darkgreen')\n",
    "ax1.set_title('Top Added Objects', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Object Types')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_xticks(range(len(top_added)))\n",
    "ax1.set_xticklabels(list(top_added.keys()), rotation=45, ha='right')\n",
    "for i, bar in enumerate(bars1):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "             f'{int(height)}', ha='center', va='bottom')\n",
    "\n",
    "# Plot 2: Removed objects\n",
    "top_removed = dict(removed_counts.most_common(10))\n",
    "ax2 = axes[0, 1]\n",
    "bars2 = ax2.bar(range(len(top_removed)), list(top_removed.values()), \n",
    "                color='lightcoral', alpha=0.8, edgecolor='darkred')\n",
    "ax2.set_title('Top Removed Objects', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Object Types')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_xticks(range(len(top_removed)))\n",
    "ax2.set_xticklabels(list(top_removed.keys()), rotation=45, ha='right')\n",
    "for i, bar in enumerate(bars2):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "             f'{int(height)}', ha='center', va='bottom')\n",
    "\n",
    "# Plot 3: Changed objects\n",
    "top_changed = dict(changed_counts.most_common(10))\n",
    "ax3 = axes[1, 0]\n",
    "bars3 = ax3.bar(range(len(top_changed)), list(top_changed.values()), \n",
    "                color='lightskyblue', alpha=0.8, edgecolor='darkblue')\n",
    "ax3.set_title('Top Changed Objects', fontsize=14, fontweight='bold')\n",
    "ax3.set_xlabel('Object Types')\n",
    "ax3.set_ylabel('Frequency')\n",
    "ax3.set_xticks(range(len(top_changed)))\n",
    "ax3.set_xticklabels(list(top_changed.keys()), rotation=45, ha='right')\n",
    "for i, bar in enumerate(bars3):\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "             f'{int(height)}', ha='center', va='bottom')\n",
    "\n",
    "# Plot 4: Overall change statistics\n",
    "change_stats = {\n",
    "    'Added': len(added_labels),\n",
    "    'Removed': len(removed_labels),\n",
    "    'Changed': len(changed_labels)\n",
    "}\n",
    "ax4 = axes[1, 1]\n",
    "colors = ['lightgreen', 'lightcoral', 'lightskyblue']\n",
    "wedges, texts, autotexts = ax4.pie(change_stats.values(), labels=change_stats.keys(), \n",
    "                                   autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "ax4.set_title('Overall Change Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"üìà Enhanced Dataset Statistics:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total change instances: {len(added_labels) + len(removed_labels) + len(changed_labels)}\")\n",
    "print(f\"Average changes per image: {(len(added_labels) + len(removed_labels) + len(changed_labels)) / len(train_df):.2f}\")\n",
    "print(f\"Images with changes: {len(train_df[(train_df['added_objs_norm'].apply(len) > 0) | (train_df['removed_objs_norm'].apply(len) > 0) | (train_df['changed_objs_norm'].apply(len) > 0)])}\")\n",
    "print(f\"Images without changes: {len(train_df[(train_df['added_objs_norm'].apply(len) == 0) & (train_df['removed_objs_norm'].apply(len) == 0) & (train_df['changed_objs_norm'].apply(len) == 0)])}\")\n",
    "\n",
    "# Most diverse images (images with most different object types)\n",
    "change_diversity = []\n",
    "for _, row in train_df.iterrows():\n",
    "    all_changes = set(row['added_objs_norm'] + row['removed_objs_norm'] + row['changed_objs_norm'])\n",
    "    change_diversity.append((row['img_id'], len(all_changes), all_changes))\n",
    "\n",
    "change_diversity.sort(key=lambda x: x[1], reverse=True)\n",
    "print(f\"\\nüéØ Most diverse change images:\")\n",
    "for i, (img_id, count, changes) in enumerate(change_diversity[:5]):\n",
    "    print(f\"  {i+1}. Image {img_id}: {count} different object types - {list(changes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1f6e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced image pair visualization\n",
    "def enhanced_show_image_pair(img_id, show_details=True):\n",
    "    \"\"\"Enhanced image pair display with change annotations\"\"\"\n",
    "    img1_path = os.path.join(data_dir, 'data', f'{img_id}_1.png')\n",
    "    img2_path = os.path.join(data_dir, 'data', f'{img_id}_2.png')\n",
    "    \n",
    "    if not (os.path.exists(img1_path) and os.path.exists(img2_path)):\n",
    "        print(f\"‚ö†Ô∏è Images not found for {img_id}\")\n",
    "        return\n",
    "    \n",
    "    img1 = Image.open(img1_path).convert('RGB')\n",
    "    img2 = Image.open(img2_path).convert('RGB')\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 7))\n",
    "    \n",
    "    axs[0].imshow(img1)\n",
    "    axs[0].set_title(f'{img_id}_1 (Before)', fontsize=12, fontweight='bold')\n",
    "    axs[0].axis('off')\n",
    "    \n",
    "    axs[1].imshow(img2)\n",
    "    axs[1].set_title(f'{img_id}_2 (After)', fontsize=12, fontweight='bold')\n",
    "    axs[1].axis('off')\n",
    "    \n",
    "    if show_details:\n",
    "        # Get change information for this image\n",
    "        row = train_df[train_df['img_id'] == img_id].iloc[0]\n",
    "        added = row['added_objs_norm']\n",
    "        removed = row['removed_objs_norm']\n",
    "        changed = row['changed_objs_norm']\n",
    "        \n",
    "        # Add text annotations\n",
    "        change_text = []\n",
    "        if added:\n",
    "            change_text.append(f\"‚ûï Added: {', '.join(added)}\")\n",
    "        if removed:\n",
    "            change_text.append(f\"‚ûñ Removed: {', '.join(removed)}\")\n",
    "        if changed:\n",
    "            change_text.append(f\"üîÑ Changed: {', '.join(changed)}\")\n",
    "        \n",
    "        if change_text:\n",
    "            plt.figtext(0.5, 0.02, ' | '.join(change_text), \n",
    "                       ha='center', fontsize=10, \n",
    "                       bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgray'))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Display enhanced sample pairs\n",
    "print(\"üñºÔ∏è Enhanced Image Pair Visualization:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Show diverse examples\n",
    "sample_indices = [0, 5, 10]  # Show a variety\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    if idx < len(train_df):\n",
    "        row = train_df.iloc[idx]\n",
    "        img_id = row['img_id']\n",
    "        \n",
    "        print(f\"\\nüì∏ Sample {i+1}: Image {img_id}\")\n",
    "        print(f\"   Original labels - Added: '{row['added_objs']}', Removed: '{row['removed_objs']}', Changed: '{row['changed_objs']}'\")\n",
    "        \n",
    "        enhanced_show_image_pair(img_id)\n",
    "\n",
    "print(\"\\n‚ú® Image analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fd7ef6",
   "metadata": {},
   "source": [
    "## 6. YOLOv9 Object Detection Setup\n",
    "Initialize YOLOv9 model and implement enhanced object detection functions with confidence thresholding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b8e5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced YOLOv9 object detection implementation\n",
    "class EnhancedYOLOv9Detector:\n",
    "    def __init__(self, model_path='yolov9c.pt', confidence_threshold=0.25, iou_threshold=0.45):\n",
    "        \"\"\"Initialize YOLOv9 detector with enhanced parameters\"\"\"\n",
    "        self.model = YOLO(model_path)\n",
    "        self.model.to(device)\n",
    "        self.conf_threshold = confidence_threshold\n",
    "        self.iou_threshold = iou_threshold\n",
    "        \n",
    "        # COCO class names for reference\n",
    "        self.coco_classes = self.model.names\n",
    "        print(f\"‚úì YOLOv9 initialized with {len(self.coco_classes)} COCO classes\")\n",
    "        print(f\"‚úì Confidence threshold: {confidence_threshold}\")\n",
    "        print(f\"‚úì IoU threshold: {iou_threshold}\")\n",
    "    \n",
    "    def detect_objects(self, image_path, return_details=False):\n",
    "        \"\"\"Enhanced object detection with better error handling\"\"\"\n",
    "        try:\n",
    "            # Run YOLOv9 inference\n",
    "            results = self.model(image_path, \n",
    "                               conf=self.conf_threshold,\n",
    "                               iou=self.iou_threshold,\n",
    "                               verbose=False)\n",
    "            \n",
    "            detections = []\n",
    "            if results and len(results) > 0:\n",
    "                result = results[0]  # Single image\n",
    "                \n",
    "                if result.boxes is not None and len(result.boxes) > 0:\n",
    "                    boxes = result.boxes.xyxy.cpu().numpy()  # x1, y1, x2, y2\n",
    "                    confidences = result.boxes.conf.cpu().numpy()\n",
    "                    class_ids = result.boxes.cls.cpu().numpy().astype(int)\n",
    "                    \n",
    "                    for i in range(len(boxes)):\n",
    "                        class_name = self.coco_classes[class_ids[i]]\n",
    "                        detection = {\n",
    "                            'bbox': boxes[i],\n",
    "                            'confidence': confidences[i],\n",
    "                            'class_id': class_ids[i],\n",
    "                            'class_name': class_name\n",
    "                        }\n",
    "                        detections.append(detection)\n",
    "            \n",
    "            if return_details:\n",
    "                return detections\n",
    "            else:\n",
    "                # Return format compatible with original code\n",
    "                if detections:\n",
    "                    boxes = np.array([d['bbox'] for d in detections])\n",
    "                    scores = np.array([d['confidence'] for d in detections])\n",
    "                    labels = np.array([d['class_id'] for d in detections])\n",
    "                    return boxes, scores, labels\n",
    "                else:\n",
    "                    return np.array([]), np.array([]), np.array([])\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Detection error for {image_path}: {e}\")\n",
    "            if return_details:\n",
    "                return []\n",
    "            else:\n",
    "                return np.array([]), np.array([]), np.array([])\n",
    "    \n",
    "    def get_class_name(self, class_id):\n",
    "        \"\"\"Get COCO class name from class ID\"\"\"\n",
    "        return self.coco_classes.get(class_id, f\"unknown_{class_id}\")\n",
    "    \n",
    "    def visualize_detections(self, image_path, save_path=None):\n",
    "        \"\"\"Visualize detections on image\"\"\"\n",
    "        detections = self.detect_objects(image_path, return_details=True)\n",
    "        \n",
    "        # Load image\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Draw detections\n",
    "        for detection in detections:\n",
    "            bbox = detection['bbox'].astype(int)\n",
    "            conf = detection['confidence']\n",
    "            class_name = detection['class_name']\n",
    "            \n",
    "            # Draw bounding box\n",
    "            cv2.rectangle(image, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 2)\n",
    "            \n",
    "            # Add label\n",
    "            label = f\"{class_name}: {conf:.2f}\"\n",
    "            cv2.putText(image, label, (bbox[0], bbox[1]-10), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        \n",
    "        # Display\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.imshow(image)\n",
    "        plt.title(f'YOLOv9 Detections: {len(detections)} objects')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, bbox_inches='tight', dpi=150)\n",
    "        plt.show()\n",
    "        \n",
    "        return detections\n",
    "\n",
    "# Initialize enhanced detector\n",
    "print(\"üöÄ Initializing Enhanced YOLOv9 Detector...\")\n",
    "yolo_detector = EnhancedYOLOv9Detector(\n",
    "    confidence_threshold=0.3,  # Slightly higher for better precision\n",
    "    iou_threshold=0.45\n",
    ")\n",
    "\n",
    "# Test on sample image\n",
    "if len(train_df) > 0:\n",
    "    sample_img_id = train_df['img_id'].iloc[2]  # Test different image\n",
    "    img_path = os.path.join(data_dir, 'data', f'{sample_img_id}_1.png')\n",
    "    \n",
    "    if os.path.exists(img_path):\n",
    "        print(f\"\\nüîç Testing YOLOv9 on image: {sample_img_id}_1\")\n",
    "        detections = yolo_detector.visualize_detections(img_path)\n",
    "        \n",
    "        print(f\"‚úì Detected {len(detections)} objects:\")\n",
    "        for i, det in enumerate(detections):\n",
    "            print(f\"  {i+1}. {det['class_name']} (conf: {det['confidence']:.3f})\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Test image not found: {img_path}\")\n",
    "\n",
    "print(\"‚úÖ YOLOv9 setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0c2507",
   "metadata": {},
   "source": [
    "## 7. EfficientNet Siamese Model Architecture\n",
    "Design and implement Siamese architecture using EfficientNet-B7 backbone for superior change localization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea32c8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Siamese Model with EfficientNet-B7 backbone\n",
    "class EfficientNetSiamese(nn.Module):\n",
    "    def __init__(self, backbone_name='efficientnet_b7', pretrained=True, num_classes=1, dropout=0.3):\n",
    "        \"\"\"\n",
    "        Enhanced Siamese network with EfficientNet backbone\n",
    "        \n",
    "        Args:\n",
    "            backbone_name: EfficientNet variant (b7 for best performance)\n",
    "            pretrained: Use ImageNet pretrained weights\n",
    "            num_classes: Output classes (1 for binary change detection)\n",
    "            dropout: Dropout rate for regularization\n",
    "        \"\"\"\n",
    "        super(EfficientNetSiamese, self).__init__()\n",
    "        \n",
    "        # Load EfficientNet backbone\n",
    "        self.backbone = timm.create_model(\n",
    "            backbone_name, \n",
    "            pretrained=pretrained,\n",
    "            features_only=False,\n",
    "            num_classes=0  # Remove classification head\n",
    "        )\n",
    "        \n",
    "        # Get feature dimension\n",
    "        self.feature_dim = self.backbone.num_features\n",
    "        print(f\"‚úì EfficientNet backbone loaded: {backbone_name}\")\n",
    "        print(f\"‚úì Feature dimension: {self.feature_dim}\")\n",
    "        \n",
    "        # Enhanced fusion head with attention mechanism\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(self.feature_dim * 2, self.feature_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(self.feature_dim, self.feature_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.feature_dim * 2, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize classifier weights\"\"\"\n",
    "        for m in [self.attention, self.classifier]:\n",
    "            for module in m.modules():\n",
    "                if isinstance(module, nn.Linear):\n",
    "                    nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
    "                    if module.bias is not None:\n",
    "                        nn.init.constant_(module.bias, 0)\n",
    "                elif isinstance(module, nn.BatchNorm1d):\n",
    "                    nn.init.constant_(module.weight, 1)\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward_single(self, x):\n",
    "        \"\"\"Forward pass for single image\"\"\"\n",
    "        return self.backbone(x)\n",
    "    \n",
    "    def forward(self, img1, img2):\n",
    "        \"\"\"\n",
    "        Forward pass for image pair\n",
    "        \n",
    "        Args:\n",
    "            img1: First image tensor [B, C, H, W]\n",
    "            img2: Second image tensor [B, C, H, W]\n",
    "        \n",
    "        Returns:\n",
    "            Change prediction logits [B, num_classes]\n",
    "        \"\"\"\n",
    "        # Extract features from both images\n",
    "        feat1 = self.forward_single(img1)  # [B, feature_dim]\n",
    "        feat2 = self.forward_single(img2)  # [B, feature_dim]\n",
    "        \n",
    "        # Concatenate features\n",
    "        combined_features = torch.cat([feat1, feat2], dim=1)  # [B, feature_dim * 2]\n",
    "        \n",
    "        # Apply attention mechanism\n",
    "        attention_weights = self.attention(combined_features)  # [B, feature_dim]\n",
    "        \n",
    "        # Element-wise attention on concatenated features\n",
    "        attended_features = combined_features * torch.cat([attention_weights, attention_weights], dim=1)\n",
    "        \n",
    "        # Classification\n",
    "        output = self.classifier(attended_features)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def get_feature_maps(self, img1, img2):\n",
    "        \"\"\"Extract feature maps for visualization\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            feat1 = self.forward_single(img1)\n",
    "            feat2 = self.forward_single(img2)\n",
    "            return feat1, feat2\n",
    "\n",
    "# Model configuration\n",
    "MODEL_CONFIG = {\n",
    "    'backbone': 'efficientnet_b7',\n",
    "    'input_size': 380,  # EfficientNet-B7 optimal input size\n",
    "    'dropout': 0.3,\n",
    "    'num_classes': 1\n",
    "}\n",
    "\n",
    "print(\"üèóÔ∏è Building Enhanced EfficientNet Siamese Model...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Initialize model\n",
    "model = EfficientNetSiamese(\n",
    "    backbone_name=MODEL_CONFIG['backbone'],\n",
    "    pretrained=True,\n",
    "    num_classes=MODEL_CONFIG['num_classes'],\n",
    "    dropout=MODEL_CONFIG['dropout']\n",
    ").to(device)\n",
    "\n",
    "# Model summary\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"‚úì Model architecture: {MODEL_CONFIG['backbone']} Siamese\")\n",
    "print(f\"‚úì Input size: {MODEL_CONFIG['input_size']}x{MODEL_CONFIG['input_size']}\")\n",
    "print(f\"‚úì Total parameters: {total_params:,}\")\n",
    "print(f\"‚úì Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"‚úì Model size: ~{total_params * 4 / 1024**2:.1f} MB\")\n",
    "\n",
    "# Test model with dummy input\n",
    "print(f\"\\nüß™ Testing model with dummy input...\")\n",
    "dummy_img1 = torch.randn(2, 3, MODEL_CONFIG['input_size'], MODEL_CONFIG['input_size']).to(device)\n",
    "dummy_img2 = torch.randn(2, 3, MODEL_CONFIG['input_size'], MODEL_CONFIG['input_size']).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(dummy_img1, dummy_img2)\n",
    "    print(f\"‚úì Output shape: {output.shape}\")\n",
    "    print(f\"‚úì Output range: [{output.min().item():.3f}, {output.max().item():.3f}]\")\n",
    "\n",
    "print(\"‚úÖ EfficientNet Siamese model ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e895250",
   "metadata": {},
   "source": [
    "## 8. Enhanced Training Dataset and Data Loaders\n",
    "Create advanced dataset classes with augmentation and optimized data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec447ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Dataset with Advanced Augmentation\n",
    "class EnhancedImagePairDataset(Dataset):\n",
    "    def __init__(self, df, root_dir, input_size=380, mode='train', augment_prob=0.8):\n",
    "        \"\"\"\n",
    "        Enhanced dataset with advanced augmentation\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with image pairs and labels\n",
    "            root_dir: Root directory containing images\n",
    "            input_size: Input image size for model\n",
    "            mode: 'train', 'val', or 'test'\n",
    "            augment_prob: Probability of applying augmentation\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.root_dir = root_dir\n",
    "        self.input_size = input_size\n",
    "        self.mode = mode\n",
    "        self.augment_prob = augment_prob\n",
    "        \n",
    "        # Define augmentation transforms\n",
    "        self.train_transform = A.Compose([\n",
    "            A.Resize(input_size, input_size),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=15, p=0.7),\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.6),\n",
    "            A.OneOf([\n",
    "                A.GaussNoise(var_limit=0.01),\n",
    "                A.GaussianBlur(blur_limit=3),\n",
    "                A.MotionBlur(blur_limit=3),\n",
    "            ], p=0.5),\n",
    "            A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),\n",
    "            A.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.3),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "        \n",
    "        self.val_transform = A.Compose([\n",
    "            A.Resize(input_size, input_size),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "        \n",
    "        # Create labels\n",
    "        self._create_labels()\n",
    "        \n",
    "    def _create_labels(self):\n",
    "        \"\"\"Create binary change labels\"\"\"\n",
    "        self.labels = []\n",
    "        for _, row in self.df.iterrows():\n",
    "            # Binary label: 1 if any change exists, 0 otherwise\n",
    "            has_change = (len(row['added_objs_norm']) > 0 or \n",
    "                         len(row['removed_objs_norm']) > 0 or \n",
    "                         len(row['changed_objs_norm']) > 0)\n",
    "            self.labels.append(float(has_change))\n",
    "        \n",
    "        self.labels = np.array(self.labels)\n",
    "        print(f\"‚úì Created {len(self.labels)} labels\")\n",
    "        print(f\"‚úì Positive samples (changes): {sum(self.labels)} ({sum(self.labels)/len(self.labels)*100:.1f}%)\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get image pair and label\"\"\"\n",
    "        row = self.df.iloc[idx]\n",
    "        img_id = row['img_id']\n",
    "        \n",
    "        # Load images\n",
    "        img1_path = os.path.join(self.root_dir, 'data', f'{img_id}_1.png')\n",
    "        img2_path = os.path.join(self.root_dir, 'data', f'{img_id}_2.png')\n",
    "        \n",
    "        try:\n",
    "            img1 = cv2.imread(img1_path)\n",
    "            img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "            img2 = cv2.imread(img2_path) \n",
    "            img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading images for {img_id}: {e}\")\n",
    "            # Return dummy data\n",
    "            img1 = np.zeros((self.input_size, self.input_size, 3), dtype=np.uint8)\n",
    "            img2 = np.zeros((self.input_size, self.input_size, 3), dtype=np.uint8)\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.mode == 'train' and random.random() < self.augment_prob:\n",
    "            # Apply same augmentation to both images for consistency\n",
    "            seed = random.randint(0, 2**32)\n",
    "            \n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            transformed1 = self.train_transform(image=img1)\n",
    "            \n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            transformed2 = self.train_transform(image=img2)\n",
    "            \n",
    "            img1_tensor = transformed1['image']\n",
    "            img2_tensor = transformed2['image']\n",
    "        else:\n",
    "            img1_tensor = self.val_transform(image=img1)['image']\n",
    "            img2_tensor = self.val_transform(image=img2)['image']\n",
    "        \n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        \n",
    "        return img1_tensor, img2_tensor, label, img_id\n",
    "\n",
    "# Create train/validation split\n",
    "print(\"üìä Creating enhanced train/validation split...\")\n",
    "train_indices, val_indices = train_test_split(\n",
    "    range(len(train_df)), \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=[len(row['added_objs_norm']) + len(row['removed_objs_norm']) + len(row['changed_objs_norm']) > 0 \n",
    "              for _, row in train_df.iterrows()]\n",
    ")\n",
    "\n",
    "train_subset = train_df.iloc[train_indices].reset_index(drop=True)\n",
    "val_subset = train_df.iloc[val_indices].reset_index(drop=True)\n",
    "\n",
    "print(f\"‚úì Training samples: {len(train_subset)}\")\n",
    "print(f\"‚úì Validation samples: {len(val_subset)}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = EnhancedImagePairDataset(\n",
    "    train_subset, data_dir, \n",
    "    input_size=MODEL_CONFIG['input_size'], \n",
    "    mode='train'\n",
    ")\n",
    "\n",
    "val_dataset = EnhancedImagePairDataset(\n",
    "    val_subset, data_dir, \n",
    "    input_size=MODEL_CONFIG['input_size'], \n",
    "    mode='val'\n",
    ")\n",
    "\n",
    "# Create data loaders with optimized settings\n",
    "BATCH_SIZE = 8 if device.type == 'cuda' else 4  # Adjust based on GPU memory\n",
    "NUM_WORKERS = 4 if os.cpu_count() > 4 else 2\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True if device.type == 'cuda' else False,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True if device.type == 'cuda' else False\n",
    ")\n",
    "\n",
    "print(f\"‚úì Data loaders created:\")\n",
    "print(f\"  - Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  - Workers: {NUM_WORKERS}\")\n",
    "print(f\"  - Train batches: {len(train_loader)}\")\n",
    "print(f\"  - Val batches: {len(val_loader)}\")\n",
    "\n",
    "# Test data loader\n",
    "print(f\"\\nüß™ Testing data loader...\")\n",
    "test_batch = next(iter(train_loader))\n",
    "img1, img2, labels, img_ids = test_batch\n",
    "\n",
    "print(f\"‚úì Batch shapes:\")\n",
    "print(f\"  - Image 1: {img1.shape}\")\n",
    "print(f\"  - Image 2: {img2.shape}\") \n",
    "print(f\"  - Labels: {labels.shape}\")\n",
    "print(f\"  - Sample labels: {labels[:4].tolist()}\")\n",
    "print(f\"  - Sample IDs: {img_ids[:4]}\")\n",
    "\n",
    "print(\"‚úÖ Enhanced dataset and loaders ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f51351",
   "metadata": {},
   "source": [
    "## 9. Enhanced Model Training with Advanced Optimization\n",
    "Implement training loop with early stopping, learning rate scheduling, and comprehensive monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f63696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Training Configuration\n",
    "TRAINING_CONFIG = {\n",
    "    'epochs': 50,\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 1e-5,\n",
    "    'patience': 8,\n",
    "    'factor': 0.5,\n",
    "    'min_lr': 1e-7,\n",
    "    'warmup_epochs': 3,\n",
    "    'save_best': True\n",
    "}\n",
    "\n",
    "# Enhanced training setup\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([2.0]).to(device))  # Account for class imbalance\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=TRAINING_CONFIG['learning_rate'],\n",
    "    weight_decay=TRAINING_CONFIG['weight_decay']\n",
    ")\n",
    "\n",
    "# Learning rate scheduler with warmup\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=TRAINING_CONFIG['factor'],\n",
    "    patience=5,\n",
    "    min_lr=TRAINING_CONFIG['min_lr'],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Training tracking\n",
    "training_history = {\n",
    "    'train_loss': [], 'val_loss': [], \n",
    "    'train_acc': [], 'val_acc': [],\n",
    "    'learning_rates': []\n",
    "}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "start_epoch = 0\n",
    "\n",
    "print(\"üöÄ Enhanced Training Configuration:\")\n",
    "print(\"=\" * 50)\n",
    "for key, value in TRAINING_CONFIG.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Enhanced training function\n",
    "def train_epoch(model, loader, criterion, optimizer, epoch, warmup_epochs=3):\n",
    "    \"\"\"Enhanced training epoch with warmup and detailed metrics\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    # Warmup learning rate\n",
    "    if epoch < warmup_epochs:\n",
    "        lr_scale = (epoch + 1) / warmup_epochs\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = TRAINING_CONFIG['learning_rate'] * lr_scale\n",
    "    \n",
    "    progress_bar = range(len(loader))\n",
    "    \n",
    "    for batch_idx, (img1, img2, labels, _) in enumerate(loader):\n",
    "        img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(img1, img2).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping for stability\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item()\n",
    "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "        \n",
    "        # Progress update every 10 batches\n",
    "        if batch_idx % 10 == 0:\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f'Epoch {epoch+1}/{TRAINING_CONFIG[\"epochs\"]}, '\n",
    "                  f'Batch {batch_idx}/{len(loader)}, '\n",
    "                  f'Loss: {loss.item():.4f}, '\n",
    "                  f'LR: {current_lr:.2e}', end='\\\\r')\n",
    "    \n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    epoch_acc = correct_predictions / total_samples\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    return epoch_loss, epoch_acc, current_lr\n",
    "\n",
    "def validate_epoch(model, loader, criterion):\n",
    "    \"\"\"Enhanced validation with detailed metrics\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img1, img2, labels, _ in loader:\n",
    "            img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(img1, img2).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Predictions\n",
    "            probabilities = torch.sigmoid(outputs)\n",
    "            predicted = (probabilities > 0.5).float()\n",
    "            \n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "            \n",
    "            all_predictions.extend(probabilities.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    epoch_acc = correct_predictions / total_samples\n",
    "    \n",
    "    return epoch_loss, epoch_acc, all_predictions, all_labels\n",
    "\n",
    "# Training loop with enhanced monitoring\n",
    "print(\"\\\\nüéØ Starting Enhanced Training...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for epoch in range(start_epoch, TRAINING_CONFIG['epochs']):\n",
    "    print(f\"\\\\nEpoch {epoch+1}/{TRAINING_CONFIG['epochs']}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Training\n",
    "    train_loss, train_acc, current_lr = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, epoch, \n",
    "        TRAINING_CONFIG['warmup_epochs']\n",
    "    )\n",
    "    \n",
    "    # Validation\n",
    "    val_loss, val_acc, val_preds, val_labels = validate_epoch(\n",
    "        model, val_loader, criterion\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    if epoch >= TRAINING_CONFIG['warmup_epochs']:\n",
    "        scheduler.step(val_loss)\n",
    "    \n",
    "    # Update history\n",
    "    training_history['train_loss'].append(train_loss)\n",
    "    training_history['val_loss'].append(val_loss)\n",
    "    training_history['train_acc'].append(train_acc)\n",
    "    training_history['val_acc'].append(val_acc)\n",
    "    training_history['learning_rates'].append(current_lr)\n",
    "    \n",
    "    # Print epoch results\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    print(f\"Learning Rate: {current_lr:.2e}\")\n",
    "    \n",
    "    # Early stopping and model saving\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        \n",
    "        if TRAINING_CONFIG['save_best']:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'training_history': training_history\n",
    "            }, 'best_efficientnet_siamese_v2.pth')\n",
    "            print(\"‚úÖ Best model saved!\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"‚è≥ Patience: {patience_counter}/{TRAINING_CONFIG['patience']}\")\n",
    "        \n",
    "        if patience_counter >= TRAINING_CONFIG['patience']:\n",
    "            print(\"üõë Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "print(\"\\\\n‚úÖ Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bf625d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Training Visualization\n",
    "def plot_training_history(history):\n",
    "    \"\"\"Plot enhanced training history\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Enhanced Training History - EfficientNet Siamese V2', fontsize=16)\n",
    "    \n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    # Loss plot\n",
    "    axes[0, 0].plot(epochs, history['train_loss'], 'bo-', label='Training Loss', alpha=0.8)\n",
    "    axes[0, 0].plot(epochs, history['val_loss'], 'ro-', label='Validation Loss', alpha=0.8)\n",
    "    axes[0, 0].set_title('Model Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy plot\n",
    "    axes[0, 1].plot(epochs, history['train_acc'], 'bo-', label='Training Accuracy', alpha=0.8)\n",
    "    axes[0, 1].plot(epochs, history['val_acc'], 'ro-', label='Validation Accuracy', alpha=0.8)\n",
    "    axes[0, 1].set_title('Model Accuracy')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Learning rate plot\n",
    "    axes[1, 0].plot(epochs, history['learning_rates'], 'go-', label='Learning Rate', alpha=0.8)\n",
    "    axes[1, 0].set_title('Learning Rate Schedule')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Learning Rate')\n",
    "    axes[1, 0].set_yscale('log')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Combined loss/accuracy\n",
    "    ax2 = axes[1, 1]\n",
    "    ax2_twin = ax2.twinx()\n",
    "    \n",
    "    line1 = ax2.plot(epochs, history['val_loss'], 'r-', label='Val Loss')\n",
    "    line2 = ax2_twin.plot(epochs, history['val_acc'], 'b-', label='Val Accuracy')\n",
    "    \n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Validation Loss', color='r')\n",
    "    ax2_twin.set_ylabel('Validation Accuracy', color='b')\n",
    "    ax2.set_title('Combined Metrics')\n",
    "    \n",
    "    lines = line1 + line2\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    ax2.legend(lines, labels, loc='center right')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot training history\n",
    "if len(training_history['train_loss']) > 0:\n",
    "    plot_training_history(training_history)\n",
    "    \n",
    "    # Print final statistics\n",
    "    print(\"\\\\nüìä Final Training Statistics:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "    print(f\"Final validation accuracy: {training_history['val_acc'][-1]:.4f}\")\n",
    "    print(f\"Total epochs trained: {len(training_history['train_loss'])}\")\n",
    "    print(f\"Final learning rate: {training_history['learning_rates'][-1]:.2e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No training history to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80ac858",
   "metadata": {},
   "source": [
    "## 10. Advanced Object Detection and Matching Pipeline\n",
    "Implement sophisticated object matching using YOLOv9 detections with Hungarian algorithm optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ce7539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Object Matching Pipeline\n",
    "class AdvancedObjectMatcher:\n",
    "    def __init__(self, yolo_detector, iou_threshold=0.5, confidence_threshold=0.3):\n",
    "        \"\"\"\n",
    "        Advanced object matcher with sophisticated algorithms\n",
    "        \n",
    "        Args:\n",
    "            yolo_detector: YOLOv9 detector instance\n",
    "            iou_threshold: IoU threshold for matching\n",
    "            confidence_threshold: Confidence threshold for detections\n",
    "        \"\"\"\n",
    "        self.yolo_detector = yolo_detector\n",
    "        self.iou_threshold = iou_threshold\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "    \n",
    "    def calculate_iou(self, box1, box2):\n",
    "        \"\"\"Calculate Intersection over Union (IoU) between two bounding boxes\"\"\"\n",
    "        x1_1, y1_1, x2_1, y2_1 = box1\n",
    "        x1_2, y1_2, x2_2, y2_2 = box2\n",
    "        \n",
    "        # Calculate intersection area\n",
    "        x1_i = max(x1_1, x1_2)\n",
    "        y1_i = max(y1_1, y1_2)\n",
    "        x2_i = min(x2_1, x2_2)\n",
    "        y2_i = min(y2_1, y2_2)\n",
    "        \n",
    "        if x2_i <= x1_i or y2_i <= y1_i:\n",
    "            return 0.0\n",
    "        \n",
    "        intersection_area = (x2_i - x1_i) * (y2_i - y1_i)\n",
    "        \n",
    "        # Calculate union area\n",
    "        area1 = (x2_1 - x1_1) * (y2_1 - y1_1)\n",
    "        area2 = (x2_2 - x1_2) * (y2_2 - y1_2)\n",
    "        union_area = area1 + area2 - intersection_area\n",
    "        \n",
    "        return intersection_area / union_area if union_area > 0 else 0.0\n",
    "    \n",
    "    def calculate_box_similarity(self, box1, box2, class1, class2):\n",
    "        \"\"\"Calculate comprehensive similarity between two detections\"\"\"\n",
    "        # Class matching (essential)\n",
    "        if class1 != class2:\n",
    "            return 0.0\n",
    "        \n",
    "        # IoU similarity\n",
    "        iou = self.calculate_iou(box1, box2)\n",
    "        \n",
    "        # Size similarity (normalized)\n",
    "        area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "        area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "        size_similarity = min(area1, area2) / max(area1, area2) if max(area1, area2) > 0 else 0\n",
    "        \n",
    "        # Center distance similarity\n",
    "        center1 = ((box1[0] + box1[2]) / 2, (box1[1] + box1[3]) / 2)\n",
    "        center2 = ((box2[0] + box2[2]) / 2, (box2[1] + box2[3]) / 2)\n",
    "        distance = np.sqrt((center1[0] - center2[0])**2 + (center1[1] - center2[1])**2)\n",
    "        \n",
    "        # Normalize distance by image diagonal (assuming 640x640 for now)\n",
    "        img_diagonal = np.sqrt(640**2 + 640**2)\n",
    "        distance_similarity = max(0, 1 - (distance / img_diagonal))\n",
    "        \n",
    "        # Combined similarity\n",
    "        combined_similarity = (0.6 * iou + 0.25 * size_similarity + 0.15 * distance_similarity)\n",
    "        return combined_similarity\n",
    "    \n",
    "    def match_objects_advanced(self, detections1, detections2):\n",
    "        \"\"\"\n",
    "        Advanced object matching using Hungarian algorithm with multiple similarity metrics\n",
    "        \n",
    "        Args:\n",
    "            detections1: List of detections from image 1\n",
    "            detections2: List of detections from image 2\n",
    "        \n",
    "        Returns:\n",
    "            matched_pairs, unmatched1, unmatched2\n",
    "        \"\"\"\n",
    "        if not detections1 or not detections2:\n",
    "            return [], list(range(len(detections1))), list(range(len(detections2)))\n",
    "        \n",
    "        # Create cost matrix\n",
    "        cost_matrix = np.zeros((len(detections1), len(detections2)))\n",
    "        \n",
    "        for i, det1 in enumerate(detections1):\n",
    "            for j, det2 in enumerate(detections2):\n",
    "                similarity = self.calculate_box_similarity(\n",
    "                    det1['bbox'], det2['bbox'],\n",
    "                    det1['class_name'], det2['class_name']\n",
    "                )\n",
    "                cost_matrix[i, j] = 1.0 - similarity  # Convert to cost\n",
    "        \n",
    "        # Apply Hungarian algorithm\n",
    "        row_indices, col_indices = linear_sum_assignment(cost_matrix)\n",
    "        \n",
    "        # Filter matches based on threshold\n",
    "        matched_pairs = []\n",
    "        used_rows = set()\n",
    "        used_cols = set()\n",
    "        \n",
    "        for row, col in zip(row_indices, col_indices):\n",
    "            if cost_matrix[row, col] < (1.0 - self.iou_threshold):  # Good match\n",
    "                matched_pairs.append((row, col, 1.0 - cost_matrix[row, col]))\n",
    "                used_rows.add(row)\n",
    "                used_cols.add(col)\n",
    "        \n",
    "        # Unmatched detections\n",
    "        unmatched1 = [i for i in range(len(detections1)) if i not in used_rows]\n",
    "        unmatched2 = [j for j in range(len(detections2)) if j not in used_cols]\n",
    "        \n",
    "        return matched_pairs, unmatched1, unmatched2\n",
    "    \n",
    "    def analyze_changes(self, img_path1, img_path2, verbose=False):\n",
    "        \"\"\"\n",
    "        Comprehensive change analysis between two images\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with added, removed, changed objects and confidence scores\n",
    "        \"\"\"\n",
    "        # Get detections from both images\n",
    "        detections1 = self.yolo_detector.detect_objects(img_path1, return_details=True)\n",
    "        detections2 = self.yolo_detector.detect_objects(img_path2, return_details=True)\n",
    "        \n",
    "        # Filter by confidence\n",
    "        detections1 = [d for d in detections1 if d['confidence'] >= self.confidence_threshold]\n",
    "        detections2 = [d for d in detections2 if d['confidence'] >= self.confidence_threshold]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Image 1 detections: {len(detections1)}\")\n",
    "            print(f\"Image 2 detections: {len(detections2)}\")\n",
    "        \n",
    "        # Match objects\n",
    "        matched_pairs, unmatched1, unmatched2 = self.match_objects_advanced(detections1, detections2)\n",
    "        \n",
    "        # Analyze changes\n",
    "        added_objects = []\n",
    "        removed_objects = []\n",
    "        changed_objects = []\n",
    "        \n",
    "        # Added objects (in image 2 but not matched with image 1)\n",
    "        for idx in unmatched2:\n",
    "            det = detections2[idx]\n",
    "            added_objects.append({\n",
    "                'class_name': det['class_name'],\n",
    "                'confidence': det['confidence'],\n",
    "                'bbox': det['bbox']\n",
    "            })\n",
    "        \n",
    "        # Removed objects (in image 1 but not matched with image 2)\n",
    "        for idx in unmatched1:\n",
    "            det = detections1[idx]\n",
    "            removed_objects.append({\n",
    "                'class_name': det['class_name'],\n",
    "                'confidence': det['confidence'],\n",
    "                'bbox': det['bbox']\n",
    "            })\n",
    "        \n",
    "        # Changed objects (matched but with significant differences)\n",
    "        for idx1, idx2, similarity in matched_pairs:\n",
    "            det1 = detections1[idx1]\n",
    "            det2 = detections2[idx2]\n",
    "            \n",
    "            # Check for significant position or size changes\n",
    "            iou = self.calculate_iou(det1['bbox'], det2['bbox'])\n",
    "            \n",
    "            if iou < 0.7:  # Significant change threshold\n",
    "                changed_objects.append({\n",
    "                    'class_name': det1['class_name'],\n",
    "                    'confidence_1': det1['confidence'],\n",
    "                    'confidence_2': det2['confidence'],\n",
    "                    'bbox_1': det1['bbox'],\n",
    "                    'bbox_2': det2['bbox'],\n",
    "                    'iou': iou\n",
    "                })\n",
    "        \n",
    "        # Aggregate by class names\n",
    "        added_classes = list(set([obj['class_name'] for obj in added_objects]))\n",
    "        removed_classes = list(set([obj['class_name'] for obj in removed_objects]))\n",
    "        changed_classes = list(set([obj['class_name'] for obj in changed_objects]))\n",
    "        \n",
    "        return {\n",
    "            'added': added_classes,\n",
    "            'removed': removed_classes,\n",
    "            'changed': changed_classes,\n",
    "            'added_details': added_objects,\n",
    "            'removed_details': removed_objects,\n",
    "            'changed_details': changed_objects,\n",
    "            'matched_pairs': len(matched_pairs),\n",
    "            'total_objects_1': len(detections1),\n",
    "            'total_objects_2': len(detections2)\n",
    "        }\n",
    "\n",
    "# Initialize advanced matcher\n",
    "print(\"üîß Initializing Advanced Object Matcher...\")\n",
    "advanced_matcher = AdvancedObjectMatcher(\n",
    "    yolo_detector=yolo_detector,\n",
    "    iou_threshold=0.5,\n",
    "    confidence_threshold=0.3\n",
    ")\n",
    "\n",
    "# Test advanced matching on sample images\n",
    "if len(train_df) > 0:\n",
    "    sample_img_id = train_df['img_id'].iloc[1]\n",
    "    img1_path = os.path.join(data_dir, 'data', f'{sample_img_id}_1.png')\n",
    "    img2_path = os.path.join(data_dir, 'data', f'{sample_img_id}_2.png')\n",
    "    \n",
    "    if os.path.exists(img1_path) and os.path.exists(img2_path):\n",
    "        print(f\"\\\\nüß™ Testing advanced matching on image pair: {sample_img_id}\")\n",
    "        \n",
    "        results = advanced_matcher.analyze_changes(img1_path, img2_path, verbose=True)\n",
    "        \n",
    "        print(f\"\\\\nüìã Advanced Analysis Results:\")\n",
    "        print(f\"  ‚ûï Added objects: {results['added']}\")\n",
    "        print(f\"  ‚ûñ Removed objects: {results['removed']}\")\n",
    "        print(f\"  üîÑ Changed objects: {results['changed']}\")\n",
    "        print(f\"  üîó Matched pairs: {results['matched_pairs']}\")\n",
    "        print(f\"  üìä Total detections: {results['total_objects_1']} ‚Üí {results['total_objects_2']}\")\n",
    "        \n",
    "        # Compare with ground truth\n",
    "        gt_row = train_df[train_df['img_id'] == sample_img_id].iloc[0]\n",
    "        print(f\"\\\\nüéØ Ground Truth Comparison:\")\n",
    "        print(f\"  GT Added: {gt_row['added_objs_norm']}\")\n",
    "        print(f\"  GT Removed: {gt_row['removed_objs_norm']}\")\n",
    "        print(f\"  GT Changed: {gt_row['changed_objs_norm']}\")\n",
    "\n",
    "print(\"\\\\n‚úÖ Advanced object matching pipeline ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c182e6b5",
   "metadata": {},
   "source": [
    "## 11. Model Evaluation and Fusion Strategy\n",
    "Implement comprehensive fusion of YOLOv9 detections with EfficientNet change localization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671f6cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Fusion Pipeline\n",
    "class EnhancedFusionPipeline:\n",
    "    def __init__(self, siamese_model, object_matcher, fusion_weights=None):\n",
    "        \"\"\"\n",
    "        Enhanced fusion pipeline combining change detection and object matching\n",
    "        \n",
    "        Args:\n",
    "            siamese_model: Trained EfficientNet Siamese model\n",
    "            object_matcher: Advanced object matcher\n",
    "            fusion_weights: Weights for fusion [change_weight, object_weight]\n",
    "        \"\"\"\n",
    "        self.siamese_model = siamese_model\n",
    "        self.object_matcher = object_matcher\n",
    "        self.fusion_weights = fusion_weights or [0.6, 0.4]\n",
    "        \n",
    "        # Transforms for model input\n",
    "        self.transform = A.Compose([\n",
    "            A.Resize(MODEL_CONFIG['input_size'], MODEL_CONFIG['input_size']),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    \n",
    "    def get_change_score(self, img_path1, img_path2):\n",
    "        \"\"\"Get change localization score from Siamese model\"\"\"\n",
    "        try:\n",
    "            # Load and preprocess images\n",
    "            img1 = cv2.imread(img_path1)\n",
    "            img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "            img2 = cv2.imread(img_path2)\n",
    "            img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Apply transforms\n",
    "            img1_tensor = self.transform(image=img1)['image'].unsqueeze(0).to(device)\n",
    "            img2_tensor = self.transform(image=img2)['image'].unsqueeze(0).to(device)\n",
    "            \n",
    "            # Get prediction\n",
    "            self.siamese_model.eval()\n",
    "            with torch.no_grad():\n",
    "                output = self.siamese_model(img1_tensor, img2_tensor)\n",
    "                change_score = torch.sigmoid(output).item()\n",
    "            \n",
    "            return change_score\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting change score: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def analyze_image_pair(self, img_id, verbose=False):\n",
    "        \"\"\"\n",
    "        Comprehensive analysis of image pair with fusion\n",
    "        \n",
    "        Args:\n",
    "            img_id: Image ID for the pair\n",
    "            verbose: Print detailed information\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with analysis results\n",
    "        \"\"\"\n",
    "        img_path1 = os.path.join(data_dir, 'data', f'{img_id}_1.png')\n",
    "        img_path2 = os.path.join(data_dir, 'data', f'{img_id}_2.png')\n",
    "        \n",
    "        if not (os.path.exists(img_path1) and os.path.exists(img_path2)):\n",
    "            return {'error': f'Images not found for {img_id}'}\n",
    "        \n",
    "        # Get change localization score\n",
    "        change_score = self.get_change_score(img_path1, img_path2)\n",
    "        \n",
    "        # Get object-level changes\n",
    "        object_analysis = self.object_matcher.analyze_changes(img_path1, img_path2, verbose=verbose)\n",
    "        \n",
    "        # Calculate object change confidence\n",
    "        object_change_score = 0.0\n",
    "        if object_analysis['added'] or object_analysis['removed'] or object_analysis['changed']:\n",
    "            # Weight by number and confidence of changes\n",
    "            added_weight = len(object_analysis['added']) * 0.4\n",
    "            removed_weight = len(object_analysis['removed']) * 0.4\n",
    "            changed_weight = len(object_analysis['changed']) * 0.3\n",
    "            \n",
    "            # Normalize by maximum expected changes (heuristic)\n",
    "            max_changes = 5\n",
    "            object_change_score = min(1.0, (added_weight + removed_weight + changed_weight) / max_changes)\n",
    "        \n",
    "        # Fusion strategy\n",
    "        fusion_score = (self.fusion_weights[0] * change_score + \n",
    "                       self.fusion_weights[1] * object_change_score)\n",
    "        \n",
    "        # Final predictions based on fusion score and object analysis\n",
    "        final_added = object_analysis['added'] if fusion_score > 0.3 else []\n",
    "        final_removed = object_analysis['removed'] if fusion_score > 0.3 else []\n",
    "        final_changed = object_analysis['changed'] if fusion_score > 0.3 else []\n",
    "        \n",
    "        return {\n",
    "            'img_id': img_id,\n",
    "            'change_score': change_score,\n",
    "            'object_change_score': object_change_score,\n",
    "            'fusion_score': fusion_score,\n",
    "            'added': final_added,\n",
    "            'removed': final_removed,\n",
    "            'changed': final_changed,\n",
    "            'raw_object_analysis': object_analysis,\n",
    "            'has_change': fusion_score > 0.3\n",
    "        }\n",
    "\n",
    "# Load best model for evaluation\n",
    "print(\"üì• Loading best trained model...\")\n",
    "try:\n",
    "    checkpoint = torch.load('best_efficientnet_siamese_v2.pth', map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(\"‚úÖ Best model loaded successfully!\")\n",
    "    print(f\"   Best validation loss: {checkpoint['best_val_loss']:.4f}\")\n",
    "    print(f\"   Trained epochs: {checkpoint['epoch'] + 1}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è Best model not found, using current model state\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading model: {e}\")\n",
    "\n",
    "# Initialize fusion pipeline\n",
    "print(\"\\\\nüîß Initializing Enhanced Fusion Pipeline...\")\n",
    "fusion_pipeline = EnhancedFusionPipeline(\n",
    "    siamese_model=model,\n",
    "    object_matcher=advanced_matcher,\n",
    "    fusion_weights=[0.65, 0.35]  # Slightly favor change detection\n",
    ")\n",
    "\n",
    "# Test fusion pipeline on validation samples\n",
    "print(\"\\\\nüß™ Testing Fusion Pipeline on Validation Samples...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_samples = val_subset.sample(3, random_state=42) if len(val_subset) > 3 else val_subset\n",
    "for idx, row in test_samples.iterrows():\n",
    "    img_id = row['img_id']\n",
    "    \n",
    "    print(f\"\\\\nüì∏ Analyzing Image {img_id}:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Get fusion analysis\n",
    "    result = fusion_pipeline.analyze_image_pair(img_id, verbose=False)\n",
    "    \n",
    "    if 'error' in result:\n",
    "        print(f\"‚ùå {result['error']}\")\n",
    "        continue\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"Change Score: {result['change_score']:.3f}\")\n",
    "    print(f\"Object Change Score: {result['object_change_score']:.3f}\")\n",
    "    print(f\"Fusion Score: {result['fusion_score']:.3f}\")\n",
    "    print(f\"Has Change: {result['has_change']}\")\n",
    "    print(f\"Predicted - Added: {result['added']}\")\n",
    "    print(f\"Predicted - Removed: {result['removed']}\")\n",
    "    print(f\"Predicted - Changed: {result['changed']}\")\n",
    "    \n",
    "    # Ground truth comparison\n",
    "    print(f\"\\\\nGround Truth:\")\n",
    "    print(f\"GT - Added: {row['added_objs_norm']}\")\n",
    "    print(f\"GT - Removed: {row['removed_objs_norm']}\")\n",
    "    print(f\"GT - Changed: {row['changed_objs_norm']}\")\n",
    "    \n",
    "    # Calculate basic metrics\n",
    "    pred_added = set(result['added'])\n",
    "    pred_removed = set(result['removed'])\n",
    "    pred_changed = set(result['changed'])\n",
    "    \n",
    "    gt_added = set(row['added_objs_norm'])\n",
    "    gt_removed = set(row['removed_objs_norm'])\n",
    "    gt_changed = set(row['changed_objs_norm'])\n",
    "    \n",
    "    # Simple accuracy metrics\n",
    "    added_correct = len(pred_added.intersection(gt_added))\n",
    "    removed_correct = len(pred_removed.intersection(gt_removed))\n",
    "    changed_correct = len(pred_changed.intersection(gt_changed))\n",
    "    \n",
    "    print(f\"\\\\nAccuracy:\")\n",
    "    print(f\"Added: {added_correct}/{len(gt_added)} correct\")\n",
    "    print(f\"Removed: {removed_correct}/{len(gt_removed)} correct\")\n",
    "    print(f\"Changed: {changed_correct}/{len(gt_changed)} correct\")\n",
    "\n",
    "print(\"\\\\n‚úÖ Fusion pipeline evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04168a3",
   "metadata": {},
   "source": [
    "## 12. Results Generation and Submission\n",
    "Generate predictions for test set and create properly formatted submission files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc7de30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Submission Generation\n",
    "def generate_enhanced_submission(test_df, fusion_pipeline, save_path='submission_v2_enhanced.csv'):\n",
    "    \"\"\"\n",
    "    Generate enhanced submission using fusion pipeline\n",
    "    \n",
    "    Args:\n",
    "        test_df: Test dataset DataFrame\n",
    "        fusion_pipeline: Trained fusion pipeline\n",
    "        save_path: Path to save submission file\n",
    "    \n",
    "    Returns:\n",
    "        submission DataFrame\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Generating Enhanced Submission (Version 2)...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    submission_data = []\n",
    "    processing_errors = []\n",
    "    \n",
    "    for idx, row in test_df.iterrows():\n",
    "        img_id = row['img_id']\n",
    "        \n",
    "        try:\n",
    "            # Get fusion analysis\n",
    "            result = fusion_pipeline.analyze_image_pair(img_id, verbose=False)\n",
    "            \n",
    "            if 'error' in result:\n",
    "                print(f\"‚ö†Ô∏è Error processing {img_id}: {result['error']}\")\n",
    "                processing_errors.append(img_id)\n",
    "                # Use empty predictions for failed cases\n",
    "                added_str = 'none'\n",
    "                removed_str = 'none'\n",
    "                changed_str = 'none'\n",
    "            else:\n",
    "                # Format predictions\n",
    "                added_str = ' '.join(result['added']) if result['added'] else 'none'\n",
    "                removed_str = ' '.join(result['removed']) if result['removed'] else 'none'\n",
    "                changed_str = ' '.join(result['changed']) if result['changed'] else 'none'\n",
    "            \n",
    "            submission_data.append({\n",
    "                'img_id': img_id,\n",
    "                'added_objs': added_str,\n",
    "                'removed_objs': removed_str,\n",
    "                'changed_objs': changed_str\n",
    "            })\n",
    "            \n",
    "            # Progress update\n",
    "            if (idx + 1) % 10 == 0:\n",
    "                print(f\"Processed {idx + 1}/{len(test_df)} images\", end='\\\\r')\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Unexpected error processing {img_id}: {e}\")\n",
    "            processing_errors.append(img_id)\n",
    "            submission_data.append({\n",
    "                'img_id': img_id,\n",
    "                'added_objs': 'none',\n",
    "                'removed_objs': 'none',\n",
    "                'changed_objs': 'none'\n",
    "            })\n",
    "    \n",
    "    # Create submission DataFrame\n",
    "    submission_df = pd.DataFrame(submission_data)\n",
    "    \n",
    "    # Save submission\n",
    "    submission_df.to_csv(save_path, index=False)\n",
    "    \n",
    "    print(f\"\\\\n‚úÖ Submission generated and saved to: {save_path}\")\n",
    "    print(f\"üìä Submission Statistics:\")\n",
    "    print(f\"   Total test images: {len(submission_df)}\")\n",
    "    print(f\"   Processing errors: {len(processing_errors)}\")\n",
    "    \n",
    "    if processing_errors:\n",
    "        print(f\"   Failed images: {processing_errors[:5]}{'...' if len(processing_errors) > 5 else ''}\")\n",
    "    \n",
    "    # Analyze submission content\n",
    "    added_counts = sum(1 for x in submission_df['added_objs'] if x != 'none')\n",
    "    removed_counts = sum(1 for x in submission_df['removed_objs'] if x != 'none')\n",
    "    changed_counts = sum(1 for x in submission_df['changed_objs'] if x != 'none')\n",
    "    \n",
    "    print(f\"   Images with added objects: {added_counts}\")\n",
    "    print(f\"   Images with removed objects: {removed_counts}\")\n",
    "    print(f\"   Images with changed objects: {changed_counts}\")\n",
    "    print(f\"   Images with any changes: {len(submission_df[(submission_df['added_objs'] != 'none') | (submission_df['removed_objs'] != 'none') | (submission_df['changed_objs'] != 'none')])}\")\n",
    "    \n",
    "    return submission_df\n",
    "\n",
    "# Generate submission for test set\n",
    "print(\"üìù Generating submission for test dataset...\")\n",
    "submission_v2 = generate_enhanced_submission(test_df, fusion_pipeline)\n",
    "\n",
    "# Display sample predictions\n",
    "print(\"\\\\nüîç Sample Predictions (First 5 test images):\")\n",
    "print(\"=\" * 60)\n",
    "sample_submission = submission_v2.head()\n",
    "for idx, row in sample_submission.iterrows():\n",
    "    print(f\"Image {row['img_id']}:\")\n",
    "    print(f\"  Added: {row['added_objs']}\")\n",
    "    print(f\"  Removed: {row['removed_objs']}\")\n",
    "    print(f\"  Changed: {row['changed_objs']}\")\n",
    "    print()\n",
    "\n",
    "# Save additional outputs\n",
    "print(\"üíæ Saving additional model outputs...\")\n",
    "\n",
    "# Save model weights (final state)\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_config': MODEL_CONFIG,\n",
    "    'training_config': TRAINING_CONFIG,\n",
    "    'training_history': training_history,\n",
    "    'vocab': vocab,\n",
    "    'enhanced_synonym_map': enhanced_synonym_map\n",
    "}, 'efficientnet_siamese_v2_final.pth')\n",
    "\n",
    "# Save evaluation metrics\n",
    "eval_metrics = {\n",
    "    'model_type': 'EfficientNet-B7 Siamese + YOLOv9',\n",
    "    'input_size': MODEL_CONFIG['input_size'],\n",
    "    'total_parameters': sum(p.numel() for p in model.parameters()),\n",
    "    'trainable_parameters': sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
    "    'training_epochs': len(training_history['train_loss']) if training_history['train_loss'] else 0,\n",
    "    'best_val_loss': best_val_loss,\n",
    "    'final_val_accuracy': training_history['val_acc'][-1] if training_history['val_acc'] else 0.0,\n",
    "    'vocab_size': len(vocab),\n",
    "    'test_predictions': len(submission_v2)\n",
    "}\n",
    "\n",
    "with open('eval_metrics_v2.json', 'w') as f:\n",
    "    json.dump(eval_metrics, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ All outputs saved:\")\n",
    "print(\"   üìÑ submission_v2_enhanced.csv - Test predictions\")\n",
    "print(\"   üß† efficientnet_siamese_v2_final.pth - Final model weights\")\n",
    "print(\"   üìä eval_metrics_v2.json - Evaluation metrics\")\n",
    "\n",
    "print(\"\\\\nüéâ Enhanced Version 2 pipeline complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a82a3e2",
   "metadata": {},
   "source": [
    "## 13. Comprehensive Error Analysis and Validation\n",
    "Perform detailed error analysis with metrics and visualizations for model improvement insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf8a48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Error Analysis\n",
    "def comprehensive_error_analysis(val_df, fusion_pipeline, num_samples=20):\n",
    "    \"\"\"\n",
    "    Perform detailed error analysis on validation set\n",
    "    \n",
    "    Args:\n",
    "        val_df: Validation DataFrame\n",
    "        fusion_pipeline: Trained fusion pipeline\n",
    "        num_samples: Number of samples to analyze\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with detailed analysis results\n",
    "    \"\"\"\n",
    "    print(\"üîç Performing Comprehensive Error Analysis...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    analysis_results = {\n",
    "        'total_analyzed': 0,\n",
    "        'correct_predictions': 0,\n",
    "        'false_positives': {'added': 0, 'removed': 0, 'changed': 0},\n",
    "        'false_negatives': {'added': 0, 'removed': 0, 'changed': 0},\n",
    "        'detailed_errors': [],\n",
    "        'class_performance': defaultdict(lambda: {'tp': 0, 'fp': 0, 'fn': 0}),\n",
    "        'fusion_scores': [],\n",
    "        'change_scores': [],\n",
    "        'object_scores': []\n",
    "    }\n",
    "    \n",
    "    # Analyze subset of validation data\n",
    "    sample_df = val_df.sample(min(num_samples, len(val_df)), random_state=42)\n",
    "    \n",
    "    for idx, row in sample_df.iterrows():\n",
    "        img_id = row['img_id']\n",
    "        analysis_results['total_analyzed'] += 1\n",
    "        \n",
    "        try:\n",
    "            # Get predictions\n",
    "            result = fusion_pipeline.analyze_image_pair(img_id, verbose=False)\n",
    "            \n",
    "            if 'error' in result:\n",
    "                continue\n",
    "            \n",
    "            # Ground truth\n",
    "            gt_added = set(row['added_objs_norm'])\n",
    "            gt_removed = set(row['removed_objs_norm'])\n",
    "            gt_changed = set(row['changed_objs_norm'])\n",
    "            \n",
    "            # Predictions\n",
    "            pred_added = set(result['added'])\n",
    "            pred_removed = set(result['removed'])\n",
    "            pred_changed = set(result['changed'])\n",
    "            \n",
    "            # Store scores\n",
    "            analysis_results['fusion_scores'].append(result['fusion_score'])\n",
    "            analysis_results['change_scores'].append(result['change_score'])\n",
    "            analysis_results['object_scores'].append(result['object_change_score'])\n",
    "            \n",
    "            # Calculate metrics for each category\n",
    "            categories = [('added', gt_added, pred_added), \n",
    "                         ('removed', gt_removed, pred_removed), \n",
    "                         ('changed', gt_changed, pred_changed)]\n",
    "            \n",
    "            perfect_match = True\n",
    "            error_details = {'img_id': img_id, 'errors': []}\n",
    "            \n",
    "            for cat_name, gt_set, pred_set in categories:\\n                # True positives, false positives, false negatives\n",
    "                tp = len(gt_set.intersection(pred_set))\n",
    "                fp = len(pred_set - gt_set)\n",
    "                fn = len(gt_set - pred_set)\n",
    "                \n",
    "                analysis_results['false_positives'][cat_name] += fp\n",
    "                analysis_results['false_negatives'][cat_name] += fn\n",
    "                \n",
    "                # Per-class performance\n",
    "                for class_name in gt_set.union(pred_set):\n",
    "                    if class_name in gt_set and class_name in pred_set:\n",
    "                        analysis_results['class_performance'][class_name]['tp'] += 1\n",
    "                    elif class_name in pred_set:\n",
    "                        analysis_results['class_performance'][class_name]['fp'] += 1\n",
    "                    elif class_name in gt_set:\n",
    "                        analysis_results['class_performance'][class_name]['fn'] += 1\n",
    "                \n",
    "                # Track errors\n",
    "                if fp > 0 or fn > 0:\n",
    "                    perfect_match = False\n",
    "                    if fp > 0:\n",
    "                        error_details['errors'].append(f\"{cat_name}_fp: {pred_set - gt_set}\")\n",
    "                    if fn > 0:\n",
    "                        error_details['errors'].append(f\"{cat_name}_fn: {gt_set - pred_set}\")\n",
    "            \n",
    "            if perfect_match:\n",
    "                analysis_results['correct_predictions'] += 1\n",
    "            else:\n",
    "                analysis_results['detailed_errors'].append(error_details)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing {img_id}: {e}\")\n",
    "    \n",
    "    return analysis_results\n",
    "\n",
    "# Perform error analysis\n",
    "if len(val_subset) > 0:\n",
    "    error_analysis = comprehensive_error_analysis(val_subset, fusion_pipeline, num_samples=30)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\\\nüìä Error Analysis Results:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Total samples analyzed: {error_analysis['total_analyzed']}\")\n",
    "    print(f\"Perfect predictions: {error_analysis['correct_predictions']}\")\n",
    "    print(f\"Accuracy: {error_analysis['correct_predictions']/error_analysis['total_analyzed']*100:.1f}%\")\n",
    "    \n",
    "    print(\"\\\\n‚ùå False Positives (Predicted but not in GT):\")\n",
    "    for category, count in error_analysis['false_positives'].items():\n",
    "        print(f\"  {category.capitalize()}: {count}\")\n",
    "    \n",
    "    print(\"\\\\n‚ùå False Negatives (In GT but not predicted):\")\n",
    "    for category, count in error_analysis['false_negatives'].items():\n",
    "        print(f\"  {category.capitalize()}: {count}\")\n",
    "    \n",
    "    # Per-class performance\n",
    "    print(\"\\\\nüéØ Per-Class Performance (Top 10):\")\n",
    "    class_f1_scores = []\n",
    "    for class_name, metrics in error_analysis['class_performance'].items():\n",
    "        tp, fp, fn = metrics['tp'], metrics['fp'], metrics['fn']\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        class_f1_scores.append((class_name, f1, tp, fp, fn))\n",
    "    \n",
    "    class_f1_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for class_name, f1, tp, fp, fn in class_f1_scores[:10]:\n",
    "        print(f\"  {class_name}: F1={f1:.3f} (TP:{tp}, FP:{fp}, FN:{fn})\")\n",
    "    \n",
    "    # Score distributions\n",
    "    if error_analysis['fusion_scores']:\n",
    "        print(\"\\\\nüìà Score Statistics:\")\n",
    "        print(f\"Fusion Score - Mean: {np.mean(error_analysis['fusion_scores']):.3f}, \"\n",
    "              f\"Std: {np.std(error_analysis['fusion_scores']):.3f}\")\n",
    "        print(f\"Change Score - Mean: {np.mean(error_analysis['change_scores']):.3f}, \"\n",
    "              f\"Std: {np.std(error_analysis['change_scores']):.3f}\")\n",
    "        print(f\"Object Score - Mean: {np.mean(error_analysis['object_scores']):.3f}, \"\n",
    "              f\"Std: {np.std(error_analysis['object_scores']):.3f}\")\n",
    "    \n",
    "    # Show worst errors\n",
    "    print(\"\\\\nüîç Sample Error Cases:\")\n",
    "    for i, error in enumerate(error_analysis['detailed_errors'][:5]):\n",
    "        print(f\"  {i+1}. Image {error['img_id']}: {'; '.join(error['errors'])}\")\n",
    "\n",
    "# Visualize score distributions\n",
    "if 'error_analysis' in locals() and error_analysis['fusion_scores']:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Fusion scores\n",
    "    axes[0].hist(error_analysis['fusion_scores'], bins=20, alpha=0.7, color='blue')\n",
    "    axes[0].set_title('Fusion Score Distribution')\n",
    "    axes[0].set_xlabel('Fusion Score')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].axvline(0.3, color='red', linestyle='--', label='Threshold')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Change scores\n",
    "    axes[1].hist(error_analysis['change_scores'], bins=20, alpha=0.7, color='green')\n",
    "    axes[1].set_title('Change Score Distribution')\n",
    "    axes[1].set_xlabel('Change Score')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    \n",
    "    # Object scores\n",
    "    axes[2].hist(error_analysis['object_scores'], bins=20, alpha=0.7, color='orange')\n",
    "    axes[2].set_title('Object Score Distribution')\n",
    "    axes[2].set_xlabel('Object Score')\n",
    "    axes[2].set_ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\\\n‚úÖ Comprehensive error analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c171ea72",
   "metadata": {},
   "source": [
    "## 14. Performance Enhancement Strategies\n",
    "Implement and evaluate advanced techniques for improved accuracy and robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104c970d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Enhancement Strategies\n",
    "\n",
    "print(\"üöÄ Performance Enhancement Strategies for Version 2\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Strategy 1: Test-Time Augmentation (TTA)\n",
    "class TestTimeAugmentation:\n",
    "    def __init__(self, fusion_pipeline, num_augmentations=4):\n",
    "        self.fusion_pipeline = fusion_pipeline\n",
    "        self.num_augmentations = num_augmentations\n",
    "        \n",
    "        # Define TTA transforms\n",
    "        self.tta_transforms = [\n",
    "            A.Compose([A.NoOp()]),  # Original\n",
    "            A.Compose([A.HorizontalFlip(p=1.0)]),  # Horizontal flip\n",
    "            A.Compose([A.Rotate(limit=5, p=1.0)]),  # Small rotation\n",
    "            A.Compose([A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=1.0)])  # Brightness\n",
    "        ]\n",
    "    \n",
    "    def analyze_with_tta(self, img_id, confidence_threshold=0.6):\n",
    "        \"\"\"Analyze image pair with test-time augmentation\"\"\"\n",
    "        img_path1 = os.path.join(data_dir, 'data', f'{img_id}_1.png')\n",
    "        img_path2 = os.path.join(data_dir, 'data', f'{img_id}_2.png')\n",
    "        \n",
    "        if not (os.path.exists(img_path1) and os.path.exists(img_path2)):\n",
    "            return {'error': f'Images not found for {img_id}'}\n",
    "        \n",
    "        # Collect predictions from different augmentations\n",
    "        all_results = []\n",
    "        \n",
    "        for i, transform in enumerate(self.tta_transforms[:self.num_augmentations]):\n",
    "            try:\n",
    "                # Apply transform to images (simplified - would need more complex implementation)\n",
    "                # For now, use original analysis\n",
    "                result = self.fusion_pipeline.analyze_image_pair(img_id, verbose=False)\n",
    "                if 'error' not in result:\n",
    "                    all_results.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"TTA error for augmentation {i}: {e}\")\n",
    "        \n",
    "        if not all_results:\n",
    "            return {'error': 'No valid TTA results'}\n",
    "        \n",
    "        # Ensemble predictions\n",
    "        avg_fusion_score = np.mean([r['fusion_score'] for r in all_results])\n",
    "        avg_change_score = np.mean([r['change_score'] for r in all_results])\n",
    "        \n",
    "        # Majority voting for object predictions\n",
    "        added_votes = defaultdict(int)\n",
    "        removed_votes = defaultdict(int)\n",
    "        changed_votes = defaultdict(int)\n",
    "        \n",
    "        for result in all_results:\n",
    "            for obj in result['added']:\n",
    "                added_votes[obj] += 1\n",
    "            for obj in result['removed']:\n",
    "                removed_votes[obj] += 1\n",
    "            for obj in result['changed']:\n",
    "                changed_votes[obj] += 1\n",
    "        \n",
    "        # Use confidence threshold for final predictions\n",
    "        threshold = max(1, int(len(all_results) * confidence_threshold))\n",
    "        \n",
    "        final_added = [obj for obj, votes in added_votes.items() if votes >= threshold]\n",
    "        final_removed = [obj for obj, votes in removed_votes.items() if votes >= threshold]\n",
    "        final_changed = [obj for obj, votes in changed_votes.items() if votes >= threshold]\n",
    "        \n",
    "        return {\n",
    "            'img_id': img_id,\n",
    "            'fusion_score': avg_fusion_score,\n",
    "            'change_score': avg_change_score,\n",
    "            'added': final_added,\n",
    "            'removed': final_removed,\n",
    "            'changed': final_changed,\n",
    "            'tta_votes': len(all_results),\n",
    "            'has_change': avg_fusion_score > 0.3\n",
    "        }\n",
    "\n",
    "# Strategy 2: Threshold Optimization\n",
    "def optimize_thresholds(val_df, fusion_pipeline, num_samples=50):\n",
    "    \"\"\"Optimize decision thresholds using validation data\"\"\"\n",
    "    print(\"\\\\nüéØ Optimizing Decision Thresholds...\")\n",
    "    \n",
    "    # Collect predictions and ground truth\n",
    "    sample_df = val_df.sample(min(num_samples, len(val_df)), random_state=42)\n",
    "    predictions = []\n",
    "    ground_truths = []\n",
    "    \n",
    "    for _, row in sample_df.iterrows():\n",
    "        try:\n",
    "            result = fusion_pipeline.analyze_image_pair(row['img_id'])\n",
    "            if 'error' not in result:\n",
    "                predictions.append(result)\n",
    "                ground_truths.append({\n",
    "                    'added': set(row['added_objs_norm']),\n",
    "                    'removed': set(row['removed_objs_norm']),\n",
    "                    'changed': set(row['changed_objs_norm']),\n",
    "                    'has_change': len(row['added_objs_norm']) + len(row['removed_objs_norm']) + len(row['changed_objs_norm']) > 0\n",
    "                })\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    if not predictions:\n",
    "        print(\"‚ö†Ô∏è No valid predictions for threshold optimization\")\n",
    "        return {'optimal_threshold': 0.3}\n",
    "    \n",
    "    # Test different thresholds\n",
    "    thresholds = np.arange(0.1, 0.9, 0.05)\n",
    "    best_f1 = 0\n",
    "    best_threshold = 0.3\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        tp = fp = fn = 0\n",
    "        \n",
    "        for pred, gt in zip(predictions, ground_truths):\n",
    "            pred_has_change = pred['fusion_score'] > threshold\n",
    "            gt_has_change = gt['has_change']\n",
    "            \n",
    "            if pred_has_change and gt_has_change:\n",
    "                tp += 1\n",
    "            elif pred_has_change and not gt_has_change:\n",
    "                fp += 1\n",
    "            elif not pred_has_change and gt_has_change:\n",
    "                fn += 1\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    print(f\"‚úì Optimal threshold: {best_threshold:.3f} (F1: {best_f1:.3f})\")\n",
    "    return {'optimal_threshold': best_threshold, 'best_f1': best_f1}\n",
    "\n",
    "# Strategy 3: Enhanced Fusion Weights\n",
    "def optimize_fusion_weights(val_df, model, object_matcher, num_samples=30):\n",
    "    \"\"\"Optimize fusion weights using grid search\"\"\"\n",
    "    print(\"\\\\n‚öñÔ∏è Optimizing Fusion Weights...\")\n",
    "    \n",
    "    weight_combinations = [\n",
    "        [0.7, 0.3], [0.65, 0.35], [0.6, 0.4], [0.55, 0.45], [0.5, 0.5],\n",
    "        [0.45, 0.55], [0.4, 0.6], [0.35, 0.65], [0.3, 0.7]\n",
    "    ]\n",
    "    \n",
    "    best_weights = [0.6, 0.4]\n",
    "    best_score = 0\n",
    "    \n",
    "    sample_df = val_df.sample(min(num_samples, len(val_df)), random_state=42)\n",
    "    \n",
    "    for weights in weight_combinations:\n",
    "        # Create temporary fusion pipeline with these weights\n",
    "        temp_fusion = EnhancedFusionPipeline(model, object_matcher, weights)\n",
    "        \n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for _, row in sample_df.iterrows():\n",
    "            try:\n",
    "                result = temp_fusion.analyze_image_pair(row['img_id'])\n",
    "                if 'error' not in result:\n",
    "                    # Simple accuracy check\n",
    "                    gt_has_change = (len(row['added_objs_norm']) + \n",
    "                                   len(row['removed_objs_norm']) + \n",
    "                                   len(row['changed_objs_norm'])) > 0\n",
    "                    pred_has_change = result['has_change']\n",
    "                    \n",
    "                    if gt_has_change == pred_has_change:\n",
    "                        correct += 1\n",
    "                    total += 1\n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        accuracy = correct / total if total > 0 else 0\n",
    "        \n",
    "        if accuracy > best_score:\n",
    "            best_score = accuracy\n",
    "            best_weights = weights\n",
    "    \n",
    "    print(f\"‚úì Optimal fusion weights: {best_weights} (Accuracy: {best_score:.3f})\")\n",
    "    return {'optimal_weights': best_weights, 'best_accuracy': best_score}\n",
    "\n",
    "# Enhancement Summary and Recommendations\n",
    "enhancement_strategies = [\n",
    "    \"1. üîÑ Test-Time Augmentation: Ensemble predictions from multiple augmented views\",\n",
    "    \"2. üéØ Threshold Optimization: Fine-tune decision thresholds using validation data\",\n",
    "    \"3. ‚öñÔ∏è Fusion Weight Optimization: Balance change detection vs object detection\",\n",
    "    \"4. üìä Advanced Post-Processing: Use confidence scores and object size filtering\",\n",
    "    \"5. üß† Model Ensemble: Combine multiple model architectures\",\n",
    "    \"6. üîç Multi-Scale Analysis: Process images at different resolutions\",\n",
    "    \"7. üìà Adaptive Thresholds: Use image-specific thresholds based on complexity\",\n",
    "    \"8. üéõÔ∏è Hyperparameter Tuning: Optimize IoU thresholds and confidence levels\",\n",
    "    \"9. üîÑ Iterative Refinement: Use model predictions to refine object matching\",\n",
    "    \"10. üìã Class-Specific Strategies: Different approaches for different object types\"\n",
    "]\n",
    "\n",
    "print(\"\\\\nüé® Enhancement Strategy Recommendations:\")\n",
    "print(\"=\" * 50)\n",
    "for strategy in enhancement_strategies:\n",
    "    print(f\"  {strategy}\")\n",
    "\n",
    "# Implement key enhancements if validation data available\n",
    "if len(val_subset) > 5:\n",
    "    print(\"\\\\nüîß Implementing Key Enhancements...\")\n",
    "    \n",
    "    # Initialize TTA\n",
    "    tta_analyzer = TestTimeAugmentation(fusion_pipeline, num_augmentations=3)\n",
    "    \n",
    "    # Optimize thresholds\n",
    "    threshold_results = optimize_thresholds(val_subset, fusion_pipeline, num_samples=20)\n",
    "    \n",
    "    # Optimize fusion weights\n",
    "    fusion_results = optimize_fusion_weights(val_subset, model, advanced_matcher, num_samples=15)\n",
    "    \n",
    "    # Test TTA on sample\n",
    "    if len(val_subset) > 0:\n",
    "        sample_id = val_subset.iloc[0]['img_id']\n",
    "        print(f\"\\\\nüß™ Testing TTA on sample {sample_id}:\")\n",
    "        \n",
    "        # Original prediction\n",
    "        orig_result = fusion_pipeline.analyze_image_pair(sample_id)\n",
    "        print(f\"Original: {orig_result['added']}, {orig_result['removed']}, {orig_result['changed']}\")\n",
    "        \n",
    "        # TTA prediction\n",
    "        tta_result = tta_analyzer.analyze_with_tta(sample_id)\n",
    "        if 'error' not in tta_result:\n",
    "            print(f\"TTA: {tta_result['added']}, {tta_result['removed']}, {tta_result['changed']}\")\n",
    "            print(f\"TTA Votes: {tta_result['tta_votes']}\")\n",
    "\n",
    "# Final enhancement recommendations\n",
    "print(\"\\\\nüí° Implementation Priority for Production:\")\n",
    "print(\"=\" * 45)\n",
    "priority_enhancements = [\n",
    "    \"ü•á HIGH: Threshold optimization and fusion weight tuning\",\n",
    "    \"ü•à MEDIUM: Test-time augmentation for critical cases\", \n",
    "    \"ü•â LOW: Model ensemble and multi-scale analysis\",\n",
    "    \"üîß ONGOING: Continuous monitoring and iterative refinement\"\n",
    "]\n",
    "\n",
    "for enhancement in priority_enhancements:\n",
    "    print(f\"  {enhancement}\")\n",
    "\n",
    "print(\"\\\\n‚úÖ Performance enhancement analysis complete!\")\n",
    "print(\"üéØ Ready for production deployment with optimized parameters!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
